{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from scipy.stats import skew\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ea4a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files\n",
    "\n",
    "case_data_4Yrs = pd.read_csv('2yrs_Case_Data.csv')\n",
    "control_data_4Yrs = pd.read_csv('2yrs_Control_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b09f77ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique PATID values in case_data: 3095\n",
      "Number of unique PATID values in control_data: 119723\n"
     ]
    }
   ],
   "source": [
    "# Check the number of unique PATID values in each DataFrame\n",
    "print(\"Number of unique PATID values in case_data:\", case_data['PATID'].nunique())\n",
    "print(\"Number of unique PATID values in control_data:\", control_data['PATID'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ef624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two datasets along the rows\n",
    "data = pd.concat([case_data, control_data], ignore_index=True)\n",
    "\n",
    "# Print the combined DataFrame info to verify the changes\n",
    "print(data.info())\n",
    "\n",
    "# Print the first few rows to check the result\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce2ae532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATID               0\n",
      "Sex                 0\n",
      "Race                5\n",
      "Marital_Status     12\n",
      "Max_DBP            45\n",
      "Max_SBP            32\n",
      "Min_DBP            45\n",
      "Min_SBP            32\n",
      "Comorbidities     494\n",
      "Smoking_Status    297\n",
      "Encounter Type      0\n",
      "Age_Grp             0\n",
      "Target              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find the number of missing values per column\n",
    "data.isnull().sum(axis=0)\n",
    "print(data.isnull().sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3301f6e2",
   "metadata": {},
   "source": [
    "# Check for Symmetry and skewness to decide whether to use the mean, median or mode for imputing missing values in numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13233da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 122818 entries, 0 to 122817\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   PATID           122818 non-null  int64  \n",
      " 1   Sex             122818 non-null  object \n",
      " 2   Race            122813 non-null  object \n",
      " 3   Marital_Status  122806 non-null  object \n",
      " 4   Max_DBP         122773 non-null  float64\n",
      " 5   Max_SBP         122786 non-null  float64\n",
      " 6   Min_DBP         122773 non-null  float64\n",
      " 7   Min_SBP         122786 non-null  float64\n",
      " 8   Comorbidities   122324 non-null  object \n",
      " 9   Smoking_Status  122521 non-null  object \n",
      " 10  Encounter Type  122818 non-null  object \n",
      " 11  Age_Grp         122818 non-null  object \n",
      " 12  Target          122818 non-null  int64  \n",
      "dtypes: float64(4), int64(2), object(7)\n",
      "memory usage: 12.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81161fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Missing Values  Percentage Missing\n",
      "PATID                        0            0.000000\n",
      "Sex                          0            0.000000\n",
      "Race                         5            0.004071\n",
      "Marital_Status              12            0.009771\n",
      "Max_DBP                     45            0.036640\n",
      "Max_SBP                     32            0.026055\n",
      "Min_DBP                     45            0.036640\n",
      "Min_SBP                     32            0.026055\n",
      "Comorbidities              494            0.402221\n",
      "Smoking_Status             297            0.241821\n",
      "Encounter Type               0            0.000000\n",
      "Age_Grp                      0            0.000000\n",
      "Target                       0            0.000000\n"
     ]
    }
   ],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_values_after_imputation = data.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "percentage_missing = (missing_values_after_imputation / len(data)) * 100\n",
    "\n",
    "# Print the number of missing values and their percentage\n",
    "missing_data_summary = pd.DataFrame({\n",
    "    'Missing Values': missing_values_after_imputation,\n",
    "    'Percentage Missing': percentage_missing\n",
    "})\n",
    "\n",
    "\n",
    "print(missing_data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ad8c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "# Define the numerical columns\n",
    "numerical_columns = ['Max_DBP', 'Max_SBP', 'Min_DBP', 'Min_SBP']\n",
    "\n",
    "for column in numerical_columns:\n",
    "    # Calculate the column skewness\n",
    "    column_skewness = skew(data[column].dropna())\n",
    "    \n",
    "    # Plot the distribution and a boxplot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(data[column].dropna(), kde=True)\n",
    "    plt.title(f'Distribution of {column} (Skewness: {column_skewness:.2f})')\n",
    "    \n",
    "    # Boxplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=data[column])\n",
    "    plt.title(f'Boxplot of {column}')\n",
    "    \n",
    "    # Save the plot before showing it\n",
    "    plt.savefig(f'{column}_distribution_boxplot.png')\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # Decide whether to use mean or median\n",
    "    if abs(column_skewness) < 0.5:\n",
    "        print(f\"{column}: Distribution is fairly symmetric. Impute missing values with mean.\")\n",
    "    else:\n",
    "        print(f\"{column}: Distribution is skewed or has outliers. Impute missing values with median.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b570f",
   "metadata": {},
   "source": [
    "# Impute missing values for the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4da4c0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with median for skewed distributions or columns with outliers\n",
    "\n",
    "data['Max_DBP'].fillna(data['Max_DBP'].median(), inplace=True)\n",
    "data['Max_SBP'].fillna(data['Max_SBP'].median(), inplace=True)\n",
    "data['Min_SBP'].fillna(data['Min_SBP'].median(), inplace=True)\n",
    "data['Min_DBP'].fillna(data['Min_DBP'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4eb9555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PATID               0\n",
       "Sex                 0\n",
       "Race                5\n",
       "Marital_Status     12\n",
       "Max_DBP             0\n",
       "Max_SBP             0\n",
       "Min_DBP             0\n",
       "Min_SBP             0\n",
       "Comorbidities     494\n",
       "Smoking_Status    297\n",
       "Encounter Type      0\n",
       "Age_Grp             0\n",
       "Target              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_values_after_imputation = data.isnull().sum()\n",
    "missing_values_after_imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87ad02",
   "metadata": {},
   "source": [
    "# Mapping and Encoding Diastolic Blood Pressure (DBP), and Systolic Blood Pressure (SBP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35be4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SBP and DBP categories with desired labels\n",
    "data['SBP_Category'] = pd.cut(data['Max_SBP'], bins=[-float('inf'), 120, 140, float('inf')], labels=['SBP <=120', 'SBP 120-140', 'SBP >=140'])\n",
    "data['DBP_Category'] = pd.cut(data['Max_DBP'], bins=[-float('inf'), 80, 90, float('inf')], labels=['DBP <=80', 'DBP 80-90', 'DBP >=90'])\n",
    "\n",
    "# One-hot encoding with the correct names\n",
    "data = pd.get_dummies(data, columns=['SBP_Category', 'DBP_Category'])\n",
    "\n",
    "# Rename the columns to match the format\n",
    "data = data.rename(columns={\n",
    "    'SBP_Category_SBP <=120': 'SBP below 120',\n",
    "    'SBP_Category_SBP 120-140': 'SBP 120-140',\n",
    "    'SBP_Category_SBP >=140': 'SBP above 140',\n",
    "    'DBP_Category_DBP <=80': 'DBP below 80',\n",
    "    'DBP_Category_DBP 80-90': 'DBP 80-90',\n",
    "    'DBP_Category_DBP >=90': 'DBP above 90'\n",
    "})\n",
    "\n",
    "# Remove the original columns\n",
    "data = data.drop(columns=['Max_SBP', 'Min_SBP', 'Max_DBP', 'Min_DBP'])\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(data.head())\n",
    "\n",
    "# Check and print the count for each category\n",
    "print(\"Count for each SBP category:\")\n",
    "print(f\"SBP below 120: {data['SBP below 120'].sum()}\")\n",
    "print(f\"SBP 120-140: {data['SBP 120-140'].sum()}\")\n",
    "print(f\"SBP above 140: {data['SBP above 140'].sum()}\")\n",
    "\n",
    "print(\"\\nCount for each DBP category:\")\n",
    "print(f\"DBP below 80: {data['DBP below 80'].sum()}\")\n",
    "print(f\"DBP 80-90: {data['DBP 80-90'].sum()}\")\n",
    "print(f\"DBP above 90: {data['DBP above 90'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23fd2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "percentage_missing = (missing_values / len(data)) * 100\n",
    "\n",
    "# Print the number of missing values and their percentage\n",
    "missing_data_summary = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage Missing': percentage_missing\n",
    "})\n",
    "\n",
    "\n",
    "print(missing_data_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52643c67",
   "metadata": {},
   "source": [
    "# Mapping and Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0fa757",
   "metadata": {},
   "source": [
    "# 1. Encoding 'Age' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa322690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding on the 'Age_Grp' column\n",
    "age_group_dummies = pd.get_dummies(data['Age_Grp'], prefix='Age_Grp')\n",
    "\n",
    "# Concatenate the original DataFrame with the new one hot encoded columns\n",
    "data = pd.concat([data, age_group_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8cdcaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf255aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'Age_Group' column\n",
    "data = data.drop(columns=['Age_Grp'])\n",
    "\n",
    "# Display the DataFrame\n",
    "#print(\"DataFrame with One Hot Encoding:\")\n",
    "print(data.head())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54628456",
   "metadata": {},
   "source": [
    "# 2. Imputation, Mapping and Encoding for 'Smoking_Status' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3fe866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value counts for 'Smoking_Status' column:\n",
      "Never smoker                                51445\n",
      "Former smoker quit longer than 12 months    35896\n",
      "Current every day smoker                    31262\n",
      "Current some day smoker                      1973\n",
      "Former smoker quit within 12 months          1470\n",
      "NaN                                           297\n",
      "Light tobacco smoker                          292\n",
      "Heavy tobacco smoker                          121\n",
      "Smoker ###                                     62\n",
      "Name: Smoking_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print initial count\n",
    "\n",
    "initial_count = data['Smoking_Status'].value_counts(dropna=False)\n",
    "print(\"Initial value counts for 'Smoking_Status' column:\")\n",
    "print(initial_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43c77615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for 'Smoking_Status' column after imputation:\n",
      "Never smoker                                51742\n",
      "Former smoker quit longer than 12 months    35896\n",
      "Current every day smoker                    31262\n",
      "Current some day smoker                      1973\n",
      "Former smoker quit within 12 months          1470\n",
      "Light tobacco smoker                          292\n",
      "Heavy tobacco smoker                          121\n",
      "Smoker ###                                     62\n",
      "Name: Smoking_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values (with most frequent value) in the 'Smoking_Status' column\n",
    "\n",
    "data['Smoking_Status'].fillna(data['Smoking_Status'].mode()[0], inplace=True)  \n",
    "\n",
    "# Verify the changes after imputation\n",
    "print(\"\\nValue counts for 'Smoking_Status' column after imputation:\")\n",
    "print(data['Smoking_Status'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6c0ce",
   "metadata": {},
   "source": [
    "# Define the Mapping Function of Smoking_Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4872ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and clean the string values in 'Smoking_Status'\n",
    "\n",
    "data['Smoking_Status'] = data['Smoking_Status'].str.replace('#', '').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fadf866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for encoding\n",
    "smoking_status_mapping = {\n",
    "    'Never smoker': 0,\n",
    "    'Former smoker quit longer than 12 months': 1,\n",
    "    'Former smoker quit within 12 months': 1,\n",
    "    'Light tobacco smoker': 1,\n",
    "    'Current some day smoker': 1,\n",
    "    'Current every day smoker': 1,\n",
    "    'Smoker': 1,\n",
    "    'Heavy tobacco smoker': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16aa0bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Smoking_Status'] = data['Smoking_Status'].map(smoking_status_mapping)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84cbe974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    71076\n",
      "0    51742\n",
      "Name: Smoking_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the counts of each category after encoding\n",
    "encoded_counts = data['Smoking_Status'].value_counts()\n",
    "\n",
    "print(encoded_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5e86779",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7592ffd3",
   "metadata": {},
   "source": [
    "# 3. Mapping and Encoding 'Sex' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9913dd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'F']\n",
      "  Sex\n",
      "0   M\n",
      "1   M\n",
      "2   F\n",
      "3   F\n",
      "4   F\n",
      "F    65935\n",
      "M    56883\n",
      "Name: Sex, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in the 'Sex' column\n",
    "unique_Sex_Status = data['Sex'].unique()\n",
    "print(unique_Sex_Status)\n",
    "\n",
    "# Verify the new columns and their unique values\n",
    "print(data[['Sex']].head())\n",
    "print(data['Sex'].value_counts(dropna=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2fda38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to the 'Sex' column\n",
    "data = pd.get_dummies(data, columns=['Sex'], dummy_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa538232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify the one-hot encoded DataFrame\n",
    "# print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa57d54",
   "metadata": {},
   "source": [
    "# 4. Imputation, Mapping and Encoding for 'Race' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d4a0a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHITE                                      112489\n",
      "BLACK OR AFRICAN AMERICAN                    7411\n",
      "SOME OTHER RACE                              1124\n",
      "ASIAN                                        1073\n",
      "Unknown                                       457\n",
      "AMERICAN INDIAN OR ALASKAN NATIVE             210\n",
      "NATIVE HAWAIIAN OR OTHER PACIFIC ISLAND        49\n",
      "NaN                                             5\n",
      "Name: Race, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in the 'Race' column\n",
    "print(data['Race'].value_counts(dropna=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0398e64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for 'Race' column after replacing 'Unknown' with NaN:\n",
      "WHITE                                      112489\n",
      "BLACK OR AFRICAN AMERICAN                    7411\n",
      "SOME OTHER RACE                              1124\n",
      "ASIAN                                        1073\n",
      "NaN                                           462\n",
      "AMERICAN INDIAN OR ALASKAN NATIVE             210\n",
      "NATIVE HAWAIIAN OR OTHER PACIFIC ISLAND        49\n",
      "Name: Race, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Unknown' with NaN\n",
    "data['Race'] = data['Race'].replace('Unknown', np.nan)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Value counts for 'Race' column after replacing 'Unknown' with NaN:\")\n",
    "print(data['Race'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffb48e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Missing Values  Percentage Missing\n",
      "PATID                        0            0.000000\n",
      "Race                       462            0.376166\n",
      "Marital_Status              12            0.009771\n",
      "Comorbidities              494            0.402221\n",
      "Smoking_Status               0            0.000000\n",
      "Encounter Type               0            0.000000\n",
      "Target                       0            0.000000\n",
      "SBP below 120                0            0.000000\n",
      "SBP 120-140                  0            0.000000\n",
      "SBP above 140                0            0.000000\n",
      "DBP below 80                 0            0.000000\n",
      "DBP 80-90                    0            0.000000\n",
      "DBP above 90                 0            0.000000\n",
      "Age_Grp_50-60                0            0.000000\n",
      "Age_Grp_60-70                0            0.000000\n",
      "Age_Grp_70-80                0            0.000000\n",
      "Age_Grp_80-90                0            0.000000\n",
      "Age_Grp_90-100               0            0.000000\n",
      "Sex_F                        0            0.000000\n",
      "Sex_M                        0            0.000000\n"
     ]
    }
   ],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "percentage_missing = (missing_values / len(data)) * 100\n",
    "\n",
    "# Print the number of missing values and their percentage\n",
    "missing_data_summary = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage Missing': percentage_missing\n",
    "})\n",
    "\n",
    "print(missing_data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7acdfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATID               0\n",
      "Race              462\n",
      "Marital_Status     12\n",
      "Comorbidities     494\n",
      "Smoking_Status      0\n",
      "Encounter Type      0\n",
      "Target              0\n",
      "SBP below 120       0\n",
      "SBP 120-140         0\n",
      "SBP above 140       0\n",
      "DBP below 80        0\n",
      "DBP 80-90           0\n",
      "DBP above 90        0\n",
      "Age_Grp_50-60       0\n",
      "Age_Grp_60-70       0\n",
      "Age_Grp_70-80       0\n",
      "Age_Grp_80-90       0\n",
      "Age_Grp_90-100      0\n",
      "Sex_F               0\n",
      "Sex_M               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # Check the number of missing values in each column\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2521a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for 'Race' column after imputation:\n",
      "WHITE                                      112951\n",
      "BLACK OR AFRICAN AMERICAN                    7411\n",
      "SOME OTHER RACE                              1124\n",
      "ASIAN                                        1073\n",
      "AMERICAN INDIAN OR ALASKAN NATIVE             210\n",
      "NATIVE HAWAIIAN OR OTHER PACIFIC ISLAND        49\n",
      "Name: Race, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values in the 'Race' column\n",
    "\n",
    "data['Race'].fillna(data['Race'].mode()[0], inplace=True)  # Replace with most frequent value\n",
    "\n",
    "# Verify the changes after imputation\n",
    "print(\"\\nValue counts for 'Race' column after imputation:\")\n",
    "print(data['Race'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27a826",
   "metadata": {},
   "source": [
    "# Define the Mapping Function for 'Race'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4934aca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white                                      112951\n",
      "black or african american                    7411\n",
      "some other race                              1124\n",
      "asian                                        1073\n",
      "american indian or alaskan native             210\n",
      "native hawaiian or other pacific island        49\n",
      "Name: Race_Grouped, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Normalize and clean the string and Define the mapping function for 'Race'\n",
    "# Treat any other unknown or unexpected values as Unknown\n",
    "\n",
    "def map_race(status):\n",
    "    status = status.lower().strip()  # Normalize and clean the string\n",
    "    \n",
    "    if status in ['white', \n",
    "                  'black or african american', \n",
    "                  'some other race',\n",
    "                  'asian', \n",
    "                  'american indian or alaskan native', \n",
    "                  'native hawaiian or other pacific island']:\n",
    "        return status\n",
    "    else:\n",
    "        return 'unknown'  # Treat any other unknown or unexpected values as Unknown\n",
    "\n",
    "# Apply the updated function to the 'Race' column\n",
    "data['Race_Grouped'] = data['Race'].apply(map_race)\n",
    "\n",
    "# Verify the changes\n",
    "print(data['Race_Grouped'].value_counts(dropna=False))\n",
    "\n",
    "# One-hot encoding on the 'Race_Grouped' column\n",
    "data = pd.get_dummies(data, columns=['Race_Grouped'], prefix='Race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "297e0f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values and their counts in the 'Race' column after mapping\n",
    "race_counts = data.filter(like='Race_').sum()\n",
    "print(race_counts)\n",
    "\n",
    "# Display the dataframe to verify encoding\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbc9c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping old column names to new column names\n",
    "\n",
    "new_column_names = {\n",
    "    'Race_american indian or alaskan native': 'AMERICAN_IND/ALASKAN',\n",
    "    'ace_asian': 'ASIAN',\n",
    "    'Race_black or african american': 'BLACK/AFRIC_AMERICAN',\n",
    "    'Race_native hawaiian or other pacific island': 'NAT_HAWAIIN',\n",
    "    'Race_some other race': 'OTHER',\n",
    "    'Race_white': 'WHITE',\n",
    "}\n",
    "\n",
    "# Rename the columns using the rename method\n",
    "data.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "# Drop the original 'Race' column as it has been encoded\n",
    "data = data.drop(columns=['Race'], errors='ignore')\n",
    "\n",
    "# Verify the changes\n",
    "#print(data.columns)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50eb5d3",
   "metadata": {},
   "source": [
    "# 5. Imputation, Mapping and Encoding 'Marital_Status' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41249386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Married' 'Widowed' 'Divorced' 'Single' 'Unknown' 'Separated' nan\n",
      " 'Life Partner']\n",
      "Married         70428\n",
      "Single          20895\n",
      "Divorced        15008\n",
      "Widowed         13393\n",
      "Unknown          1509\n",
      "Separated        1494\n",
      "Life Partner       79\n",
      "NaN                12\n",
      "Name: Marital_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in the 'Marital_Status' column\n",
    "unique_Marital_Status = data['Marital_Status'].unique()\n",
    "print(unique_Marital_Status)\n",
    "\n",
    "# Verify the unique values\n",
    "print(data['Marital_Status'].value_counts(dropna=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af59bd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for 'Marital_Status' column after replacing 'Unknown' with NaN:\n",
      "Married         70428\n",
      "Single          20895\n",
      "Divorced        15008\n",
      "Widowed         13393\n",
      "NaN              1521\n",
      "Separated        1494\n",
      "Life Partner       79\n",
      "Name: Marital_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Unknown' with NaN in the 'Marital_Status' column\n",
    "data['Marital_Status'] = data['Marital_Status'].replace('Unknown', np.nan)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Value counts for 'Marital_Status' column after replacing 'Unknown' with NaN:\")\n",
    "print(data['Marital_Status'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a1c4013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for 'Marital_Status' column after imputation:\n",
      "Married         71949\n",
      "Single          20895\n",
      "Divorced        15008\n",
      "Widowed         13393\n",
      "Separated        1494\n",
      "Life Partner       79\n",
      "Name: Marital_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values in the 'Marital_Status' column\n",
    "\n",
    "mode_value = data['Marital_Status'].mode()[0]\n",
    "data['Marital_Status'].fillna(mode_value, inplace=True)\n",
    "\n",
    "# Verify the changes after imputation\n",
    "print(\"\\nValue counts for 'Marital_Status' column after imputation:\")\n",
    "print(data['Marital_Status'].value_counts(dropna=False))\n",
    "\n",
    "# Perform one-hot encoding on the 'Marital_Status' column\n",
    "data = pd.get_dummies(data, columns=['Marital_Status'], prefix='Marital_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a6b3198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe to verify encoding\n",
    "\n",
    "print(data.head())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ff4fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding created new columns for each marital status value. We'll use a dictionary to map old column names to the new ones.\n",
    "\n",
    "new_column_names = {\n",
    "    'Marital_Status_Divorced': 'Divorced',\n",
    "    'Marital_Status_Life Partner': 'Life Partner',\n",
    "    'Marital_Status_Married': 'Married',\n",
    "    'Marital_Status_Separated': 'Separated',\n",
    "    'Marital_Status_Single': 'Single',\n",
    "    'Marital_Status_Widowed': 'Widowed'\n",
    "}\n",
    "\n",
    "# Rename the columns using the rename method\n",
    "data.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "\n",
    "# Verify the changes\n",
    "print(data.columns)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb92566",
   "metadata": {},
   "source": [
    "# 6. Mapping and Encoding 'Encounter Type' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f39b000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encounter Type Counts:\n",
      "OUTPATIENT                        30441\n",
      "OUTPATIENT,INPATIENT              15779\n",
      "INPATIENT,OUTPATIENT              12966\n",
      "OUTPATIENT,EMERGENCY               9448\n",
      "OUTPATIENT,EMERGENCY,INPATIENT     8840\n",
      "OUTPATIENT,INPATIENT,EMERGENCY     8827\n",
      "EMERGENCY,OUTPATIENT               7638\n",
      "EMERGENCY,OUTPATIENT,INPATIENT     7603\n",
      "INPATIENT,OUTPATIENT,EMERGENCY     7467\n",
      "EMERGENCY,INPATIENT,OUTPATIENT     6465\n",
      "INPATIENT,EMERGENCY,OUTPATIENT     6298\n",
      "EMERGENCY,INPATIENT                 388\n",
      "INPATIENT,EMERGENCY                 365\n",
      "INPATIENT                           220\n",
      "EMERGENCY                            73\n",
      "Name: Encounter Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the value counts for the 'Encounter Type' column\n",
    "\n",
    "encounter_type_counts = data['Encounter Type'].value_counts()\n",
    "print(\"Encounter Type Counts:\")\n",
    "print(encounter_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8a9daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'Encounter Type' values into separate columns\n",
    "split_encounters = data['Encounter Type'].str.get_dummies(sep=',')\n",
    "\n",
    "# Concatenate the original DataFrame with the new one hot encoded columns\n",
    "data = pd.concat([data, split_encounters], axis=1)\n",
    "\n",
    "# Drop the original 'Encounter Type' column\n",
    "data = data.drop(columns=['Encounter Type'])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"Updated DataFrame with One Hot Encoding for 'Encounter Type':\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9cc1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the new one hot encoded columns\n",
    "encoded_columns = split_encounters.columns\n",
    "print(\"One Hot Encoded Columns for 'Encounter Type':\")\n",
    "print(data[encoded_columns].head())\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d55694",
   "metadata": {},
   "source": [
    "# Rename the columns 'EMERGENCY' to 'EcType_ED', 'INPATIENT' to 'EcType_IP', and 'OUTPATIENT' to 'EcType_AV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11d7e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the specified columns\n",
    "data = data.rename(columns={\n",
    "    'EMERGENCY': 'EcType_ED',\n",
    "    'INPATIENT': 'EcType_IP',\n",
    "    'OUTPATIENT': 'EcType_AV'\n",
    "})\n",
    "\n",
    "# Display the DataFrame info to verify the changes\n",
    "print(data.info())\n",
    "\n",
    "# Print the first few rows to check the result\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177d212",
   "metadata": {},
   "source": [
    "# 7. Imputing, Mapping and Encoding for 'Comorbidities' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e333764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75e273c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in the 'Comorbidities' column\n",
    "unique_Comorbidities = data['Comorbidities'].unique()\n",
    "print(unique_Comorbidities)\n",
    "\n",
    "# Verify the new columns and their unique values\n",
    "#print(data[['Comorbidities']].head())\n",
    "print(data['Comorbidities'].value_counts(dropna=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2999d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of specific diseases we want to encode\n",
    "diseases_to_encode = [\n",
    "    'Diabetes', 'Type 2 Diabetes Mellitus', 'Epilepsy', 'Depression', 'Obesity', 'Stroke', 'Anxiety', 'Hypertension',\n",
    "    'Hyperlipidemia', 'Cardiovascular Disease', 'Sleep Disorder', 'Headache', 'Periodontitis', 'Concussion',\n",
    "    'Heart Disease', 'Sleep Apnea', 'Insomnia', 'Kidney Disease', 'Cholesterol', 'Vitamin D Deficiency',\n",
    "    'Enlarge Prostate', 'Osteoporosis', 'Bone Disease', 'Depressive Disorder'\n",
    "]\n",
    "\n",
    "# Define the function to create a new column for each disease\n",
    "def map_comorbidities(comorbidities, disease):\n",
    "    if pd.isna(comorbidities):\n",
    "        return 0\n",
    "    return 1 if disease.lower() in comorbidities.lower() else 0\n",
    "\n",
    "# Create new columns for each disease\n",
    "for disease in diseases_to_encode:\n",
    "    data[disease.replace(' ', '_')] = data['Comorbidities'].apply(lambda x: map_comorbidities(x, disease))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7c46b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the count of each newly created disease column\n",
    "for disease in diseases_to_encode:\n",
    "    column_name = disease.replace(' ', '_')\n",
    "    count = data[column_name].sum()\n",
    "    print(f\"Count of {disease}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21a415a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the  dataframe to verify encoding\n",
    "print(data.head())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaf0dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the changes\n",
    "print(data.head())\n",
    "\n",
    "# Drop the original 'Comorbidities'and 'Osteoporosis' column after encoding\n",
    "data.drop(columns=['Comorbidities', 'Osteoporosis'], inplace=True)\n",
    "\n",
    "# Display the dataframe\n",
    "print(data.head())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c409c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the 'Targer' column to the end\n",
    "target_col = data.pop('Target')\n",
    "data['Target'] = target_col\n",
    "\n",
    "# Print the DataFrame info to verify the changes\n",
    "print(data.info())\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eb8bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the number of missing values in each column\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898b8ee",
   "metadata": {},
   "source": [
    "# Save the Preprocessing DataFrame to a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file for ML analysis\n",
    "\n",
    "data.to_csv('Combined_ML_2Yrs_ML_Analysis.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
