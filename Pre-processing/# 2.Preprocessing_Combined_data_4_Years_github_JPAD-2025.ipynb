{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9816416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from scipy.stats import skew\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ea4a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files\n",
    "\n",
    "case_data_4Yrs = pd.read_csv('4yrs_Case_Data.csv')\n",
    "control_data_4Yrs = pd.read_csv('4yrs_Control_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b09f77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique PATID values in each DataFrame\n",
    "print(\"Number of unique PATID values in case_data:\", case_data_4Yrs['PATID'].nunique())\n",
    "print(\"Number of unique PATID values in control_data:\", control_data_4Yrs['PATID'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ef624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two datasets along the rows\n",
    "data = pd.concat([case_data_4Yrs, control_data_4Yrs], ignore_index=True)\n",
    "\n",
    "# Print the combined DataFrame info to verify the changes\n",
    "print(data.info())\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce2ae532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATID               0\n",
      "Sex                 0\n",
      "Race                7\n",
      "Marital_Status     14\n",
      "Max_DBP            66\n",
      "Max_SBP            46\n",
      "Min_DBP            66\n",
      "Min_SBP            46\n",
      "Comorbidities     738\n",
      "Smoking_Status    393\n",
      "Encounter Type      0\n",
      "Age_Grp             0\n",
      "Target              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find the number of missing values per column\n",
    "\n",
    "data.isnull().sum(axis=0)\n",
    "print(data.isnull().sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3301f6e2",
   "metadata": {},
   "source": [
    "# Check for Symmetry and skewness to decide whether to use the mean, median or mode for imputing missing values in numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13233da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 123545 entries, 0 to 123544\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   PATID           123545 non-null  int64  \n",
      " 1   Sex             123545 non-null  object \n",
      " 2   Race            123538 non-null  object \n",
      " 3   Marital_Status  123531 non-null  object \n",
      " 4   Max_DBP         123479 non-null  float64\n",
      " 5   Max_SBP         123499 non-null  float64\n",
      " 6   Min_DBP         123479 non-null  float64\n",
      " 7   Min_SBP         123499 non-null  float64\n",
      " 8   Comorbidities   122807 non-null  object \n",
      " 9   Smoking_Status  123152 non-null  object \n",
      " 10  Encounter Type  123545 non-null  object \n",
      " 11  Age_Grp         123545 non-null  object \n",
      " 12  Target          123545 non-null  int64  \n",
      "dtypes: float64(4), int64(2), object(7)\n",
      "memory usage: 12.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81161fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Missing Values  Percentage Missing\n",
      "PATID                        0            0.000000\n",
      "Sex                          0            0.000000\n",
      "Race                         7            0.005666\n",
      "Marital_Status              14            0.011332\n",
      "Max_DBP                     66            0.053422\n",
      "Max_SBP                     46            0.037233\n",
      "Min_DBP                     66            0.053422\n",
      "Min_SBP                     46            0.037233\n",
      "Comorbidities              738            0.597353\n",
      "Smoking_Status             393            0.318103\n",
      "Encounter Type               0            0.000000\n",
      "Age_Grp                      0            0.000000\n",
      "Target                       0            0.000000\n"
     ]
    }
   ],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_values_after_imputation = data.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "percentage_missing = (missing_values_after_imputation / len(data)) * 100\n",
    "\n",
    "# Print the number of missing values and their percentage\n",
    "missing_data_summary = pd.DataFrame({'Missing Values': missing_values_after_imputation,\n",
    "    'Percentage Missing': percentage_missing\n",
    "})\n",
    "\n",
    "\n",
    "print(missing_data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ad8c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numerical columns\n",
    "numerical_columns = ['Max_DBP', 'Max_SBP', 'Min_DBP', 'Min_SBP']\n",
    "\n",
    "for column in numerical_columns:\n",
    "    # Calculate the column skewness\n",
    "    column_skewness = skew(data[column].dropna())\n",
    "    \n",
    "    # Plot the distribution and a boxplot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(data[column].dropna(), kde=True)\n",
    "    plt.title(f'Distribution of {column} (Skewness: {column_skewness:.2f})')\n",
    "    \n",
    "    # Boxplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=data[column])\n",
    "    plt.title(f'Boxplot of {column}')\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # Choose whether mean or median.\n",
    "    if abs(column_skewness) < 0.5:\n",
    "        print(f\"{column}: Distribution is fairly symmetric. Impute missing values with mean.\")\n",
    "    else:\n",
    "        print(f\"{column}: Distribution is skewed or has outliers. Impute missing values with median.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b570f",
   "metadata": {},
   "source": [
    "# Impute missing values for the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4da4c0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with median for skewed distributions or columns with outliers\n",
    "\n",
    "data['Max_DBP'].fillna(data['Max_DBP'].median(), inplace=True)\n",
    "data['Max_SBP'].fillna(data['Max_SBP'].median(), inplace=True)\n",
    "data['Min_SBP'].fillna(data['Min_SBP'].median(), inplace=True)\n",
    "data['Min_DBP'].fillna(data['Min_DBP'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4eb9555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PATID               0\n",
       "Sex                 0\n",
       "Race                7\n",
       "Marital_Status     14\n",
       "Max_DBP             0\n",
       "Max_SBP             0\n",
       "Min_DBP             0\n",
       "Min_SBP             0\n",
       "Comorbidities     738\n",
       "Smoking_Status    393\n",
       "Encounter Type      0\n",
       "Age_Grp             0\n",
       "Target              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of missing values in each column\n",
    "\n",
    "missing_values_after_imputation = data.isnull().sum()\n",
    "missing_values_after_imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87ad02",
   "metadata": {},
   "source": [
    "# Mapping and Encoding Diastolic Blood Pressure (DBP), and Systolic Blood Pressure (SBP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35be4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SBP and DBP categories with desired labels\n",
    "data['SBP_Category'] = pd.cut(data['Max_SBP'], bins=[-float('inf'), 120, 140, float('inf')], labels=['SBP <=120', 'SBP 120-140', 'SBP >=140'])\n",
    "data['DBP_Category'] = pd.cut(data['Max_DBP'], bins=[-float('inf'), 80, 90, float('inf')], labels=['DBP <=80', 'DBP 80-90', 'DBP >=90'])\n",
    "\n",
    "# One-hot encoding with the correct names\n",
    "data = pd.get_dummies(data, columns=['SBP_Category', 'DBP_Category'])\n",
    "\n",
    "# Rename the columns to match your format\n",
    "data = data.rename(columns={\n",
    "    'SBP_Category_SBP <=120': 'SBP below 120',\n",
    "    'SBP_Category_SBP 120-140': 'SBP 120-140',\n",
    "    'SBP_Category_SBP >=140': 'SBP above 140',\n",
    "    'DBP_Category_DBP <=80': 'DBP below 80',\n",
    "    'DBP_Category_DBP 80-90': 'DBP 80-90',\n",
    "    'DBP_Category_DBP >=90': 'DBP above 90'\n",
    "})\n",
    "\n",
    "# Remove the original columns\n",
    "data = data.drop(columns=['Max_SBP', 'Min_SBP', 'Max_DBP', 'Min_DBP'])\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(data.head())\n",
    "\n",
    "# Check and print the count for each category\n",
    "print(\"Count for each SBP category:\")\n",
    "print(f\"SBP below 120: {data['SBP below 120'].sum()}\")\n",
    "print(f\"SBP 120-140: {data['SBP 120-140'].sum()}\")\n",
    "print(f\"SBP above 140: {data['SBP above 140'].sum()}\")\n",
    "\n",
    "print(\"\\nCount for each DBP category:\")\n",
    "print(f\"DBP below 80: {data['DBP below 80'].sum()}\")\n",
    "print(f\"DBP 80-90: {data['DBP 80-90'].sum()}\")\n",
    "print(f\"DBP above 90: {data['DBP above 90'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23fd2e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Missing Values  Percentage Missing\n",
      "PATID                        0            0.000000\n",
      "Sex                          0            0.000000\n",
      "Race                         7            0.005666\n",
      "Marital_Status              14            0.011332\n",
      "Comorbidities              738            0.597353\n",
      "Smoking_Status             393            0.318103\n",
      "Encounter Type               0            0.000000\n",
      "Age_Grp                      0            0.000000\n",
      "Target                       0            0.000000\n",
      "SBP below 120                0            0.000000\n",
      "SBP 120-140                  0            0.000000\n",
      "SBP above 140                0            0.000000\n",
      "DBP below 80                 0            0.000000\n",
      "DBP 80-90                    0            0.000000\n",
      "DBP above 90                 0            0.000000\n"
     ]
    }
   ],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "percentage_missing = (missing_values / len(data)) * 100\n",
    "\n",
    "# Print the number of missing values and their percentage\n",
    "missing_data_summary = pd.DataFrame({'Missing Values': missing_values,'Percentage Missing': percentage_missing})\n",
    "\n",
    "\n",
    "print(missing_data_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52643c67",
   "metadata": {},
   "source": [
    "# Mapping and Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0fa757",
   "metadata": {},
   "source": [
    "# 1. Encoding 'Age' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa322690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding on the 'Age_Grp' column\n",
    "age_group_dummies = pd.get_dummies(data['Age_Grp'], prefix='Age_Grp')\n",
    "\n",
    "# Concatenate the original DataFrame with the new one hot encoded columns\n",
    "data = pd.concat([data, age_group_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8cdcaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf255aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'Age_Group' column\n",
    "data = data.drop(columns=['Age_Grp'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(data.head())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54628456",
   "metadata": {},
   "source": [
    "# 2. Imputation, Mapping and Encoding for 'Smoking_Status' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3fe866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value counts for 'Smoking_Status' column:\n",
      "Never smoker                                51761\n",
      "Former smoker quit longer than 12 months    36116\n",
      "Current every day smoker                    31343\n",
      "Current some day smoker                      1978\n",
      "Former smoker quit within 12 months          1478\n",
      "NaN                                           393\n",
      "Light tobacco smoker                          293\n",
      "Heavy tobacco smoker                          121\n",
      "Smoker ###                                     62\n",
      "Name: Smoking_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print initial count of Smoking_Status\n",
    "\n",
    "initial_count = data['Smoking_Status'].value_counts(dropna=False)\n",
    "print(\"Initial value counts for 'Smoking_Status' column:\")\n",
    "print(initial_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43c77615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for 'Smoking_Status' column after imputation:\n",
      "Never smoker                                52154\n",
      "Former smoker quit longer than 12 months    36116\n",
      "Current every day smoker                    31343\n",
      "Current some day smoker                      1978\n",
      "Former smoker quit within 12 months          1478\n",
      "Light tobacco smoker                          293\n",
      "Heavy tobacco smoker                          121\n",
      "Smoker ###                                     62\n",
      "Name: Smoking_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values (with most frequent value) in the 'Smoking_Status' column\n",
    "\n",
    "data['Smoking_Status'].fillna(data['Smoking_Status'].mode()[0], inplace=True)  \n",
    "\n",
    "# Verify the changes after imputation\n",
    "print(\"\\nValue counts for 'Smoking_Status' column after imputation:\")\n",
    "print(data['Smoking_Status'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6c0ce",
   "metadata": {},
   "source": [
    "# Define the Mapping Function of Smoking_Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4872ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Normalize and clean the string values in 'Smoking_Status'\n",
    "data['Smoking_Status'] = data['Smoking_Status'].str.replace('#', '').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eabb0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for encoding\n",
    "\n",
    "smoking_status_mapping = {\n",
    "    'Never smoker': 0,\n",
    "    'Former smoker quit longer than 12 months': 1,\n",
    "    'Former smoker quit within 12 months': 1,\n",
    "    'Light tobacco smoker': 1,\n",
    "    'Current some day smoker': 1,\n",
    "    'Current every day smoker': 1,\n",
    "    'Smoker': 1,\n",
    "    'Heavy tobacco smoker': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719042d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Smoking_Status'] = data['Smoking_Status'].map(smoking_status_mapping)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60c59a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    71391\n",
      "0    52154\n",
      "Name: Smoking_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the counts of each category after encoding\n",
    "\n",
    "encoded_counts = data['Smoking_Status'].value_counts()\n",
    "\n",
    "print(encoded_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23db94de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7592ffd3",
   "metadata": {},
   "source": [
    "# 3. Mapping and Encoding 'Sex' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9913dd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'F']\n",
      "  Sex\n",
      "0   M\n",
      "1   F\n",
      "2   M\n",
      "3   M\n",
      "4   M\n",
      "F    66377\n",
      "M    57168\n",
      "Name: Sex, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# List unique 'Sex' values\n",
    "\n",
    "unique_Sex_Status = data['Sex'].unique()\n",
    "print(unique_Sex_Status)\n",
    "\n",
    "# Verify the unique values\n",
    "print(data[['Sex']].head())\n",
    "print(data['Sex'].value_counts(dropna=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2fda38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to the 'Sex' column\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Sex'], dummy_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa538232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the info of the one-hot encoded DataFrame\n",
    "\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa57d54",
   "metadata": {},
   "source": [
    "# 4. Imputation, Mapping and Encoding for 'Race' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d4a0a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHITE                                      113165\n",
      "BLACK OR AFRICAN AMERICAN                    7450\n",
      "SOME OTHER RACE                              1127\n",
      "ASIAN                                        1077\n",
      "Unknown                                       459\n",
      "AMERICAN INDIAN OR ALASKAN NATIVE             211\n",
      "NATIVE HAWAIIAN OR OTHER PACIFIC ISLAND        49\n",
      "NaN                                             7\n",
      "Name: Race, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in the 'Race' column\n",
    "\n",
    "print(data['Race'].value_counts(dropna=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0398e64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for 'Race' column after replacing 'Unknown' with NaN:\n",
      "WHITE                                      113165\n",
      "BLACK OR AFRICAN AMERICAN                    7450\n",
      "SOME OTHER RACE                              1127\n",
      "ASIAN                                        1077\n",
      "NaN                                           466\n",
      "AMERICAN INDIAN OR ALASKAN NATIVE             211\n",
      "NATIVE HAWAIIAN OR OTHER PACIFIC ISLAND        49\n",
      "Name: Race, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Unknown' with NaN\n",
    "data['Race'] = data['Race'].replace('Unknown', np.nan)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Value counts for 'Race' column after replacing 'Unknown' with NaN:\")\n",
    "print(data['Race'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffb48e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Missing Values  Percentage Missing\n",
      "PATID                        0            0.000000\n",
      "Race                       466            0.377190\n",
      "Marital_Status              14            0.011332\n",
      "Comorbidities              738            0.597353\n",
      "Smoking_Status               0            0.000000\n",
      "Encounter Type               0            0.000000\n",
      "Target                       0            0.000000\n",
      "SBP below 120                0            0.000000\n",
      "SBP 120-140                  0            0.000000\n",
      "SBP above 140                0            0.000000\n",
      "DBP below 80                 0            0.000000\n",
      "DBP 80-90                    0            0.000000\n",
      "DBP above 90                 0            0.000000\n",
      "Age_Grp_50-60                0            0.000000\n",
      "Age_Grp_60-70                0            0.000000\n",
      "Age_Grp_70-80                0            0.000000\n",
      "Age_Grp_80-90                0            0.000000\n",
      "Age_Grp_90-100               0            0.000000\n",
      "Sex_F                        0            0.000000\n",
      "Sex_M                        0            0.000000\n"
     ]
    }
   ],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "percentage_missing = (missing_values / len(data)) * 100\n",
    "\n",
    "# Print the number of missing values and their percentage\n",
    "missing_data_summary = pd.DataFrame({'Missing Values': missing_values,'Percentage Missing': percentage_missing})\n",
    "\n",
    "print(missing_data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7acdfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATID               0\n",
      "Race              466\n",
      "Marital_Status     14\n",
      "Comorbidities     738\n",
      "Smoking_Status      0\n",
      "Encounter Type      0\n",
      "Target              0\n",
      "SBP below 120       0\n",
      "SBP 120-140         0\n",
      "SBP above 140       0\n",
      "DBP below 80        0\n",
      "DBP 80-90           0\n",
      "DBP above 90        0\n",
      "Age_Grp_50-60       0\n",
      "Age_Grp_60-70       0\n",
      "Age_Grp_70-80       0\n",
      "Age_Grp_80-90       0\n",
      "Age_Grp_90-100      0\n",
      "Sex_F               0\n",
      "Sex_M               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2521a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for 'Race' column after imputation:\n",
      "WHITE                                      113631\n",
      "BLACK OR AFRICAN AMERICAN                    7450\n",
      "SOME OTHER RACE                              1127\n",
      "ASIAN                                        1077\n",
      "AMERICAN INDIAN OR ALASKAN NATIVE             211\n",
      "NATIVE HAWAIIAN OR OTHER PACIFIC ISLAND        49\n",
      "Name: Race, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values (with most frequent value) in the 'Race' column\n",
    "\n",
    "data['Race'].fillna(data['Race'].mode()[0], inplace=True)  \n",
    "\n",
    "# Verify the changes after imputation\n",
    "print(\"\\nValue counts for 'Race' column after imputation:\")\n",
    "print(data['Race'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27a826",
   "metadata": {},
   "source": [
    "# Define the Mapping Function for 'Race'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4934aca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white                                      113631\n",
      "black or african american                    7450\n",
      "some other race                              1127\n",
      "asian                                        1077\n",
      "american indian or alaskan native             211\n",
      "native hawaiian or other pacific island        49\n",
      "Name: Race_Grouped, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Normalize and clean the string and Define the mapping function for 'Race'\n",
    "# Treat any other unknown or unexpected values as Unknown\n",
    "\n",
    "def map_race(status):\n",
    "    status = status.lower().strip()  \n",
    "    \n",
    "    if status in ['white', \n",
    "                  'black or african american', \n",
    "                  'some other race',\n",
    "                  'asian', \n",
    "                  'american indian or alaskan native', \n",
    "                  'native hawaiian or other pacific island']:\n",
    "        return status\n",
    "    else:\n",
    "        return 'unknown'  \n",
    "\n",
    "\n",
    "data['Race_Grouped'] = data['Race'].apply(map_race)\n",
    "\n",
    "print(data['Race_Grouped'].value_counts(dropna=False))\n",
    "\n",
    "# One-hot encoding on the 'Race_Grouped' column\n",
    "data = pd.get_dummies(data, columns=['Race_Grouped'], prefix='Race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "297e0f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values and their counts in the 'Race' column after mapping\n",
    "\n",
    "race_counts = data.filter(like='Race_').sum()\n",
    "print(race_counts)\n",
    "\n",
    "# Display the dataframe to verify encoding\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbc9c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping old column names to new column names\n",
    "\n",
    "new_column_names = {\n",
    "    'Race_american indian or alaskan native': 'AMERICAN_IND/ALASKAN',\n",
    "    'ace_asian': 'ASIAN',\n",
    "    'Race_black or african american': 'BLACK/AFRIC_AMERICAN',\n",
    "    'Race_native hawaiian or other pacific island': 'NAT_HAWAIIN',\n",
    "    'Race_some other race': 'OTHER',\n",
    "    'Race_white': 'WHITE',\n",
    "}\n",
    "\n",
    "# Rename the columns \n",
    "data.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "# Drop the original 'Race' column as it has been encoded\n",
    "data = data.drop(columns=['Race'], errors='ignore')\n",
    "\n",
    "# Verify the changes\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50eb5d3",
   "metadata": {},
   "source": [
    "# 5. Imputation, Mapping and Encoding 'Marital_Status' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41249386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Married' 'Widowed' 'Single' 'Divorced' 'Unknown' 'Separated' nan\n",
      " 'Life Partner']\n",
      "Married         70816\n",
      "Single          20973\n",
      "Divorced        15065\n",
      "Widowed         13573\n",
      "Unknown          1523\n",
      "Separated        1502\n",
      "Life Partner       79\n",
      "NaN                14\n",
      "Name: Marital_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in the 'Marital_Status' column\n",
    "unique_Marital_Status = data['Marital_Status'].unique()\n",
    "print(unique_Marital_Status)\n",
    "\n",
    "# Verify the unique values\n",
    "\n",
    "print(data['Marital_Status'].value_counts(dropna=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af59bd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for 'Marital_Status' column after replacing 'Unknown' with NaN:\n",
      "Married         70816\n",
      "Single          20973\n",
      "Divorced        15065\n",
      "Widowed         13573\n",
      "NaN              1537\n",
      "Separated        1502\n",
      "Life Partner       79\n",
      "Name: Marital_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Unknown' with NaN in the 'Marital_Status' column\n",
    "data['Marital_Status'] = data['Marital_Status'].replace('Unknown', np.nan)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Value counts for 'Marital_Status' column after replacing 'Unknown' with NaN:\")\n",
    "print(data['Marital_Status'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a1c4013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for 'Marital_Status' column after imputation:\n",
      "Married         72353\n",
      "Single          20973\n",
      "Divorced        15065\n",
      "Widowed         13573\n",
      "Separated        1502\n",
      "Life Partner       79\n",
      "Name: Marital_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values in the 'Marital_Status' column\n",
    "\n",
    "mode_value = data['Marital_Status'].mode()[0]\n",
    "data['Marital_Status'].fillna(mode_value, inplace=True)\n",
    "\n",
    "# Verify the changes after imputation\n",
    "print(\"\\nValue counts for 'Marital_Status' column after imputation:\")\n",
    "print(data['Marital_Status'].value_counts(dropna=False))\n",
    "\n",
    "# One-hot encoding on the 'Marital_Status' column\n",
    "data = pd.get_dummies(data, columns=['Marital_Status'], prefix='Marital_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a6b3198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe to verify encoding\n",
    "\n",
    "print(data.head())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ff4fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding created new columns for each marital status value. We'll use a dictionary to map old column names to the new ones.\n",
    "\n",
    "new_column_names = {\n",
    "    'Marital_Status_Divorced': 'Divorced',\n",
    "    'Marital_Status_Life Partner': 'Life Partner',\n",
    "    'Marital_Status_Married': 'Married',\n",
    "    'Marital_Status_Separated': 'Separated',\n",
    "    'Marital_Status_Single': 'Single',\n",
    "    'Marital_Status_Widowed': 'Widowed'\n",
    "}\n",
    "\n",
    "# Rename the columns using the rename method\n",
    "data.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "\n",
    "# Verify the changes\n",
    "print(data.columns)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb92566",
   "metadata": {},
   "source": [
    "# 6. Mapping and Encoding 'Encounter Type' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f39b000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encounter Type Counts:\n",
      "OUTPATIENT                        30598\n",
      "OUTPATIENT,INPATIENT              15869\n",
      "INPATIENT,OUTPATIENT              13043\n",
      "OUTPATIENT,EMERGENCY               9486\n",
      "OUTPATIENT,EMERGENCY,INPATIENT     8897\n",
      "OUTPATIENT,INPATIENT,EMERGENCY     8892\n",
      "EMERGENCY,OUTPATIENT               7675\n",
      "EMERGENCY,OUTPATIENT,INPATIENT     7659\n",
      "INPATIENT,OUTPATIENT,EMERGENCY     7532\n",
      "EMERGENCY,INPATIENT,OUTPATIENT     6511\n",
      "INPATIENT,EMERGENCY,OUTPATIENT     6336\n",
      "EMERGENCY,INPATIENT                 388\n",
      "INPATIENT,EMERGENCY                 366\n",
      "INPATIENT                           220\n",
      "EMERGENCY                            73\n",
      "Name: Encounter Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the value counts for the 'Encounter Type' column\n",
    "\n",
    "encounter_type_counts = data['Encounter Type'].value_counts()\n",
    "print(\"Encounter Type Counts:\")\n",
    "print(encounter_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8a9daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'Encounter Type' values into separate columns\n",
    "split_encounters = data['Encounter Type'].str.get_dummies(sep=',')\n",
    "\n",
    "# Concatenate the original DataFrame with the new one hot encoded columns\n",
    "data = pd.concat([data, split_encounters], axis=1)\n",
    "\n",
    "# Drop the original 'Encounter Type' column\n",
    "data = data.drop(columns=['Encounter Type'])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"Updated DataFrame with One Hot Encoding for 'Encounter Type':\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9cc1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the new one hot encoded columns\n",
    "\n",
    "encoded_columns = split_encounters.columns\n",
    "print(\"One Hot Encoded Columns for 'Encounter Type':\")\n",
    "print(data[encoded_columns].head())\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d55694",
   "metadata": {},
   "source": [
    "# Rename the columns 'EMERGENCY' to 'EcType_ED', 'INPATIENT' to 'EcType_IP', and 'OUTPATIENT' to 'EcType_AV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11d7e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the specified columns\n",
    "\n",
    "data = data.rename(columns={\n",
    "    'EMERGENCY': 'EcType_ED',\n",
    "    'INPATIENT': 'EcType_IP',\n",
    "    'OUTPATIENT': 'EcType_AV'\n",
    "})\n",
    "\n",
    "# Display the DataFrame \n",
    "print(data.info())\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177d212",
   "metadata": {},
   "source": [
    "# 7. Imputing, Mapping and Encoding for 'Comorbidities' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e333764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATID                     0\n",
      "Comorbidities           738\n",
      "Smoking_Status            0\n",
      "Target                    0\n",
      "SBP below 120             0\n",
      "SBP 120-140               0\n",
      "SBP above 140             0\n",
      "DBP below 80              0\n",
      "DBP 80-90                 0\n",
      "DBP above 90              0\n",
      "Age_Grp_50-60             0\n",
      "Age_Grp_60-70             0\n",
      "Age_Grp_70-80             0\n",
      "Age_Grp_80-90             0\n",
      "Age_Grp_90-100            0\n",
      "Sex_F                     0\n",
      "Sex_M                     0\n",
      "AMERICAN_IND/ALASKAN      0\n",
      "Race_asian                0\n",
      "BLACK/AFRIC_AMERICAN      0\n",
      "NAT_HAWAIIN               0\n",
      "OTHER                     0\n",
      "WHITE                     0\n",
      "Divorced                  0\n",
      "Life Partner              0\n",
      "Married                   0\n",
      "Separated                 0\n",
      "Single                    0\n",
      "Widowed                   0\n",
      "EcType_ED                 0\n",
      "EcType_IP                 0\n",
      "EcType_AV                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the number of missing values in each column\n",
    "\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75e273c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in the 'Comorbidities' column\n",
    "\n",
    "unique_Comorbidities = data['Comorbidities'].unique()\n",
    "print(unique_Comorbidities)\n",
    "\n",
    "# Verify the new columns and their unique values\n",
    "#print(data[['Comorbidities']].head())\n",
    "print(data['Comorbidities'].value_counts(dropna=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2999d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of specific diseases to encode\n",
    "\n",
    "diseases_to_encode = [\n",
    "    'Diabetes', 'Type 2 Diabetes Mellitus', 'Epilepsy', 'Depression', 'Obesity', 'Stroke', 'Anxiety', 'Hypertension',\n",
    "    'Hyperlipidemia', 'Cardiovascular Disease', 'Sleep Disorder', 'Headache', 'Periodontitis', 'Concussion',\n",
    "    'Heart Disease', 'Sleep Apnea', 'Insomnia', 'Kidney Disease', 'Cholesterol', 'Vitamin D Deficiency',\n",
    "    'Enlarge Prostate', 'Osteoporosis', 'Bone Disease', 'Depressive Disorder'\n",
    "]\n",
    "\n",
    "# Define the function to create a new column for each disease\n",
    "\n",
    "def map_comorbidities(comorbidities, disease):\n",
    "    if pd.isna(comorbidities):\n",
    "        return 0\n",
    "    return 1 if disease.lower() in comorbidities.lower() else 0\n",
    "\n",
    "# Create new columns for each disease\n",
    "\n",
    "for disease in diseases_to_encode:\n",
    "    data[disease.replace(' ', '_')] = data['Comorbidities'].apply(lambda x: map_comorbidities(x, disease))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7c46b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the count of each newly created disease column\n",
    "\n",
    "for disease in diseases_to_encode:\n",
    "    column_name = disease.replace(' ', '_')\n",
    "    count = data[column_name].sum()\n",
    "    print(f\"Count of {disease}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21a415a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe to verify encoding\n",
    "\n",
    "print(data.head())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaf0dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the changes\n",
    "print(data.head())\n",
    "\n",
    "# Drop the original 'Comorbidities' column after encoding as it's not needed. Also, we want to drop 'Osteoporosis'\n",
    "data.drop(columns=['Comorbidities', 'Osteoporosis'], inplace=True)\n",
    "\n",
    "# Display the first few rows of the dataframe to verify encoding\n",
    "print(data.head())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c409c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Target' is our output variable. Place the 'Target' column to the end\n",
    "\n",
    "target_col = data.pop('Target')\n",
    "data['Target'] = target_col\n",
    "\n",
    "# Print the DataFrame info to verify the changes\n",
    "print(data.info())\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898b8ee",
   "metadata": {},
   "source": [
    "# Save the Preprocessing DataFrame to a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file for ML analysis\n",
    "\n",
    "data.to_csv('Combined_ML_4Yrs_ML_Analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39285c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
