{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9816416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from scipy.stats import skew\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ea4a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files\n",
    "\n",
    "case_data = pd.read_csv('5yrs_Case_Data.csv')\n",
    "control_data = pd.read_csv('5yrs_Control_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09f77ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique PATID values in case_data: 4012\n",
      "Number of unique PATID values in control_data: 119723\n"
     ]
    }
   ],
   "source": [
    "# Check the number of unique PATID values in each DataFrame\n",
    "\n",
    "print(\"Number of unique PATID values in case_data:\", case_data['PATID'].nunique())\n",
    "print(\"Number of unique PATID values in control_data:\", control_data['PATID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ef624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two datasets along the rows\n",
    "\n",
    "data = pd.concat([case_data, control_data], ignore_index=True)\n",
    "\n",
    "# Print the combined DataFrame info to verify the changes\n",
    "print(data.info())\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce2ae532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATID               0\n",
      "Sex                 0\n",
      "Race                7\n",
      "Marital_Status     14\n",
      "Max_DBP            79\n",
      "Max_SBP            55\n",
      "Min_DBP            79\n",
      "Min_SBP            55\n",
      "Comorbidities     813\n",
      "Smoking_Status    445\n",
      "Encounter Type      0\n",
      "Age_Grp             0\n",
      "Target              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find the number of missing values per column\n",
    "\n",
    "data.isnull().sum(axis=0)\n",
    "print(data.isnull().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13233da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81161fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Missing Values  Percentage Missing\n",
      "PATID                        0            0.000000\n",
      "Sex                          0            0.000000\n",
      "Race                         7            0.005657\n",
      "Marital_Status              14            0.011315\n",
      "Max_DBP                     79            0.063846\n",
      "Max_SBP                     55            0.044450\n",
      "Min_DBP                     79            0.063846\n",
      "Min_SBP                     55            0.044450\n",
      "Comorbidities              813            0.657049\n",
      "Smoking_Status             445            0.359640\n",
      "Encounter Type               0            0.000000\n",
      "Age_Grp                      0            0.000000\n",
      "Target                       0            0.000000\n"
     ]
    }
   ],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_values_after_imputation = data.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "percentage_missing = (missing_values_after_imputation / len(data)) * 100\n",
    "\n",
    "# Print the number of missing values and their percentage\n",
    "missing_data_summary = pd.DataFrame({\n",
    "    'Missing Values': missing_values_after_imputation,\n",
    "    'Percentage Missing': percentage_missing\n",
    "})\n",
    "\n",
    "\n",
    "print(missing_data_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3301f6e2",
   "metadata": {},
   "source": [
    "# Check for Symmetry and skewness to decide whether to use the mean, median or mode for imputing missing values in numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ad8c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the numerical columns\n",
    "numerical_columns = ['Max_DBP', 'Max_SBP', 'Min_DBP', 'Min_SBP']\n",
    "\n",
    "for column in numerical_columns:\n",
    "    # Calculate the column skewness\n",
    "    column_skewness = skew(data[column].dropna())\n",
    "    \n",
    "    # Plot the distribution and a boxplot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(data[column].dropna(), kde=True)\n",
    "    plt.title(f'Distribution of {column} (Skewness: {column_skewness:.2f})')\n",
    "    \n",
    "    # Boxplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=data[column])\n",
    "    plt.title(f'Boxplot of {column}')\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(f'{column}_distribution_boxplot.png')\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # Choose mean or median.\n",
    "    if abs(column_skewness) < 0.5:\n",
    "        print(f\"{column}: Distribution is fairly symmetric. Impute missing values with mean.\")\n",
    "    else:\n",
    "        print(f\"{column}: Distribution is skewed or has outliers. Impute missing values with median.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b570f",
   "metadata": {},
   "source": [
    "# Impute missing values for the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4da4c0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with median for skewed distributions or columns with outliers\n",
    "\n",
    "data['Max_DBP'].fillna(data['Max_DBP'].median(), inplace=True)\n",
    "data['Max_SBP'].fillna(data['Max_SBP'].median(), inplace=True)\n",
    "data['Min_SBP'].fillna(data['Min_SBP'].median(), inplace=True)\n",
    "data['Min_DBP'].fillna(data['Min_DBP'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4eb9555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PATID               0\n",
       "Sex                 0\n",
       "Race                7\n",
       "Marital_Status     14\n",
       "Max_DBP             0\n",
       "Max_SBP             0\n",
       "Min_DBP             0\n",
       "Min_SBP             0\n",
       "Comorbidities     813\n",
       "Smoking_Status    445\n",
       "Encounter Type      0\n",
       "Age_Grp             0\n",
       "Target              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of missing values in each column\n",
    "\n",
    "missing_values_after_imputation = data.isnull().sum()\n",
    "missing_values_after_imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87ad02",
   "metadata": {},
   "source": [
    "# Mapping and Encoding Diastolic Blood Pressure (DBP), and Systolic Blood Pressure (SBP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35be4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SBP and DBP categories with desired labels\n",
    "data['SBP_Category'] = pd.cut(data['Max_SBP'], bins=[-float('inf'), 120, 140, float('inf')], labels=['SBP <=120', 'SBP 120-140', 'SBP >=140'])\n",
    "data['DBP_Category'] = pd.cut(data['Max_DBP'], bins=[-float('inf'), 80, 90, float('inf')], labels=['DBP <=80', 'DBP 80-90', 'DBP >=90'])\n",
    "\n",
    "# Perform one-hot encoding with the correct names\n",
    "data = pd.get_dummies(data, columns=['SBP_Category', 'DBP_Category'])\n",
    "\n",
    "# Rename the columns to match the format\n",
    "data = data.rename(columns={\n",
    "    'SBP_Category_SBP <=120': 'SBP below 120',\n",
    "    'SBP_Category_SBP 120-140': 'SBP 120-140',\n",
    "    'SBP_Category_SBP >=140': 'SBP above 140',\n",
    "    'DBP_Category_DBP <=80': 'DBP below 80',\n",
    "    'DBP_Category_DBP 80-90': 'DBP 80-90',\n",
    "    'DBP_Category_DBP >=90': 'DBP above 90'\n",
    "})\n",
    "\n",
    "# Remove the original columns\n",
    "data = data.drop(columns=['Max_SBP', 'Min_SBP', 'Max_DBP', 'Min_DBP'])\n",
    "\n",
    "# Display the result\n",
    "print(data.head())\n",
    "\n",
    "# Check and print the count for each category\n",
    "print(\"Count for each SBP category:\")\n",
    "print(f\"SBP below 120: {data['SBP below 120'].sum()}\")\n",
    "print(f\"SBP 120-140: {data['SBP 120-140'].sum()}\")\n",
    "print(f\"SBP above 140: {data['SBP above 140'].sum()}\")\n",
    "\n",
    "print(\"\\nCount for each DBP category:\")\n",
    "print(f\"DBP below 80: {data['DBP below 80'].sum()}\")\n",
    "print(f\"DBP 80-90: {data['DBP 80-90'].sum()}\")\n",
    "print(f\"DBP above 90: {data['DBP above 90'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23fd2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "percentage_missing = (missing_values / len(data)) * 100\n",
    "\n",
    "# Print the number of missing values and their percentage\n",
    "missing_data_summary = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage Missing': percentage_missing\n",
    "})\n",
    "\n",
    "\n",
    "print(missing_data_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52643c67",
   "metadata": {},
   "source": [
    "# Mapping and Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0fa757",
   "metadata": {},
   "source": [
    "# 1. Encoding 'Age' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa322690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding on the 'Age_Grp' column\n",
    "age_group_dummies = pd.get_dummies(data['Age_Grp'], prefix='Age_Grp')\n",
    "\n",
    "# Concatenate the original DataFrame with the new one hot encoded columns\n",
    "data = pd.concat([data, age_group_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e7005a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f91716",
   "metadata": {},
   "source": [
    "# Descriptive Statistics of 'Age Group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68140c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of each age group\n",
    "age_group_counts = data['Age_Grp'].value_counts().sort_index()\n",
    "\n",
    "# Percentage of each age group\n",
    "age_group_percentages = data['Age_Grp'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "# Create a DataFrame to display counts and percentages together\n",
    "age_group_stats = pd.DataFrame({'Count': age_group_counts,'Percentage': age_group_percentages})\n",
    "\n",
    "# Display the descriptive statistics\n",
    "print(age_group_stats)\n",
    "\n",
    "# Visualization for Age Group\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Age_Grp', data=data, color=  'blue', order=sorted(data['Age_Grp'].unique()))\n",
    "plt.title('Distribution of Age Group')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('age_group_distribution.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf255aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'Age_Group' column\n",
    "data = data.drop(columns=['Age_Grp'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(data.head())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54628456",
   "metadata": {},
   "source": [
    "# 2. Imputation, Mapping and Encoding for 'Smoking_Status' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3fe866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value counts for 'Smoking_Status' column:\n",
      "Never smoker                                51837\n",
      "Former smoker quit longer than 12 months    36157\n",
      "Current every day smoker                    31363\n",
      "Current some day smoker                      1979\n",
      "Former smoker quit within 12 months          1478\n",
      "NaN                                           445\n",
      "Light tobacco smoker                          293\n",
      "Heavy tobacco smoker                          121\n",
      "Smoker ###                                     62\n",
      "Name: Smoking_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print initial count of Smoking_Status\n",
    "\n",
    "initial_count = data['Smoking_Status'].value_counts(dropna=False)\n",
    "print(\"Initial value counts for 'Smoking_Status' column:\")\n",
    "print(initial_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43c77615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for 'Smoking_Status' column after imputation:\n",
      "Never smoker                                52282\n",
      "Former smoker quit longer than 12 months    36157\n",
      "Current every day smoker                    31363\n",
      "Current some day smoker                      1979\n",
      "Former smoker quit within 12 months          1478\n",
      "Light tobacco smoker                          293\n",
      "Heavy tobacco smoker                          121\n",
      "Smoker ###                                     62\n",
      "Name: Smoking_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values (with most frequent value) in the 'Smoking_Status' column\n",
    "\n",
    "data['Smoking_Status'].fillna(data['Smoking_Status'].mode()[0], inplace=True)\n",
    "\n",
    "# Verify the changes after imputation\n",
    "\n",
    "print(\"\\nValue counts for 'Smoking_Status' column after imputation:\")\n",
    "print(data['Smoking_Status'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6c0ce",
   "metadata": {},
   "source": [
    "# Define the Mapping Function of Smoking_Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4872ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Normalize and clean the string values in 'Smoking_Status'\n",
    "\n",
    "data['Smoking_Status'] = data['Smoking_Status'].str.replace('#', '').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9bf153e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for 'Smoking_Status' column:\n",
      "Never smoker                                52282\n",
      "Former smoker quit longer than 12 months    36157\n",
      "Current every day smoker                    31363\n",
      "Current some day smoker                      1979\n",
      "Former smoker quit within 12 months          1478\n",
      "Light tobacco smoker                          293\n",
      "Heavy tobacco smoker                          121\n",
      "Smoker                                         62\n",
      "Name: Smoking_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify the changes \n",
    "\n",
    "print(\"\\nValue counts for 'Smoking_Status' column:\")\n",
    "print(data['Smoking_Status'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18b23b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for encoding\n",
    "\n",
    "smoking_status_mapping = {\n",
    "    'Never smoker': 0,\n",
    "    'Former smoker quit longer than 12 months': 1,\n",
    "    'Former smoker quit within 12 months': 1,\n",
    "    'Light tobacco smoker': 1,\n",
    "    'Current some day smoker': 1,\n",
    "    'Current every day smoker': 1,\n",
    "    'Smoker': 1,\n",
    "    'Heavy tobacco smoker': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38ce6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Smoking_Status'] = data['Smoking_Status'].map(smoking_status_mapping)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b32c044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    71453\n",
      "0    52282\n",
      "Name: Smoking_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the counts of each category after encoding\n",
    "\n",
    "encoded_counts = data['Smoking_Status'].value_counts()\n",
    "\n",
    "print(encoded_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d44515ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7592ffd3",
   "metadata": {},
   "source": [
    "# 3. Mapping and Encoding 'Sex' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9913dd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'F']\n",
      "  Sex\n",
      "0   M\n",
      "1   F\n",
      "2   F\n",
      "3   M\n",
      "4   F\n",
      "F    66487\n",
      "M    57248\n",
      "Name: Sex, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# List unique 'Sex' values\n",
    "\n",
    "unique_Sex_Status = data['Sex'].unique()\n",
    "print(unique_Sex_Status)\n",
    "\n",
    "# Verify the unique values\n",
    "print(data[['Sex']].head())\n",
    "print(data['Sex'].value_counts(dropna=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2fda38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to the 'Sex' column\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Sex'], dummy_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5c15f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the info of the one-hot encoded DataFrame\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "31809e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    66487\n",
      "1    57248\n",
      "Name: Sex_M, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the counts of each category after encoding\n",
    "encoded_counts = data['Sex_F'].value_counts()\n",
    "encoded_counts = data['Sex_M'].value_counts()\n",
    "\n",
    "print(encoded_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa57d54",
   "metadata": {},
   "source": [
    "# 4. Imputation, Mapping and Encoding for 'Race' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4d4a0a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHITE                                      113346\n",
      "BLACK OR AFRICAN AMERICAN                    7458\n",
      "SOME OTHER RACE                              1127\n",
      "ASIAN                                        1077\n",
      "Unknown                                       460\n",
      "AMERICAN INDIAN OR ALASKAN NATIVE             211\n",
      "NATIVE HAWAIIAN OR OTHER PACIFIC ISLAND        49\n",
      "NaN                                             7\n",
      "Name: Race, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in the 'Race' column\n",
    "\n",
    "print(data['Race'].value_counts(dropna=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0398e64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for 'Race' column after replacing 'Unknown' with NaN:\n",
      "WHITE                                      113346\n",
      "BLACK OR AFRICAN AMERICAN                    7458\n",
      "SOME OTHER RACE                              1127\n",
      "ASIAN                                        1077\n",
      "NaN                                           467\n",
      "AMERICAN INDIAN OR ALASKAN NATIVE             211\n",
      "NATIVE HAWAIIAN OR OTHER PACIFIC ISLAND        49\n",
      "Name: Race, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Unknown' with NaN\n",
    "data['Race'] = data['Race'].replace('Unknown', np.nan)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Value counts for 'Race' column after replacing 'Unknown' with NaN:\")\n",
    "print(data['Race'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ffb48e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Missing Values  Percentage Missing\n",
      "PATID                        0            0.000000\n",
      "Race                       467            0.377419\n",
      "Marital_Status              14            0.011315\n",
      "Comorbidities              813            0.657049\n",
      "Smoking_Status               0            0.000000\n",
      "Encounter Type               0            0.000000\n",
      "Target                       0            0.000000\n",
      "SBP below 120                0            0.000000\n",
      "SBP 120-140                  0            0.000000\n",
      "SBP above 140                0            0.000000\n",
      "DBP below 80                 0            0.000000\n",
      "DBP 80-90                    0            0.000000\n",
      "DBP above 90                 0            0.000000\n",
      "Age_Grp_50-60                0            0.000000\n",
      "Age_Grp_60-70                0            0.000000\n",
      "Age_Grp_70-80                0            0.000000\n",
      "Age_Grp_80-90                0            0.000000\n",
      "Age_Grp_90-100               0            0.000000\n",
      "Sex_F                        0            0.000000\n",
      "Sex_M                        0            0.000000\n"
     ]
    }
   ],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "percentage_missing = (missing_values / len(data)) * 100\n",
    "\n",
    "# Print the number of missing values and their percentage\n",
    "missing_data_summary = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage Missing': percentage_missing\n",
    "})\n",
    "\n",
    "print(missing_data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7acdfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a2521a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for 'Race' column after imputation:\n",
      "WHITE                                      113813\n",
      "BLACK OR AFRICAN AMERICAN                    7458\n",
      "SOME OTHER RACE                              1127\n",
      "ASIAN                                        1077\n",
      "AMERICAN INDIAN OR ALASKAN NATIVE             211\n",
      "NATIVE HAWAIIAN OR OTHER PACIFIC ISLAND        49\n",
      "Name: Race, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values (with most frequent value) in the 'Race' column\n",
    "\n",
    "data['Race'].fillna(data['Race'].mode()[0], inplace=True)  \n",
    "\n",
    "# Verify the changes after imputation\n",
    "print(\"\\nValue counts for 'Race' column after imputation:\")\n",
    "print(data['Race'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27a826",
   "metadata": {},
   "source": [
    "# Define the Mapping Function for 'Race'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4934aca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white                                      113813\n",
      "black or african american                    7458\n",
      "some other race                              1127\n",
      "asian                                        1077\n",
      "american indian or alaskan native             211\n",
      "native hawaiian or other pacific island        49\n",
      "Name: Race_Grouped, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Normalize and clean the string and Define the mapping function for 'Race'\n",
    "# Treat any other unknown or unexpected values as Unknown\n",
    "\n",
    "def map_race(status):\n",
    "    status = status.lower().strip() \n",
    "    \n",
    "    if status in ['white', \n",
    "                  'black or african american', \n",
    "                  'some other race',\n",
    "                  'asian', \n",
    "                  'american indian or alaskan native', \n",
    "                  'native hawaiian or other pacific island']:\n",
    "        return status\n",
    "    else:\n",
    "        return 'unknown'  \n",
    "\n",
    "\n",
    "data['Race_Grouped'] = data['Race'].apply(map_race)\n",
    "\n",
    "print(data['Race_Grouped'].value_counts(dropna=False))\n",
    "\n",
    "# One-hot encoding on the 'Race_Grouped' column\n",
    "data = pd.get_dummies(data, columns=['Race_Grouped'], prefix='Race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "297e0f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values and their counts in the 'Race' column after mapping\n",
    "\n",
    "race_counts = data.filter(like='Race_').sum()\n",
    "print(race_counts)\n",
    "\n",
    "# Display the dataframe to verify encoding\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbc9c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping old column names to new column names\n",
    "\n",
    "new_column_names = {\n",
    "    'Race_american indian or alaskan native': 'AMERICAN_IND/ALASKAN',\n",
    "    'ace_asian': 'ASIAN',\n",
    "    'Race_black or african american': 'BLACK/AFRIC_AMERICAN',\n",
    "    'Race_native hawaiian or other pacific island': 'NAT_HAWAIIN',\n",
    "    'Race_some other race': 'OTHER',\n",
    "    'Race_white': 'WHITE',\n",
    "}\n",
    "\n",
    "# Rename the columns \n",
    "data.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "# Drop the original 'Race' column as it has been encoded\n",
    "data = data.drop(columns=['Race'], errors='ignore')\n",
    "\n",
    "# Verify the changes\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50eb5d3",
   "metadata": {},
   "source": [
    "# 5. Imputation, Mapping and Encoding 'Marital_Status' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "41249386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Married' 'Widowed' 'Single' 'Divorced' 'Unknown' 'Separated' nan\n",
      " 'Life Partner']\n",
      "Married         70922\n",
      "Single          20982\n",
      "Divorced        15079\n",
      "Widowed         13628\n",
      "Unknown          1529\n",
      "Separated        1502\n",
      "Life Partner       79\n",
      "NaN                14\n",
      "Name: Marital_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Unique values in the 'Marital_Status' column\n",
    "unique_Marital_Status = data['Marital_Status'].unique()\n",
    "print(unique_Marital_Status)\n",
    "\n",
    "# Verify the unique values\n",
    "print(data['Marital_Status'].value_counts(dropna=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "af59bd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for 'Marital_Status' column after replacing 'Unknown' with NaN:\n",
      "Married         70922\n",
      "Single          20982\n",
      "Divorced        15079\n",
      "Widowed         13628\n",
      "NaN              1543\n",
      "Separated        1502\n",
      "Life Partner       79\n",
      "Name: Marital_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Unknown' with NaN in the 'Marital_Status' column\n",
    "data['Marital_Status'] = data['Marital_Status'].replace('Unknown', np.nan)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Value counts for 'Marital_Status' column after replacing 'Unknown' with NaN:\")\n",
    "print(data['Marital_Status'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0a1c4013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for 'Marital_Status' column after imputation:\n",
      "Married         72465\n",
      "Single          20982\n",
      "Divorced        15079\n",
      "Widowed         13628\n",
      "Separated        1502\n",
      "Life Partner       79\n",
      "Name: Marital_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values in the 'Marital_Status' column\n",
    "\n",
    "mode_value = data['Marital_Status'].mode()[0]\n",
    "data['Marital_Status'].fillna(mode_value, inplace=True)\n",
    "\n",
    "# Verify the changes after imputation\n",
    "print(\"\\nValue counts for 'Marital_Status' column after imputation:\")\n",
    "print(data['Marital_Status'].value_counts(dropna=False))\n",
    "\n",
    "# One-hot encoding on the 'Marital_Status' column\n",
    "data = pd.get_dummies(data, columns=['Marital_Status'], prefix='Marital_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ff4fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding created new columns for each marital status value. We'll use a dictionary to map old column names to the new ones.\n",
    "\n",
    "new_column_names = {\n",
    "    'Marital_Status_Divorced': 'Divorced',\n",
    "    'Marital_Status_Life Partner': 'Life Partner',\n",
    "    'Marital_Status_Married': 'Married',\n",
    "    'Marital_Status_Separated': 'Separated',\n",
    "    'Marital_Status_Single': 'Single',\n",
    "    'Marital_Status_Widowed': 'Widowed'\n",
    "}\n",
    "\n",
    "# Rename the columns using the rename method\n",
    "data.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "\n",
    "# Verify the changes\n",
    "print(data.columns)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb92566",
   "metadata": {},
   "source": [
    "# 6. Mapping and Encoding 'Encounter Type' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f39b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the value counts for the 'Encounter Type' column\n",
    "\n",
    "encounter_type_counts = data['Encounter Type'].value_counts()\n",
    "print(\"Encounter Type Counts:\")\n",
    "print(encounter_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8a9daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'Encounter Type' values into separate columns\n",
    "split_encounters = data['Encounter Type'].str.get_dummies(sep=',')\n",
    "\n",
    "# Concatenate the original DataFrame with the new one hot encoded columns\n",
    "data = pd.concat([data, split_encounters], axis=1)\n",
    "\n",
    "# Drop the original 'Encounter Type' column\n",
    "data = data.drop(columns=['Encounter Type'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"DataFrame with One Hot Encoding for 'Encounter Type':\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9cc1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the one hot encoded columns\n",
    "\n",
    "encoded_columns = split_encounters.columns\n",
    "print(\"One Hot Encoded Columns for 'Encounter Type':\")\n",
    "print(data[encoded_columns].head())\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d55694",
   "metadata": {},
   "source": [
    "# Rename the columns 'EMERGENCY' to 'EcType_ED', 'INPATIENT' to 'EcType_IP', and 'OUTPATIENT' to 'EcType_AV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11d7e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the specified columns\n",
    "\n",
    "data = data.rename(columns={\n",
    "    'EMERGENCY': 'EcType_ED',\n",
    "    'INPATIENT': 'EcType_IP',\n",
    "    'OUTPATIENT': 'EcType_AV'\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(data.info())\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177d212",
   "metadata": {},
   "source": [
    "# 7. Imputing, Mapping and Encoding for 'Comorbidities' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e333764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of missing values in each column\n",
    "\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75e273c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in the 'Comorbidities' column\n",
    "\n",
    "unique_Comorbidities = data['Comorbidities'].unique()\n",
    "print(unique_Comorbidities)\n",
    "\n",
    "# Verify the new columns and their unique values\n",
    "# print(data[['Comorbidities']].head())\n",
    "\n",
    "print(data['Comorbidities'].value_counts(dropna=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2999d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of specific diseases to encode\n",
    "\n",
    "diseases_to_encode = [\n",
    "    'Diabetes', 'Type 2 Diabetes Mellitus', 'Epilepsy', 'Depression', 'Obesity', 'Stroke', 'Anxiety', 'Hypertension',\n",
    "    'Hyperlipidemia', 'Cardiovascular Disease', 'Sleep Disorder', 'Headache', 'Periodontitis', 'Concussion',\n",
    "    'Heart Disease', 'Sleep Apnea', 'Insomnia', 'Kidney Disease', 'Cholesterol', 'Vitamin D Deficiency',\n",
    "    'Enlarge Prostate', 'Osteoporosis', 'Bone Disease', 'Depressive Disorder'\n",
    "]\n",
    "\n",
    "# Define the function to create a new column for each disease\n",
    "def map_comorbidities(comorbidities, disease):\n",
    "    if pd.isna(comorbidities):\n",
    "        return 0\n",
    "    return 1 if disease.lower() in comorbidities.lower() else 0\n",
    "\n",
    "# Create new columns for each disease\n",
    "for disease in diseases_to_encode:\n",
    "    data[disease.replace(' ', '_')] = data['Comorbidities'].apply(lambda x: map_comorbidities(x, disease))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7c46b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the count of each newly created disease column\n",
    "\n",
    "for disease in diseases_to_encode:\n",
    "    column_name = disease.replace(' ', '_')\n",
    "    count = data[column_name].sum()\n",
    "    print(f\"Count of {disease}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21a415a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe to verify encoding\n",
    "\n",
    "print(data.head())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaf0dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the changes\n",
    "print(data.head())\n",
    "\n",
    "# Drop the original 'Comorbidities' column after encoding as it's not needed. Also, we want to drop 'Osteoporosis'\n",
    "data.drop(columns=['Comorbidities', 'Osteoporosis'], inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c409c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Target' is our output variable. Place the 'Target' column to the end\n",
    "\n",
    "target_col = data.pop('Target')\n",
    "data['Target'] = target_col\n",
    "\n",
    "# Print the DataFrame info to verify the changes\n",
    "print(data.info())\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898b8ee",
   "metadata": {},
   "source": [
    "# Save the Preprocessing DataFrame to a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1e1f4173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file for ML analysis\n",
    "\n",
    "data.to_csv('Combined_ML_5Yrs_ML_Analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8c78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
