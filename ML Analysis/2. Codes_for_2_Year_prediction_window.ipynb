{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bda6004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "import shap\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dedd113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATID</th>\n",
       "      <th>Smoking_Status</th>\n",
       "      <th>SBP below 120</th>\n",
       "      <th>SBP 120-140</th>\n",
       "      <th>SBP above 140</th>\n",
       "      <th>DBP below 80</th>\n",
       "      <th>DBP 80-90</th>\n",
       "      <th>DBP above 90</th>\n",
       "      <th>Age_Grp_50-60</th>\n",
       "      <th>Age_Grp_60-70</th>\n",
       "      <th>...</th>\n",
       "      <th>Heart_Disease</th>\n",
       "      <th>Sleep_Apnea</th>\n",
       "      <th>Insomnia</th>\n",
       "      <th>Kidney_Disease</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Vitamin_D_Deficiency</th>\n",
       "      <th>Enlarge_Prostate</th>\n",
       "      <th>Bone_Disease</th>\n",
       "      <th>Depressive_Disorder</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122813</th>\n",
       "      <td>2633300</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122814</th>\n",
       "      <td>1575030</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122815</th>\n",
       "      <td>2940185</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122816</th>\n",
       "      <td>1210750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122817</th>\n",
       "      <td>2430126</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PATID  Smoking_Status  SBP below 120  SBP 120-140  SBP above 140  \\\n",
       "122813  2633300               5              0            0              1   \n",
       "122814  1575030               1              0            0              1   \n",
       "122815  2940185               1              0            0              1   \n",
       "122816  1210750               0              0            0              1   \n",
       "122817  2430126               1              0            0              1   \n",
       "\n",
       "        DBP below 80  DBP 80-90  DBP above 90  Age_Grp_50-60  Age_Grp_60-70  \\\n",
       "122813             0          0             1              0              1   \n",
       "122814             0          0             1              0              0   \n",
       "122815             0          0             1              0              0   \n",
       "122816             0          0             1              0              0   \n",
       "122817             0          0             1              0              0   \n",
       "\n",
       "        ...  Heart_Disease  Sleep_Apnea  Insomnia  Kidney_Disease  \\\n",
       "122813  ...              1            0         0               0   \n",
       "122814  ...              0            0         0               0   \n",
       "122815  ...              0            0         0               0   \n",
       "122816  ...              0            1         0               0   \n",
       "122817  ...              0            0         0               0   \n",
       "\n",
       "        Cholesterol  Vitamin_D_Deficiency  Enlarge_Prostate  Bone_Disease  \\\n",
       "122813            0                     0                 0             0   \n",
       "122814            0                     0                 0             0   \n",
       "122815            0                     0                 0             0   \n",
       "122816            0                     0                 0             0   \n",
       "122817            0                     0                 0             0   \n",
       "\n",
       "        Depressive_Disorder  Target  \n",
       "122813                    1       0  \n",
       "122814                    0       0  \n",
       "122815                    1       0  \n",
       "122816                    1       0  \n",
       "122817                    1       0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load this file into a Python Dataframe\n",
    "\n",
    "df = pd.read_csv('2.Preprocessing_Combined_data_2_Years_github_JPAD-2025.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897ce813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bf60f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target\n",
      "0    119723\n",
      "1      3095\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking counts of 'Target' column\n",
    "df['Target'].value_counts()\n",
    "print(df['Target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e8ce65d-928c-47d3-b2a0-298bf6ade58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value counts for 'Age_Grp_90-100':\n",
      "Age_Grp_90-100\n",
      "0    107733\n",
      "1     15085\n",
      "Name: count, dtype: int64\n",
      "Initial value counts for 'Age_Grp_80-90':\n",
      "Age_Grp_80-90\n",
      "0    94857\n",
      "1    27961\n",
      "Name: count, dtype: int64\n",
      "Initial value counts for 'Age_Grp_70-80':\n",
      "Age_Grp_70-80\n",
      "0    83220\n",
      "1    39598\n",
      "Name: count, dtype: int64\n",
      "Initial value counts for 'Age_Grp_60-70':\n",
      "Age_Grp_60-70\n",
      "0    86151\n",
      "1    36667\n",
      "Name: count, dtype: int64\n",
      "Initial value counts for 'Age_Grp_50-60':\n",
      "Age_Grp_50-60\n",
      "0    119311\n",
      "1      3507\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# List of age group columns\n",
    "age_groups = [\n",
    "    'Age_Grp_90-100', 'Age_Grp_80-90', 'Age_Grp_70-80',\n",
    "    'Age_Grp_60-70', 'Age_Grp_50-60'\n",
    "]\n",
    "\n",
    "# Print initial counts for all age groups\n",
    "for age_group in age_groups:\n",
    "    initial_count = df[age_group].value_counts(dropna=False)\n",
    "    print(f\"Initial value counts for '{age_group}':\")\n",
    "    print(initial_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416a338c",
   "metadata": {},
   "source": [
    "# Drop the 'PATID' column as not relavant to our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "956bee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['PATID', 'EcType_ED', 'EcType_IP', 'EcType_AV'], inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f13c2",
   "metadata": {},
   "source": [
    "# Splitting data into feature and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd931950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted variable is 'Target'\n",
    "X = df.drop(\"Target\", axis=1)\n",
    "print(X)\n",
    "y = df[\"Target\"]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c733cce",
   "metadata": {},
   "source": [
    "# Split data into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4992f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# describes info about train and test set\n",
    "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "print(\"Number transactions y_test dataset: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb645538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts in y_test: {0: 23907, 1: 657}\n"
     ]
    }
   ],
   "source": [
    "# Check count of Target variables\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "test_class_counts = dict(zip(unique, counts))\n",
    "\n",
    "print(\"Class counts in y_test:\", test_class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e779e120",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression Classifier (LR) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74a2f307-3370-4d75-b665-9eb0fabb10af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for Grid Search - Logistic Regression (LR)\n",
    "param_grid_lr = {\n",
    "    'logreg__C': [100], \n",
    "    'logreg__class_weight': ['balanced'],  \n",
    "    'logreg__fit_intercept': [True],  \n",
    "    'logreg__max_iter': [200],  \n",
    "    'logreg__solver': ['lbfgs'] \n",
    "}\n",
    "\n",
    "# Define the model and the pipeline\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "pipeline_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', logreg)\n",
    "])\n",
    "\n",
    "# Inner cross-validation loop for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the grid search\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator=pipeline_lr, \n",
    "    param_grid=param_grid_lr, \n",
    "    scoring='roc_auc', \n",
    "    cv=inner_cv, \n",
    "    verbose=2, \n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Outer cross-validation loop for model evaluation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "specificity_scores = []\n",
    "conf_matrices = []\n",
    "best_params_list = []  \n",
    "\n",
    "# Nested cross-validation for Logistic Regression\n",
    "for train_index, test_index in outer_cv.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Grid search on the inner cross-validation loop\n",
    "    grid_search_lr.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Best estimator\n",
    "    best_model_lr = grid_search_lr.best_estimator_\n",
    "\n",
    "    # Best parameters\n",
    "    best_params_list.append(grid_search_lr.best_params_)\n",
    "    print(\"Best Parameters:\", best_params_list)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_probs = best_model_lr.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Threshold to Classify\n",
    "    threshold = 0.5\n",
    "    y_pred = (y_probs > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "    precision_scores.append(precision_score(y_val_fold, y_pred))\n",
    "    recall_scores.append(recall_score(y_val_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(y_val_fold, y_probs))\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_val_fold, y_pred)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "    \n",
    "    TN = conf_matrix[0, 0]\n",
    "    FP = conf_matrix[0, 1]\n",
    "    specificity = TN / (TN + FP)\n",
    "    specificity_scores.append(specificity)\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "print(f\"Mean Specificity: {np.mean(specificity_scores):.4f}\")\n",
    "\n",
    "# Confusion matrices for each fold\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    print(f\"Confusion Matrix for Fold {i+1}:\\n{cm}\")\n",
    "\n",
    "# Evaluation on the test set with the best model from nested CV\n",
    "best_model_lr.fit(X_train, y_train)\n",
    "y_probs_test = best_model_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Threshold to Classify\n",
    "threshold = 0.5\n",
    "y_pred_test = (y_probs_test > threshold).astype(int)\n",
    "\n",
    "# Calculate test set metrics\n",
    "test_accuracy_lr = accuracy_score(y_test, y_pred_test)\n",
    "test_precision_lr = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall_lr = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_lr = f1_score(y_test, y_pred_test, average='weighted')\n",
    "test_roc_auc_lr = roc_auc_score(y_test, y_probs_test)\n",
    "test_conf_matrix_lr = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Calculate PPV, NPV, and specificity\n",
    "TP = test_conf_matrix_lr[1, 1]\n",
    "TN = test_conf_matrix_lr[0, 0]\n",
    "FP = test_conf_matrix_lr[0, 1]\n",
    "FN = test_conf_matrix_lr[1, 0]\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Accuracy: {test_accuracy_lr:.4f}\")\n",
    "print(f\"Test Precision: {test_precision_lr:.4f}\")\n",
    "print(f\"Test Recall: {test_recall_lr:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1_lr:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc_lr:.4f}\")\n",
    "print(f\"Test Confusion Matrix_LR Model:\\n{test_conf_matrix_lr}\")\n",
    "print(f\"Test Positive Predictive Value (PPV): {PPV:.4f}\")\n",
    "print(f\"Test Negative Predictive Value (NPV): {NPV:.4f}\")\n",
    "print(f\"Test Specificity: {specificity:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix of the test set\n",
    "sns.heatmap(test_conf_matrix_lr, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.ylabel('Actual/True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix_LR Model - Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the 95% confidence intervals (CIs) for each evaluation metric using bootstrapping\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "accuracy_scores_boot = []\n",
    "precision_scores_boot = []\n",
    "recall_scores_boot = []\n",
    "f1_scores_boot = []\n",
    "roc_auc_scores_boot = []\n",
    "ppv_scores_boot = []\n",
    "npv_scores_boot = []\n",
    "specificity_scores_boot = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(y_test), len(y_test))\n",
    "    if len(np.unique(y_test.iloc[indices])) < 2:\n",
    "        continue\n",
    "    \n",
    "    y_test_boot = y_test.iloc[indices]\n",
    "    y_pred_test_boot = y_pred_test[indices]\n",
    "    y_probs_test_boot = y_probs_test[indices]\n",
    "    \n",
    "    accuracy_scores_boot.append(accuracy_score(y_test_boot, y_pred_test_boot))\n",
    "    precision_scores_boot.append(precision_score(y_test_boot, y_pred_test_boot))\n",
    "    recall_scores_boot.append(recall_score(y_test_boot, y_pred_test_boot))\n",
    "    f1_scores_boot.append(f1_score(y_test_boot, y_pred_test_boot))\n",
    "    roc_auc_scores_boot.append(roc_auc_score(y_test_boot, y_probs_test_boot))\n",
    "    \n",
    "    cm_boot = confusion_matrix(y_test_boot, y_pred_test_boot)\n",
    "    TP_boot = cm_boot[1, 1]\n",
    "    TN_boot = cm_boot[0, 0]\n",
    "    FP_boot = cm_boot[0, 1]\n",
    "    FN_boot = cm_boot[1, 0]\n",
    "    ppv_scores_boot.append(TP_boot / (TP_boot + FP_boot) if (TP_boot + FP_boot) > 0 else 0)\n",
    "    npv_scores_boot.append(TN_boot / (TN_boot + FN_boot) if (TN_boot + FN_boot) > 0 else 0)\n",
    "    specificity_scores_boot.append(TN_boot / (TN_boot + FP_boot) if (TN_boot + FP_boot) > 0 else 0)\n",
    "\n",
    "def bootstrap_confidence_interval(data, alpha=0.05):\n",
    "    sorted_data = np.sort(data)\n",
    "    lower_bound = np.percentile(sorted_data, 100 * (alpha / 2))\n",
    "    upper_bound = np.percentile(sorted_data, 100 * (1 - alpha / 2))\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Calculate mean values\n",
    "mean_accuracy = np.mean(accuracy_scores_boot)\n",
    "mean_precision = np.mean(precision_scores_boot)\n",
    "mean_recall = np.mean(recall_scores_boot)\n",
    "mean_f1 = np.mean(f1_scores_boot)\n",
    "mean_roc_auc = np.mean(roc_auc_scores_boot)\n",
    "mean_ppv = np.mean(ppv_scores_boot)\n",
    "mean_npv = np.mean(npv_scores_boot)\n",
    "mean_specificity = np.mean(specificity_scores_boot)\n",
    "\n",
    "# Print confidence intervals\n",
    "print(f\"95% CI for Accuracy: {bootstrap_confidence_interval(accuracy_scores_boot)}\")\n",
    "print(f\"95% CI for Precision: {bootstrap_confidence_interval(precision_scores_boot)}\")\n",
    "print(f\"95% CI for Recall: {bootstrap_confidence_interval(recall_scores_boot)}\")\n",
    "print(f\"95% CI for F1 Score: {bootstrap_confidence_interval(f1_scores_boot)}\")\n",
    "print(f\"95% CI for ROC AUC: {bootstrap_confidence_interval(roc_auc_scores_boot)}\")\n",
    "print(f\"95% CI for PPV: {bootstrap_confidence_interval(ppv_scores_boot)}\")\n",
    "print(f\"95% CI for NPV: {bootstrap_confidence_interval(npv_scores_boot)}\")\n",
    "print(f\"95% CI for Specificity: {bootstrap_confidence_interval(specificity_scores_boot)}\")\n",
    "\n",
    "# Plot the AUC-ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC-ROC (Mean ROC AUC = {mean_roc_auc:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC-ROC Curve Logistic Regression Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b69d7fc-7630-4cc3-9dc9-856c233f41d0",
   "metadata": {},
   "source": [
    "# 2. Gradient Boosting Tree Classifier (GBT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56df1627-328d-4c64-8d3b-5ce115586ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Define the parameter grid for Grid Search - Gradient Boosting Tree (GBT)\n",
    "param_grid_gbt = {\n",
    "    'gbt__n_estimators': [490],\n",
    "    'gbt__learning_rate': [0.1],\n",
    "    'gbt__max_depth': [4],\n",
    "    'gbt__min_samples_split': [5],\n",
    "    'gbt__min_samples_leaf': [4],\n",
    "    'gbt__subsample': [1.0],\n",
    "    'gbt__max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "# Define the model and the pipeline\n",
    "gbt = GradientBoostingClassifier(random_state=42)\n",
    "pipeline_gbt = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gbt', gbt)\n",
    "])\n",
    "\n",
    "# Inner cross-validation loop for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the grid search\n",
    "grid_search_gbt = GridSearchCV(\n",
    "    estimator=pipeline_gbt, \n",
    "    param_grid=param_grid_gbt, \n",
    "    scoring='roc_auc', \n",
    "    cv=inner_cv, \n",
    "    verbose=2, \n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Outer cross-validation loop for model evaluation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "specificity_scores = []\n",
    "conf_matrices = []\n",
    "best_params_list = []  \n",
    "\n",
    "# Nested cross-validation for GBT\n",
    "for train_index, test_index in outer_cv.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Grid search on the inner cross-validation loop\n",
    "    grid_search_gbt.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Best estimator\n",
    "    best_model_gbt = grid_search_gbt.best_estimator_\n",
    "    \n",
    "    # Best parameters\n",
    "    best_params_list.append(grid_search_gbt.best_params_)\n",
    "    print(\"Best Parameters:\", best_params_list)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_probs = best_model_gbt.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "    # Threshold to Classify\n",
    "    threshold = 0.5\n",
    "    y_pred = (y_probs > threshold).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "    precision_scores.append(precision_score(y_val_fold, y_pred))\n",
    "    recall_scores.append(recall_score(y_val_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(y_val_fold, y_probs))\n",
    "    conf_matrices.append(confusion_matrix(y_val_fold, y_pred))\n",
    "    \n",
    "    # Calculate specificity for the current fold\n",
    "    TN = conf_matrices[-1][0, 0]\n",
    "    FP = conf_matrices[-1][0, 1]\n",
    "    specificity = TN / (TN + FP)\n",
    "    specificity_scores.append(specificity)\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "print(f\"Mean Specificity: {np.mean(specificity_scores):.4f}\")\n",
    "\n",
    "# Confusion matrices for each fold\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    print(f\"Confusion Matrix for Fold {i+1}:\\n{cm}\")\n",
    "\n",
    "# Evaluation on the test set with the best model from nested CV\n",
    "best_model_gbt.fit(X_train, y_train)\n",
    "y_probs_test = best_model_gbt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Use a Threshold to Classify\n",
    "threshold = 0.5\n",
    "y_pred_test = (y_probs_test > threshold).astype(int)\n",
    "\n",
    "# Calculate test set metrics\n",
    "test_accuracy_gbt = accuracy_score(y_test, y_pred_test)\n",
    "test_precision_gbt = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall_gbt = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_gbt = f1_score(y_test, y_pred_test, average='weighted')\n",
    "test_roc_auc_gbt = roc_auc_score(y_test, y_probs_test)\n",
    "test_conf_matrix_gbt = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Calculate PPV, NPV, and specificity\n",
    "TP = test_conf_matrix_gbt[1, 1]\n",
    "TN = test_conf_matrix_gbt[0, 0]\n",
    "FP = test_conf_matrix_gbt[0, 1]\n",
    "FN = test_conf_matrix_gbt[1, 0]\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Accuracy: {test_accuracy_gbt:.4f}\")\n",
    "print(f\"Test Precision: {test_precision_gbt:.4f}\")\n",
    "print(f\"Test Recall: {test_recall_gbt:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1_gbt:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc_gbt:.4f}\")\n",
    "print(f\"Test Confusion Matrix_GBT Model:\\n{test_conf_matrix_gbt}\")\n",
    "print(f\"Test Positive Predictive Value (PPV): {PPV:.4f}\")\n",
    "print(f\"Test Negative Predictive Value (NPV): {NPV:.4f}\")\n",
    "print(f\"Test Specificity: {specificity:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix of the test set\n",
    "sns.heatmap(test_conf_matrix_gbt, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.ylabel('Actual/True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix_GBT Model - Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the 95% confidence intervals (CIs) for each evaluation metric using bootstrapping\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "accuracy_scores_boot = []\n",
    "precision_scores_boot = []\n",
    "recall_scores_boot = []\n",
    "f1_scores_boot = []\n",
    "roc_auc_scores_boot = []\n",
    "ppv_scores_boot = []\n",
    "npv_scores_boot = []\n",
    "specificity_scores_boot = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(y_test), len(y_test))\n",
    "    if len(np.unique(y_test.iloc[indices])) < 2:\n",
    "        continue\n",
    "    \n",
    "    y_test_boot = y_test.iloc[indices]\n",
    "    y_pred_test_boot = y_pred_test[indices]\n",
    "    y_probs_test_boot = y_probs_test[indices]\n",
    "    \n",
    "    accuracy_scores_boot.append(accuracy_score(y_test_boot, y_pred_test_boot))\n",
    "    precision_scores_boot.append(precision_score(y_test_boot, y_pred_test_boot))\n",
    "    recall_scores_boot.append(recall_score(y_test_boot, y_pred_test_boot))\n",
    "    f1_scores_boot.append(f1_score(y_test_boot, y_pred_test_boot))\n",
    "    roc_auc_scores_boot.append(roc_auc_score(y_test_boot, y_probs_test_boot))\n",
    "    \n",
    "    cm_boot = confusion_matrix(y_test_boot, y_pred_test_boot)\n",
    "    TP_boot = cm_boot[1, 1]\n",
    "    TN_boot = cm_boot[0, 0]\n",
    "    FP_boot = cm_boot[0, 1]\n",
    "    FN_boot = cm_boot[1, 0]\n",
    "    ppv_scores_boot.append(TP_boot / (TP_boot + FP_boot) if (TP_boot + FP_boot) > 0 else 0)\n",
    "    npv_scores_boot.append(TN_boot / (TN_boot + FN_boot) if (TN_boot + FN_boot) > 0 else 0)\n",
    "    specificity_scores_boot.append(TN_boot / (TN_boot + FP_boot) if (TN_boot + FP_boot) > 0 else 0)\n",
    "\n",
    "def bootstrap_confidence_interval(data, alpha=0.05):\n",
    "    sorted_data = np.sort(data)\n",
    "    lower_bound = np.percentile(sorted_data, 100 * (alpha / 2))\n",
    "    upper_bound = np.percentile(sorted_data, 100 * (1 - alpha / 2))\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Calculate mean values\n",
    "mean_accuracy = np.mean(accuracy_scores_boot)\n",
    "mean_precision = np.mean(precision_scores_boot)\n",
    "mean_recall = np.mean(recall_scores_boot)\n",
    "mean_f1 = np.mean(f1_scores_boot)\n",
    "mean_roc_auc = np.mean(roc_auc_scores_boot)\n",
    "mean_ppv = np.mean(ppv_scores_boot)\n",
    "mean_npv = np.mean(npv_scores_boot)\n",
    "mean_specificity = np.mean(specificity_scores_boot)\n",
    "\n",
    "# Print confidence intervals\n",
    "print(f\"95% CI for Accuracy: {bootstrap_confidence_interval(accuracy_scores_boot)}\")\n",
    "print(f\"95% CI for Precision: {bootstrap_confidence_interval(precision_scores_boot)}\")\n",
    "print(f\"95% CI for Recall: {bootstrap_confidence_interval(recall_scores_boot)}\")\n",
    "print(f\"95% CI for F1 Score: {bootstrap_confidence_interval(f1_scores_boot)}\")\n",
    "print(f\"95% CI for ROC AUC: {bootstrap_confidence_interval(roc_auc_scores_boot)}\")\n",
    "print(f\"95% CI for PPV: {bootstrap_confidence_interval(ppv_scores_boot)}\")\n",
    "print(f\"95% CI for NPV: {bootstrap_confidence_interval(npv_scores_boot)}\")\n",
    "print(f\"95% CI for Specificity: {bootstrap_confidence_interval(specificity_scores_boot)}\")\n",
    "\n",
    "# Plot the AUC-ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC-ROC (Mean ROC AUC = {mean_roc_auc:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC-ROC Curve GBT Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8cad4-c4a3-4043-a27b-7375c3b69dfe",
   "metadata": {},
   "source": [
    "# SHapley Additive exPlanations (SHAP) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27acb608-38ea-43b8-9d3d-87ddace3c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP explainer with the best model and the training dataset\n",
    "explainer = shap.Explainer(best_model_gbt.named_steps['gbt'], X_train)\n",
    "\n",
    "# Compute SHAP values for the test dataset\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Top indices based on mean absolute SHAP values\n",
    "def get_top_shap_indices(shap_values, top_n=15):\n",
    "    shap_sum = np.abs(shap_values.values).mean(axis=0)\n",
    "    \n",
    "    # Indices of the top features\n",
    "    top_indices = np.argsort(shap_sum)[-top_n:]\n",
    "    return top_indices\n",
    "\n",
    "# Get top 15 feature indices from SHAP values\n",
    "top_indices = get_top_shap_indices(shap_values, 15)\n",
    "\n",
    "# SHAP values for the top 15 features\n",
    "shap_values_top = shap_values[:, top_indices]\n",
    "\n",
    "if isinstance(X_test, pd.DataFrame):\n",
    "    X_test_top = X_test.iloc[:, top_indices]\n",
    "    feature_names = X_test.columns[top_indices]\n",
    "else:\n",
    "    X_test_top = X_test[:, top_indices]\n",
    "    feature_names = top_indices  \n",
    "\n",
    "# Create SHAP Explanation object\n",
    "shap_values_explanation = shap.Explanation(\n",
    "    values=shap_values_top,\n",
    "    base_values=shap_values.base_values,\n",
    "    data=X_test_top,\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "# Plot with the built-in SHAP function\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values_explanation, X_test_top, max_display=15, plot_type=\"dot\", show=False)\n",
    "plt.savefig('SHAP_Plot_2Years_GBT_Model.png', bbox_inches='tight')\n",
    "plt.close()  \n",
    "\n",
    "# Plot the bar plot with SHAP values\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values_explanation, X_test_top, max_display=15, plot_type=\"bar\", show=False)\n",
    "\n",
    "# Add values on top of bars with two decimal points\n",
    "shap_sum = np.abs(shap_values.values).mean(axis=0)\n",
    "top_indices_sorted = np.argsort(shap_sum)[-15:]\n",
    "sorted_shap_values = shap_sum[top_indices_sorted]\n",
    "sorted_feature_names = X_test.columns[top_indices_sorted]\n",
    "\n",
    "for i, (value, name) in enumerate(zip(sorted_shap_values, sorted_feature_names)):\n",
    "    plt.text(value, i, f'{value:.2f}', va='center')\n",
    "\n",
    "plt.savefig('SHAP_Plot_2Years_GBT_Model_Bar.png', bbox_inches='tight')\n",
    "plt.close()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82828b-9db5-4250-8caa-0902838682cf",
   "metadata": {},
   "source": [
    "# 3. Light GBM Classifier Model (Light GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc03dc8-58c7-4086-abc7-86846364c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for Grid Search - (Light GBM)\n",
    "param_grid_lgbm = {\n",
    "    'lgbm__n_estimators': [100, 200],\n",
    "    'lgbm__learning_rate': [0.01, 0.1],\n",
    "    'lgbm__max_depth': [5, 7],\n",
    "    'lgbm__num_leaves': [31, 40],\n",
    "    'lgbm__subsample': [0.8, 0.9],\n",
    "    'lgbm__colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Define the model and the pipeline\n",
    "lgbm = lgb.LGBMClassifier(random_state=42)\n",
    "pipeline_lgbm = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lgbm', lgbm)\n",
    "])\n",
    "\n",
    "# Inner cross-validation loop for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the grid search\n",
    "grid_search_lgbm = GridSearchCV(\n",
    "    estimator=pipeline_lgbm, \n",
    "    param_grid=param_grid_lgbm, \n",
    "    scoring='roc_auc', \n",
    "    cv=inner_cv, \n",
    "    verbose=2, \n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Outer cross-validation loop for model evaluation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "conf_matrices = []\n",
    "best_params_list = [] \n",
    "\n",
    "# Nested cross-validation for LightGBM\n",
    "for train_index, test_index in outer_cv.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Grid search on the inner cross-validation loop\n",
    "    grid_search_lgbm.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Best estimator\n",
    "    best_model_lgbm = grid_search_lgbm.best_estimator_\n",
    "    \n",
    "    # Best parameters\n",
    "    best_params_list.append(grid_search_lgbm.best_params_)\n",
    "    print(\"Best Parameters:\", best_params_list)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_probs = best_model_lgbm.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Threshold to Classify\n",
    "    threshold = 0.5\n",
    "    y_pred = (y_probs > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "    precision_scores.append(precision_score(y_val_fold, y_pred))\n",
    "    recall_scores.append(recall_score(y_val_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(y_val_fold, y_probs))\n",
    "    conf_matrices.append(confusion_matrix(y_val_fold, y_pred))\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "\n",
    "# Confusion matrices for each fold\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    print(f\"Confusion Matrix for Fold {i+1}:\\n{cm}\")\n",
    "\n",
    "# Evaluation on the test set with the best model from nested CV\n",
    "best_model_lgbm.fit(X_train, y_train)\n",
    "y_probs_test = best_model_lgbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Threshold to Classify\n",
    "threshold = 0.5\n",
    "y_pred_test = (y_probs_test > threshold).astype(int)\n",
    "\n",
    "# Calculate test set metrics\n",
    "test_accuracy_lgbm = accuracy_score(y_test, y_pred_test)\n",
    "test_precision_lgbm = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall_lgbm = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_lgbm = f1_score(y_test, y_pred_test, average='weighted')\n",
    "test_roc_auc_lgbm = roc_auc_score(y_test, y_probs_test)\n",
    "test_conf_matrix_lgbm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Calculate PPV, NPV, and specificity\n",
    "TP = test_conf_matrix_lgbm[1, 1]\n",
    "TN = test_conf_matrix_lgbm[0, 0]\n",
    "FP = test_conf_matrix_lgbm[0, 1]\n",
    "FN = test_conf_matrix_lgbm[1, 0]\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Accuracy: {test_accuracy_lgbm:.4f}\")\n",
    "print(f\"Test Precision: {test_precision_lgbm:.4f}\")\n",
    "print(f\"Test Recall: {test_recall_lgbm:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1_lgbm:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc_lgbm:.4f}\")\n",
    "print(f\"Test Confusion Matrix_LGBM Model:\\n{test_conf_matrix_lgbm}\")\n",
    "print(f\"Test Positive Predictive Value (PPV): {PPV:.4f}\")\n",
    "print(f\"Test Negative Predictive Value (NPV): {NPV:.4f}\")\n",
    "print(f\"Test Specificity: {specificity:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix of the test set\n",
    "sns.heatmap(test_conf_matrix_lgbm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.ylabel('Actual/True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix_LGBM Model - Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the 95% confidence intervals (CIs) for each evaluation metric using bootstrapping\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "accuracy_scores_boot = []\n",
    "precision_scores_boot = []\n",
    "recall_scores_boot = []\n",
    "f1_scores_boot = []\n",
    "roc_auc_scores_boot = []\n",
    "ppv_scores_boot = []\n",
    "npv_scores_boot = []\n",
    "specificity_scores_boot = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(y_test), len(y_test))\n",
    "    if len(np.unique(y_test.iloc[indices])) < 2:\n",
    "        continue\n",
    "    \n",
    "    y_test_boot = y_test.iloc[indices]\n",
    "    y_pred_test_boot = y_pred_test[indices]\n",
    "    y_probs_test_boot = y_probs_test[indices]\n",
    "    \n",
    "    accuracy_scores_boot.append(accuracy_score(y_test_boot, y_pred_test_boot))\n",
    "    precision_scores_boot.append(precision_score(y_test_boot, y_pred_test_boot))\n",
    "    recall_scores_boot.append(recall_score(y_test_boot, y_pred_test_boot))\n",
    "    f1_scores_boot.append(f1_score(y_test_boot, y_pred_test_boot))\n",
    "    roc_auc_scores_boot.append(roc_auc_score(y_test_boot, y_probs_test_boot))\n",
    "    \n",
    "    cm_boot = confusion_matrix(y_test_boot, y_pred_test_boot)\n",
    "    TP_boot = cm_boot[1, 1]\n",
    "    TN_boot = cm_boot[0, 0]\n",
    "    FP_boot = cm_boot[0, 1]\n",
    "    FN_boot = cm_boot[1, 0]\n",
    "    ppv_scores_boot.append(TP_boot / (TP_boot + FP_boot) if (TP_boot + FP_boot) > 0 else 0)\n",
    "    npv_scores_boot.append(TN_boot / (TN_boot + FN_boot) if (TN_boot + FN_boot) > 0 else 0)\n",
    "    specificity_scores_boot.append(TN_boot / (TN_boot + FP_boot) if (TN_boot + FP_boot) > 0 else 0)\n",
    "\n",
    "def bootstrap_confidence_interval(data, alpha=0.05):\n",
    "    sorted_data = np.sort(data)\n",
    "    lower_bound = np.percentile(sorted_data, 100 * (alpha / 2))\n",
    "    upper_bound = np.percentile(sorted_data, 100 * (1 - alpha / 2))\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "print(f\"95% CI for Accuracy: {bootstrap_confidence_interval(accuracy_scores_boot)}\")\n",
    "print(f\"95% CI for Precision: {bootstrap_confidence_interval(precision_scores_boot)}\")\n",
    "print(f\"95% CI for Recall: {bootstrap_confidence_interval(recall_scores_boot)}\")\n",
    "print(f\"95% CI for F1 Score: {bootstrap_confidence_interval(f1_scores_boot)}\")\n",
    "print(f\"95% CI for ROC AUC: {bootstrap_confidence_interval(roc_auc_scores_boot)}\")\n",
    "print(f\"95% CI for PPV: {bootstrap_confidence_interval(ppv_scores_boot)}\")\n",
    "print(f\"95% CI for NPV: {bootstrap_confidence_interval(npv_scores_boot)}\")\n",
    "print(f\"95% CI for Specificity: {bootstrap_confidence_interval(specificity_scores_boot)}\")\n",
    "\n",
    "# Calculate mean values\n",
    "mean_accuracy = np.mean(accuracy_scores_boot)\n",
    "mean_precision = np.mean(precision_scores_boot)\n",
    "mean_recall = np.mean(recall_scores_boot)\n",
    "mean_f1 = np.mean(f1_scores_boot)\n",
    "mean_roc_auc = np.mean(roc_auc_scores_boot)\n",
    "mean_ppv = np.mean(ppv_scores_boot)\n",
    "mean_npv = np.mean(npv_scores_boot)\n",
    "mean_specificity = np.mean(specificity_scores_boot)\n",
    "\n",
    "# Plot the AUC-ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC-ROC (Mean ROC AUC = {mean_roc_auc:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC-ROC Curve LightGBM Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911eb251-09b7-474d-81e1-585a93d94f14",
   "metadata": {},
   "source": [
    "# 4. XGBoost Classifier Model (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b02f2e-fe9a-416e-a528-afa912622954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for Grid Search - (XGBoost)\n",
    "param_grid_xgb = {\n",
    "    'xgb__n_estimators': [250],\n",
    "    'xgb__learning_rate': [0.09],\n",
    "    'xgb__max_depth': [7],\n",
    "    'xgb__min_child_weight': [2],\n",
    "    'xgb__subsample': [0.8],\n",
    "    'xgb__colsample_bytree': [0.8]\n",
    "}\n",
    "\n",
    "\n",
    "# Define the model and the pipeline\n",
    "xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', xgb_model)\n",
    "])\n",
    "\n",
    "# Inner cross-validation loop for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the grid search\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    estimator=pipeline_xgb, \n",
    "    param_grid=param_grid_xgb, \n",
    "    scoring='roc_auc', \n",
    "    cv=inner_cv, \n",
    "    verbose=2, \n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Outer cross-validation loop for model evaluation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "conf_matrices = []\n",
    "best_params_list = []  \n",
    "\n",
    "# Nested cross-validation for XGBoost\n",
    "for train_index, test_index in outer_cv.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Grid search on the inner cross-validation loop\n",
    "    grid_search_xgb.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Best estimator\n",
    "    best_model_xgb = grid_search_xgb.best_estimator_\n",
    "    \n",
    "    # Store the best parameters\n",
    "    best_params_list.append(grid_search_xgb.best_params_)\n",
    "    print(\"Best Parameters:\", best_params_list)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_probs = best_model_xgb.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Threshold to Classify\n",
    "    threshold = 0.5\n",
    "    y_pred = (y_probs > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "    precision_scores.append(precision_score(y_val_fold, y_pred))\n",
    "    recall_scores.append(recall_score(y_val_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(y_val_fold, y_probs))\n",
    "    conf_matrices.append(confusion_matrix(y_val_fold, y_pred))\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "\n",
    "# Confusion matrices for each fold\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    print(f\"Confusion Matrix for Fold {i+1}:\\n{cm}\")\n",
    "\n",
    "# Evaluation on the test set with the best model from nested CV\n",
    "best_model_xgb.fit(X_train, y_train)\n",
    "y_probs_test = best_model_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Threshold to Classify\n",
    "threshold = 0.5\n",
    "y_pred_test = (y_probs_test > threshold).astype(int)\n",
    "\n",
    "# Calculate test set metrics\n",
    "test_accuracy_xgb = accuracy_score(y_test, y_pred_test)\n",
    "test_precision_xgb = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall_xgb = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_xgb = f1_score(y_test, y_pred_test, average='weighted')\n",
    "test_roc_auc_xgb = roc_auc_score(y_test, y_probs_test)\n",
    "test_conf_matrix_xgb = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Calculate PPV, NPV, and specificity\n",
    "TP = test_conf_matrix_xgb[1, 1]\n",
    "TN = test_conf_matrix_xgb[0, 0]\n",
    "FP = test_conf_matrix_xgb[0, 1]\n",
    "FN = test_conf_matrix_xgb[1, 0]\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Accuracy: {test_accuracy_xgb:.4f}\")\n",
    "print(f\"Test Precision: {test_precision_xgb:.4f}\")\n",
    "print(f\"Test Recall: {test_recall_xgb:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1_xgb:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc_xgb:.4f}\")\n",
    "print(f\"Test Confusion Matrix_XGB Model:\\n{test_conf_matrix_xgb}\")\n",
    "print(f\"Test Positive Predictive Value (PPV): {PPV:.4f}\")\n",
    "print(f\"Test Negative Predictive Value (NPV): {NPV:.4f}\")\n",
    "print(f\"Test Specificity: {specificity:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix of the test set\n",
    "sns.heatmap(test_conf_matrix_xgb, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.ylabel('Actual/True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix_XGB Model - Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the 95% confidence intervals (CIs) for each evaluation metric using bootstrapping\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "accuracy_scores_boot = []\n",
    "precision_scores_boot = []\n",
    "recall_scores_boot = []\n",
    "f1_scores_boot = []\n",
    "roc_auc_scores_boot = []\n",
    "ppv_scores_boot = []\n",
    "npv_scores_boot = []\n",
    "specificity_scores_boot = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(y_test), len(y_test))\n",
    "    if len(np.unique(y_test.iloc[indices])) < 2:\n",
    "        continue\n",
    "    \n",
    "    y_test_boot = y_test.iloc[indices]\n",
    "    y_pred_test_boot = y_pred_test[indices]\n",
    "    y_probs_test_boot = y_probs_test[indices]\n",
    "    \n",
    "    accuracy_scores_boot.append(accuracy_score(y_test_boot, y_pred_test_boot))\n",
    "    precision_scores_boot.append(precision_score(y_test_boot, y_pred_test_boot))\n",
    "    recall_scores_boot.append(recall_score(y_test_boot, y_pred_test_boot))\n",
    "    f1_scores_boot.append(f1_score(y_test_boot, y_pred_test_boot))\n",
    "    roc_auc_scores_boot.append(roc_auc_score(y_test_boot, y_probs_test_boot))\n",
    "    \n",
    "    cm_boot = confusion_matrix(y_test_boot, y_pred_test_boot)\n",
    "    TP_boot = cm_boot[1, 1]\n",
    "    TN_boot = cm_boot[0, 0]\n",
    "    FP_boot = cm_boot[0, 1]\n",
    "    FN_boot = cm_boot[1, 0]\n",
    "    ppv_scores_boot.append(TP_boot / (TP_boot + FP_boot) if (TP_boot + FP_boot) > 0 else 0)\n",
    "    npv_scores_boot.append(TN_boot / (TN_boot + FN_boot) if (TN_boot + FN_boot) > 0 else 0)\n",
    "    specificity_scores_boot.append(TN_boot / (TN_boot + FP_boot) if (TN_boot + FP_boot) > 0 else 0)\n",
    "\n",
    "def bootstrap_confidence_interval(data, alpha=0.05):\n",
    "    sorted_data = np.sort(data)\n",
    "    lower_bound = np.percentile(sorted_data, 100 * (alpha / 2))\n",
    "    upper_bound = np.percentile(sorted_data, 100 * (1 - alpha / 2))\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "print(f\"95% CI for Accuracy: {bootstrap_confidence_interval(accuracy_scores_boot)}\")\n",
    "print(f\"95% CI for Precision: {bootstrap_confidence_interval(precision_scores_boot)}\")\n",
    "print(f\"95% CI for Recall: {bootstrap_confidence_interval(recall_scores_boot)}\")\n",
    "print(f\"95% CI for F1 Score: {bootstrap_confidence_interval(f1_scores_boot)}\")\n",
    "print(f\"95% CI for ROC AUC: {bootstrap_confidence_interval(roc_auc_scores_boot)}\")\n",
    "print(f\"95% CI for PPV: {bootstrap_confidence_interval(ppv_scores_boot)}\")\n",
    "print(f\"95% CI for NPV: {bootstrap_confidence_interval(npv_scores_boot)}\")\n",
    "print(f\"95% CI for Specificity: {bootstrap_confidence_interval(specificity_scores_boot)}\")\n",
    "\n",
    "# Calculate mean values\n",
    "mean_accuracy = np.mean(accuracy_scores_boot)\n",
    "mean_precision = np.mean(precision_scores_boot)\n",
    "mean_recall = np.mean(recall_scores_boot)\n",
    "mean_f1 = np.mean(f1_scores_boot)\n",
    "mean_roc_auc = np.mean(roc_auc_scores_boot)\n",
    "mean_ppv = np.mean(ppv_scores_boot)\n",
    "mean_npv = np.mean(npv_scores_boot)\n",
    "mean_specificity = np.mean(specificity_scores_boot)\n",
    "\n",
    "# Plot the AUC-ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC-ROC (Mean ROC AUC = {mean_roc_auc:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC-ROC Curve XGBoost Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c923747-6c72-44e3-bfb9-a447cc3d6ee0",
   "metadata": {},
   "source": [
    "# 5. Random Forest Classifier Model (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfdfcd99-4e74-43de-82ba-a994a5d0d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid for Grid Search - Random Forest (RF)\n",
    "param_grid_rf = {\n",
    "    'rf__n_estimators': [200],\n",
    "    'rf__max_depth': [30],\n",
    "    'rf__min_samples_split': [8],\n",
    "    'rf__min_samples_leaf': [5],\n",
    "    'rf__bootstrap': [True],\n",
    "    'rf__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "\n",
    "# Define the model and the pipeline\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "pipeline_rf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', rf_model)\n",
    "])\n",
    "\n",
    "# Inner cross-validation loop for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the grid search\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=pipeline_rf, \n",
    "    param_grid=param_grid_rf, \n",
    "    scoring='roc_auc', \n",
    "    cv=inner_cv, \n",
    "    verbose=2, \n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Outer cross-validation loop for model evaluation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "specificity_scores = []\n",
    "conf_matrices = []\n",
    "best_params_list = []  \n",
    "\n",
    "\n",
    "# Nested cross-validation for Random Forest\n",
    "for train_index, test_index in outer_cv.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Grid search on the inner cross-validation loop\n",
    "    grid_search_rf.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Best estimator\n",
    "    best_model_rf = grid_search_rf.best_estimator_\n",
    "    \n",
    "    # Best parameters\n",
    "    best_params_list.append(grid_search_rf.best_params_)\n",
    "    print(\"Best Parameters:\", best_params_list)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_probs = best_model_rf.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Threshold to Classify\n",
    "    threshold = 0.5\n",
    "    y_pred = (y_probs > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "    precision_scores.append(precision_score(y_val_fold, y_pred))\n",
    "    recall_scores.append(recall_score(y_val_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(y_val_fold, y_probs))\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_val_fold, y_pred)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "    \n",
    "    TN = conf_matrix[0, 0]\n",
    "    FP = conf_matrix[0, 1]\n",
    "    specificity = TN / (TN + FP)\n",
    "    specificity_scores.append(specificity)\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "print(f\"Mean Specificity: {np.mean(specificity_scores):.4f}\")\n",
    "\n",
    "# Print confusion matrices for each fold\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    print(f\"Confusion Matrix for Fold {i+1}:\\n{cm}\")\n",
    "\n",
    "# Evaluation on the test set with the best model from nested CV\n",
    "best_model_rf.fit(X_train, y_train)\n",
    "y_probs_test = best_model_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Threshold to Classify\n",
    "threshold = 0.5\n",
    "y_pred_test = (y_probs_test > threshold).astype(int)\n",
    "\n",
    "# Calculate test set metrics\n",
    "test_accuracy_rf = accuracy_score(y_test, y_pred_test)\n",
    "test_precision_rf = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall_rf = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_rf = f1_score(y_test, y_pred_test, average='weighted')\n",
    "test_roc_auc_rf = roc_auc_score(y_test, y_probs_test)\n",
    "test_conf_matrix_rf = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Calculate PPV, NPV, and specificity\n",
    "TP = test_conf_matrix_rf[1, 1]\n",
    "TN = test_conf_matrix_rf[0, 0]\n",
    "FP = test_conf_matrix_rf[0, 1]\n",
    "FN = test_conf_matrix_rf[1, 0]\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Accuracy: {test_accuracy_rf:.4f}\")\n",
    "print(f\"Test Precision: {test_precision_rf:.4f}\")\n",
    "print(f\"Test Recall: {test_recall_rf:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1_rf:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc_rf:.4f}\")\n",
    "print(f\"Test Confusion Matrix_RF Model:\\n{test_conf_matrix_rf}\")\n",
    "print(f\"Test Positive Predictive Value (PPV): {PPV:.4f}\")\n",
    "print(f\"Test Negative Predictive Value (NPV): {NPV:.4f}\")\n",
    "print(f\"Test Specificity: {specificity:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix of the test set\n",
    "sns.heatmap(test_conf_matrix_rf, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.ylabel('Actual/True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix_RF Model - Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the 95% confidence intervals (CIs) for each evaluation metric using bootstrapping\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "accuracy_scores_boot = []\n",
    "precision_scores_boot = []\n",
    "recall_scores_boot = []\n",
    "f1_scores_boot = []\n",
    "roc_auc_scores_boot = []\n",
    "ppv_scores_boot = []\n",
    "npv_scores_boot = []\n",
    "specificity_scores_boot = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(y_test), len(y_test))\n",
    "    if len(np.unique(y_test.iloc[indices])) < 2:\n",
    "        continue\n",
    "    \n",
    "    y_test_boot = y_test.iloc[indices]\n",
    "    y_pred_test_boot = y_pred_test[indices]\n",
    "    y_probs_test_boot = y_probs_test[indices]\n",
    "    \n",
    "    accuracy_scores_boot.append(accuracy_score(y_test_boot, y_pred_test_boot))\n",
    "    precision_scores_boot.append(precision_score(y_test_boot, y_pred_test_boot))\n",
    "    recall_scores_boot.append(recall_score(y_test_boot, y_pred_test_boot))\n",
    "    f1_scores_boot.append(f1_score(y_test_boot, y_pred_test_boot))\n",
    "    roc_auc_scores_boot.append(roc_auc_score(y_test_boot, y_probs_test_boot))\n",
    "    \n",
    "    cm_boot = confusion_matrix(y_test_boot, y_pred_test_boot)\n",
    "    TP_boot = cm_boot[1, 1]\n",
    "    TN_boot = cm_boot[0, 0]\n",
    "    FP_boot = cm_boot[0, 1]\n",
    "    FN_boot = cm_boot[1, 0]\n",
    "    ppv_scores_boot.append(TP_boot / (TP_boot + FP_boot) if (TP_boot + FP_boot) > 0 else 0)\n",
    "    npv_scores_boot.append(TN_boot / (TN_boot + FN_boot) if (TN_boot + FN_boot) > 0 else 0)\n",
    "    specificity_scores_boot.append(TN_boot / (TN_boot + FP_boot) if (TN_boot + FP_boot) > 0 else 0)\n",
    "\n",
    "def bootstrap_confidence_interval(data, alpha=0.05):\n",
    "    sorted_data = np.sort(data)\n",
    "    lower_bound = np.percentile(sorted_data, 100 * (alpha / 2))\n",
    "    upper_bound = np.percentile(sorted_data, 100 * (1 - alpha / 2))\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Calculate mean values\n",
    "mean_accuracy = np.mean(accuracy_scores_boot)\n",
    "mean_precision = np.mean(precision_scores_boot)\n",
    "mean_recall = np.mean(recall_scores_boot)\n",
    "mean_f1 = np.mean(f1_scores_boot)\n",
    "mean_roc_auc = np.mean(roc_auc_scores_boot)\n",
    "mean_ppv = np.mean(ppv_scores_boot)\n",
    "mean_npv = np.mean(npv_scores_boot)\n",
    "mean_specificity = np.mean(specificity_scores_boot)\n",
    "\n",
    "# Print confidence intervals\n",
    "print(f\"95% CI for Accuracy: {bootstrap_confidence_interval(accuracy_scores_boot)}\")\n",
    "print(f\"95% CI for Precision: {bootstrap_confidence_interval(precision_scores_boot)}\")\n",
    "print(f\"95% CI for Recall: {bootstrap_confidence_interval(recall_scores_boot)}\")\n",
    "print(f\"95% CI for F1 Score: {bootstrap_confidence_interval(f1_scores_boot)}\")\n",
    "print(f\"95% CI for ROC AUC: {bootstrap_confidence_interval(roc_auc_scores_boot)}\")\n",
    "print(f\"95% CI for PPV: {bootstrap_confidence_interval(ppv_scores_boot)}\")\n",
    "print(f\"95% CI for NPV: {bootstrap_confidence_interval(npv_scores_boot)}\")\n",
    "print(f\"95% CI for Specificity: {bootstrap_confidence_interval(specificity_scores_boot)}\")\n",
    "\n",
    "# Plot the AUC-ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC-ROC (Mean ROC AUC = {mean_roc_auc:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC-ROC Curve Random Forest Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d014011-980a-47cc-a31d-22411e7c1e91",
   "metadata": {},
   "source": [
    "# 6. AdaBoost Classifier Model (AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b90298c-868d-4213-bbdb-701863ece2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid - AdaBoost\n",
    "param_grid = {\n",
    "    'ada__n_estimators': [100],\n",
    "    'ada__learning_rate': [0.09],\n",
    "    'ada__estimator__max_depth': [5]\n",
    "}\n",
    "\n",
    "\n",
    "# Define the model and the pipeline\n",
    "ada = AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ada', ada)\n",
    "])\n",
    "\n",
    "# Inner cross-validation loop for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline, \n",
    "    param_grid=param_grid, \n",
    "    scoring='roc_auc', \n",
    "    cv=inner_cv, \n",
    "    verbose=2, \n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Outer cross-validation loop for model evaluation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "specificity_scores = []\n",
    "conf_matrices = []\n",
    "best_params_list = []\n",
    "\n",
    "# Nested cross-validation for AdaBoost\n",
    "for train_index, test_index in outer_cv.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Grid search on the inner cross-validation loop\n",
    "    grid_search.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Best parameters\n",
    "    best_params_list.append(grid_search.best_params_)\n",
    "    print(\"Best Parameters:\", best_params_list)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_probs = best_model.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Threshold to Classify\n",
    "    threshold = 0.5\n",
    "    y_pred = (y_probs > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "    precision_scores.append(precision_score(y_val_fold, y_pred))\n",
    "    recall_scores.append(recall_score(y_val_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(y_val_fold, y_probs))\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_val_fold, y_pred)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "    \n",
    "    TN = conf_matrix[0, 0]\n",
    "    FP = conf_matrix[0, 1]\n",
    "    specificity = TN / (TN + FP)\n",
    "    specificity_scores.append(specificity)\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "print(f\"Mean Specificity: {np.mean(specificity_scores):.4f}\")\n",
    "\n",
    "# Print confusion matrices for each fold\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    print(f\"Confusion Matrix for Fold {i+1}:\\n{cm}\")\n",
    "\n",
    "# Evaluation on the test set with the best model from nested CV\n",
    "best_model.fit(X_train, y_train)\n",
    "y_probs_test = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Threshold to Classify\n",
    "threshold = 0.5\n",
    "y_pred_test = (y_probs_test > threshold).astype(int)\n",
    "\n",
    "# Calculate test set metrics\n",
    "test_accuracy_ada = accuracy_score(y_test, y_pred_test)\n",
    "test_precision_ada = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall_ada = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_ada = f1_score(y_test, y_pred_test, average='weighted')\n",
    "test_roc_auc_ada = roc_auc_score(y_test, y_probs_test)\n",
    "test_conf_matrix_ada = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Calculate PPV, NPV, and specificity\n",
    "TP = test_conf_matrix_ada[1, 1]\n",
    "TN = test_conf_matrix_ada[0, 0]\n",
    "FP = test_conf_matrix_ada[0, 1]\n",
    "FN = test_conf_matrix_ada[1, 0]\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Accuracy: {test_accuracy_ada:.4f}\")\n",
    "print(f\"Test Precision: {test_precision_ada:.4f}\")\n",
    "print(f\"Test Recall: {test_recall_ada:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1_ada:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc_ada:.4f}\")\n",
    "print(f\"Test Confusion Matrix_AdaBoost Model:\\n{test_conf_matrix_ada}\")\n",
    "print(f\"Test Positive Predictive Value (PPV): {PPV:.4f}\")\n",
    "print(f\"Test Negative Predictive Value (NPV): {NPV:.4f}\")\n",
    "print(f\"Test Specificity: {specificity:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix of the test set\n",
    "sns.heatmap(test_conf_matrix_ada, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.ylabel('Actual/True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix_AdaBoost Model - Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the 95% confidence intervals (CIs) for each evaluation metric using bootstrapping\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "accuracy_scores_boot = []\n",
    "precision_scores_boot = []\n",
    "recall_scores_boot = []\n",
    "f1_scores_boot = []\n",
    "roc_auc_scores_boot = []\n",
    "ppv_scores_boot = []\n",
    "npv_scores_boot = []\n",
    "specificity_scores_boot = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(y_test), len(y_test))\n",
    "    if len(np.unique(y_test.iloc[indices])) < 2:\n",
    "        continue\n",
    "    \n",
    "    y_test_boot = y_test.iloc[indices]\n",
    "    y_pred_test_boot = y_pred_test[indices]\n",
    "    y_probs_test_boot = y_probs_test[indices]\n",
    "    \n",
    "    accuracy_scores_boot.append(accuracy_score(y_test_boot, y_pred_test_boot))\n",
    "    precision_scores_boot.append(precision_score(y_test_boot, y_pred_test_boot))\n",
    "    recall_scores_boot.append(recall_score(y_test_boot, y_pred_test_boot))\n",
    "    f1_scores_boot.append(f1_score(y_test_boot, y_pred_test_boot))\n",
    "    roc_auc_scores_boot.append(roc_auc_score(y_test_boot, y_probs_test_boot))\n",
    "    \n",
    "    cm_boot = confusion_matrix(y_test_boot, y_pred_test_boot)\n",
    "    TP_boot = cm_boot[1, 1]\n",
    "    TN_boot = cm_boot[0, 0]\n",
    "    FP_boot = cm_boot[0, 1]\n",
    "    FN_boot = cm_boot[1, 0]\n",
    "    ppv_scores_boot.append(TP_boot / (TP_boot + FP_boot) if (TP_boot + FP_boot) > 0 else 0)\n",
    "    npv_scores_boot.append(TN_boot / (TN_boot + FN_boot) if (TN_boot + FN_boot) > 0 else 0)\n",
    "    specificity_scores_boot.append(TN_boot / (TN_boot + FP_boot) if (TN_boot + FP_boot) > 0 else 0)\n",
    "\n",
    "def bootstrap_confidence_interval(data, alpha=0.05):\n",
    "    sorted_data = np.sort(data)\n",
    "    lower_bound = np.percentile(sorted_data, 100 * (alpha / 2))\n",
    "    upper_bound = np.percentile(sorted_data, 100 * (1 - alpha / 2))\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Calculate mean values\n",
    "mean_accuracy = np.mean(accuracy_scores_boot)\n",
    "mean_precision = np.mean(precision_scores_boot)\n",
    "mean_recall = np.mean(recall_scores_boot)\n",
    "mean_f1 = np.mean(f1_scores_boot)\n",
    "mean_roc_auc = np.mean(roc_auc_scores_boot)\n",
    "mean_ppv = np.mean(ppv_scores_boot)\n",
    "mean_npv = np.mean(npv_scores_boot)\n",
    "mean_specificity = np.mean(specificity_scores_boot)\n",
    "\n",
    "# Print confidence intervals\n",
    "print(f\"95% CI for Accuracy: {bootstrap_confidence_interval(accuracy_scores_boot)}\")\n",
    "print(f\"95% CI for Precision: {bootstrap_confidence_interval(precision_scores_boot)}\")\n",
    "print(f\"95% CI for Recall: {bootstrap_confidence_interval(recall_scores_boot)}\")\n",
    "print(f\"95% CI for F1 Score: {bootstrap_confidence_interval(f1_scores_boot)}\")\n",
    "print(f\"95% CI for ROC AUC: {bootstrap_confidence_interval(roc_auc_scores_boot)}\")\n",
    "print(f\"95% CI for PPV: {bootstrap_confidence_interval(ppv_scores_boot)}\")\n",
    "print(f\"95% CI for NPV: {bootstrap_confidence_interval(npv_scores_boot)}\")\n",
    "print(f\"95% CI for Specificity: {bootstrap_confidence_interval(specificity_scores_boot)}\")\n",
    "\n",
    "# Plot the AUC-ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC-ROC (Mean ROC AUC = {mean_roc_auc:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC-ROC Curve AdaBoost Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08eca3-4e2c-49d2-bfb3-34b8a8a940fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
