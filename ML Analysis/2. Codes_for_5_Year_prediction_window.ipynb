{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bda6004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "import shap\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dedd113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATID</th>\n",
       "      <th>Smoking_Status</th>\n",
       "      <th>SBP below 120</th>\n",
       "      <th>SBP 120-140</th>\n",
       "      <th>SBP above 140</th>\n",
       "      <th>DBP below 80</th>\n",
       "      <th>DBP 80-90</th>\n",
       "      <th>DBP above 90</th>\n",
       "      <th>Age_Grp_50-60</th>\n",
       "      <th>Age_Grp_60-70</th>\n",
       "      <th>...</th>\n",
       "      <th>Heart_Disease</th>\n",
       "      <th>Sleep_Apnea</th>\n",
       "      <th>Insomnia</th>\n",
       "      <th>Kidney_Disease</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Vitamin_D_Deficiency</th>\n",
       "      <th>Enlarge_Prostate</th>\n",
       "      <th>Bone_Disease</th>\n",
       "      <th>Depressive_Disorder</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123730</th>\n",
       "      <td>2633300</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123731</th>\n",
       "      <td>1575030</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123732</th>\n",
       "      <td>2940185</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123733</th>\n",
       "      <td>1210750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123734</th>\n",
       "      <td>2430126</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PATID  Smoking_Status  SBP below 120  SBP 120-140  SBP above 140  \\\n",
       "123730  2633300               5              0            0              1   \n",
       "123731  1575030               1              0            0              1   \n",
       "123732  2940185               1              0            0              1   \n",
       "123733  1210750               0              0            0              1   \n",
       "123734  2430126               1              0            0              1   \n",
       "\n",
       "        DBP below 80  DBP 80-90  DBP above 90  Age_Grp_50-60  Age_Grp_60-70  \\\n",
       "123730             0          0             1              0              1   \n",
       "123731             0          0             1              0              0   \n",
       "123732             0          0             1              0              0   \n",
       "123733             0          0             1              0              0   \n",
       "123734             0          0             1              0              0   \n",
       "\n",
       "        ...  Heart_Disease  Sleep_Apnea  Insomnia  Kidney_Disease  \\\n",
       "123730  ...              1            0         0               0   \n",
       "123731  ...              0            0         0               0   \n",
       "123732  ...              0            0         0               0   \n",
       "123733  ...              0            1         0               0   \n",
       "123734  ...              0            0         0               0   \n",
       "\n",
       "        Cholesterol  Vitamin_D_Deficiency  Enlarge_Prostate  Bone_Disease  \\\n",
       "123730            0                     0                 0             0   \n",
       "123731            0                     0                 0             0   \n",
       "123732            0                     0                 0             0   \n",
       "123733            0                     0                 0             0   \n",
       "123734            0                     0                 0             0   \n",
       "\n",
       "        Depressive_Disorder  Target  \n",
       "123730                    1       0  \n",
       "123731                    0       0  \n",
       "123732                    1       0  \n",
       "123733                    1       0  \n",
       "123734                    1       0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load this file into a Python Dataframe\n",
    "\n",
    "df = pd.read_csv('2.Preprocessing_Combined_data_5_Years_github_JPAD-2025.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897ce813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf60f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target\n",
      "0    119723\n",
      "1      4012\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking counts of 'Target' column\n",
    "df['Target'].value_counts()\n",
    "print(df['Target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416a338c",
   "metadata": {},
   "source": [
    "# Drop the 'PATID' column as not relavant to our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956bee88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 123735 entries, 0 to 123734\n",
      "Data columns (total 50 columns):\n",
      " #   Column                    Non-Null Count   Dtype\n",
      "---  ------                    --------------   -----\n",
      " 0   Smoking_Status            123735 non-null  int64\n",
      " 1   SBP below 120             123735 non-null  int64\n",
      " 2   SBP 120-140               123735 non-null  int64\n",
      " 3   SBP above 140             123735 non-null  int64\n",
      " 4   DBP below 80              123735 non-null  int64\n",
      " 5   DBP 80-90                 123735 non-null  int64\n",
      " 6   DBP above 90              123735 non-null  int64\n",
      " 7   Age_Grp_50-60             123735 non-null  int64\n",
      " 8   Age_Grp_60-70             123735 non-null  int64\n",
      " 9   Age_Grp_70-80             123735 non-null  int64\n",
      " 10  Age_Grp_80-90             123735 non-null  int64\n",
      " 11  Age_Grp_90-100            123735 non-null  int64\n",
      " 12  Sex_F                     123735 non-null  int64\n",
      " 13  Sex_M                     123735 non-null  int64\n",
      " 14  AMERICAN_IND/ALASKAN      123735 non-null  int64\n",
      " 15  Race_asian                123735 non-null  int64\n",
      " 16  BLACK/AFRIC_AMERICAN      123735 non-null  int64\n",
      " 17  NAT_HAWAIIN               123735 non-null  int64\n",
      " 18  OTHER                     123735 non-null  int64\n",
      " 19  WHITE                     123735 non-null  int64\n",
      " 20  Divorced                  123735 non-null  int64\n",
      " 21  Life Partner              123735 non-null  int64\n",
      " 22  Married                   123735 non-null  int64\n",
      " 23  Separated                 123735 non-null  int64\n",
      " 24  Single                    123735 non-null  int64\n",
      " 25  Widowed                   123735 non-null  int64\n",
      " 26  Diabetes                  123735 non-null  int64\n",
      " 27  Type_2_Diabetes_Mellitus  123735 non-null  int64\n",
      " 28  Epilepsy                  123735 non-null  int64\n",
      " 29  Depression                123735 non-null  int64\n",
      " 30  Obesity                   123735 non-null  int64\n",
      " 31  Stroke                    123735 non-null  int64\n",
      " 32  Anxiety                   123735 non-null  int64\n",
      " 33  Hypertension              123735 non-null  int64\n",
      " 34  Hyperlipidemia            123735 non-null  int64\n",
      " 35  Cardiovascular_Disease    123735 non-null  int64\n",
      " 36  Sleep_Disorder            123735 non-null  int64\n",
      " 37  Headache                  123735 non-null  int64\n",
      " 38  Periodontitis             123735 non-null  int64\n",
      " 39  Concussion                123735 non-null  int64\n",
      " 40  Heart_Disease             123735 non-null  int64\n",
      " 41  Sleep_Apnea               123735 non-null  int64\n",
      " 42  Insomnia                  123735 non-null  int64\n",
      " 43  Kidney_Disease            123735 non-null  int64\n",
      " 44  Cholesterol               123735 non-null  int64\n",
      " 45  Vitamin_D_Deficiency      123735 non-null  int64\n",
      " 46  Enlarge_Prostate          123735 non-null  int64\n",
      " 47  Bone_Disease              123735 non-null  int64\n",
      " 48  Depressive_Disorder       123735 non-null  int64\n",
      " 49  Target                    123735 non-null  int64\n",
      "dtypes: int64(50)\n",
      "memory usage: 47.2 MB\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=['PATID', 'EcType_ED', 'EcType_IP', 'EcType_AV'], inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f13c2",
   "metadata": {},
   "source": [
    "# Splitting data into feature and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd931950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted variable is 'Target'\n",
    "X = df.drop(\"Target\", axis=1)\n",
    "print(X)\n",
    "y = df[\"Target\"]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b6edfd-0ff9-44d5-aac3-cb1f29523437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value counts for 'Age_Grp_90-100':\n",
      "Age_Grp_90-100\n",
      "0    108650\n",
      "1     15085\n",
      "Name: count, dtype: int64\n",
      "Initial value counts for 'Age_Grp_80-90':\n",
      "Age_Grp_80-90\n",
      "0    95195\n",
      "1    28540\n",
      "Name: count, dtype: int64\n",
      "Initial value counts for 'Age_Grp_70-80':\n",
      "Age_Grp_70-80\n",
      "0    83931\n",
      "1    39804\n",
      "Name: count, dtype: int64\n",
      "Initial value counts for 'Age_Grp_60-70':\n",
      "Age_Grp_60-70\n",
      "0    86962\n",
      "1    36773\n",
      "Name: count, dtype: int64\n",
      "Initial value counts for 'Age_Grp_50-60':\n",
      "Age_Grp_50-60\n",
      "0    120202\n",
      "1      3533\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# List of age group columns\n",
    "age_groups = [\n",
    "    'Age_Grp_90-100', 'Age_Grp_80-90', 'Age_Grp_70-80',\n",
    "    'Age_Grp_60-70', 'Age_Grp_50-60'\n",
    "]\n",
    "\n",
    "# Print initial counts for all age groups\n",
    "for age_group in age_groups:\n",
    "    initial_count = df[age_group].value_counts(dropna=False)\n",
    "    print(f\"Initial value counts for '{age_group}':\")\n",
    "    print(initial_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c733cce",
   "metadata": {},
   "source": [
    "# Split data into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4992f6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98988, 49) (24747, 49) (98988,) (24747,)\n",
      "Number transactions X_train dataset:  (98988, 49)\n",
      "Number transactions y_train dataset:  (98988,)\n",
      "Number transactions X_test dataset:  (24747, 49)\n",
      "Number transactions y_test dataset:  (24747,)\n"
     ]
    }
   ],
   "source": [
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# describes info about train and test set\n",
    "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "print(\"Number transactions y_test dataset: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb645538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts in y_test: {0: 23917, 1: 830}\n"
     ]
    }
   ],
   "source": [
    "# Check count of Target variables\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "test_class_counts = dict(zip(unique, counts))\n",
    "\n",
    "print(\"Class counts in y_test:\", test_class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e779e120",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression Classifier (LR) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b255e93-8aa3-45cc-b2ca-c528e43eca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid for Grid Search - Logistic Regression (LR)\n",
    "\n",
    "param_grid_lr = {\n",
    "    'logreg__C': [100], \n",
    "    'logreg__class_weight': ['balanced'],  \n",
    "    'logreg__fit_intercept': [True],  \n",
    "    'logreg__max_iter': [200],  \n",
    "    'logreg__penalty': ['l2'], \n",
    "    'logreg__solver': ['lbfgs']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Define the model and the pipeline\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "pipeline_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', logreg)\n",
    "])\n",
    "\n",
    "# Inner cross-validation loop for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the grid search\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator=pipeline_lr, \n",
    "    param_grid=param_grid_lr, \n",
    "    scoring='roc_auc', \n",
    "    cv=inner_cv, \n",
    "    verbose=2, \n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Outer cross-validation loop for model evaluation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "specificity_scores = []\n",
    "conf_matrices = []\n",
    "best_params_list = []  \n",
    "\n",
    "# Nested cross-validation for Logistic Regression\n",
    "for train_index, test_index in outer_cv.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Grid search on the inner cross-validation loop\n",
    "    grid_search_lr.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Best estimator\n",
    "    best_model_lr = grid_search_lr.best_estimator_\n",
    "\n",
    "    # Best parameters\n",
    "    best_params_list.append(grid_search_lr.best_params_)\n",
    "    print(\"Best Parameters:\", best_params_list)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_probs = best_model_lr.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Threshold to Classify\n",
    "    threshold = 0.5\n",
    "    y_pred = (y_probs > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "    precision_scores.append(precision_score(y_val_fold, y_pred))\n",
    "    recall_scores.append(recall_score(y_val_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(y_val_fold, y_probs))\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_val_fold, y_pred)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "    \n",
    "    TN = conf_matrix[0, 0]\n",
    "    FP = conf_matrix[0, 1]\n",
    "    specificity = TN / (TN + FP)\n",
    "    specificity_scores.append(specificity)\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "print(f\"Mean Specificity: {np.mean(specificity_scores):.4f}\")\n",
    "\n",
    "# Confusion matrices for each fold\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    print(f\"Confusion Matrix for Fold {i+1}:\\n{cm}\")\n",
    "\n",
    "# Evaluation on the test set with the best model from nested CV\n",
    "best_model_lr.fit(X_train, y_train)\n",
    "y_probs_test = best_model_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Threshold to Classify\n",
    "threshold = 0.5\n",
    "y_pred_test = (y_probs_test > threshold).astype(int)\n",
    "\n",
    "# Calculate test set metrics\n",
    "test_accuracy_lr = accuracy_score(y_test, y_pred_test)\n",
    "test_precision_lr = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall_lr = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_lr = f1_score(y_test, y_pred_test, average='weighted')\n",
    "test_roc_auc_lr = roc_auc_score(y_test, y_probs_test)\n",
    "test_conf_matrix_lr = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Calculate PPV, NPV, and specificity\n",
    "TP = test_conf_matrix_lr[1, 1]\n",
    "TN = test_conf_matrix_lr[0, 0]\n",
    "FP = test_conf_matrix_lr[0, 1]\n",
    "FN = test_conf_matrix_lr[1, 0]\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Accuracy: {test_accuracy_lr:.4f}\")\n",
    "print(f\"Test Precision: {test_precision_lr:.4f}\")\n",
    "print(f\"Test Recall: {test_recall_lr:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1_lr:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc_lr:.4f}\")\n",
    "print(f\"Test Confusion Matrix_LR Model:\\n{test_conf_matrix_lr}\")\n",
    "print(f\"Test Positive Predictive Value (PPV): {PPV:.4f}\")\n",
    "print(f\"Test Negative Predictive Value (NPV): {NPV:.4f}\")\n",
    "print(f\"Test Specificity: {specificity:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix of the test set\n",
    "sns.heatmap(test_conf_matrix_lr, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.ylabel('Actual/True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix_LR Model - Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the 95% confidence intervals (CIs) for each evaluation metric using bootstrapping\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "accuracy_scores_boot = []\n",
    "precision_scores_boot = []\n",
    "recall_scores_boot = []\n",
    "f1_scores_boot = []\n",
    "roc_auc_scores_boot = []\n",
    "ppv_scores_boot = []\n",
    "npv_scores_boot = []\n",
    "specificity_scores_boot = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(y_test), len(y_test))\n",
    "    if len(np.unique(y_test.iloc[indices])) < 2:\n",
    "        continue\n",
    "    \n",
    "    y_test_boot = y_test.iloc[indices]\n",
    "    y_pred_test_boot = y_pred_test[indices]\n",
    "    y_probs_test_boot = y_probs_test[indices]\n",
    "    \n",
    "    accuracy_scores_boot.append(accuracy_score(y_test_boot, y_pred_test_boot))\n",
    "    precision_scores_boot.append(precision_score(y_test_boot, y_pred_test_boot))\n",
    "    recall_scores_boot.append(recall_score(y_test_boot, y_pred_test_boot))\n",
    "    f1_scores_boot.append(f1_score(y_test_boot, y_pred_test_boot))\n",
    "    roc_auc_scores_boot.append(roc_auc_score(y_test_boot, y_probs_test_boot))\n",
    "    \n",
    "    cm_boot = confusion_matrix(y_test_boot, y_pred_test_boot)\n",
    "    TP_boot = cm_boot[1, 1]\n",
    "    TN_boot = cm_boot[0, 0]\n",
    "    FP_boot = cm_boot[0, 1]\n",
    "    FN_boot = cm_boot[1, 0]\n",
    "    ppv_scores_boot.append(TP_boot / (TP_boot + FP_boot) if (TP_boot + FP_boot) > 0 else 0)\n",
    "    npv_scores_boot.append(TN_boot / (TN_boot + FN_boot) if (TN_boot + FN_boot) > 0 else 0)\n",
    "    specificity_scores_boot.append(TN_boot / (TN_boot + FP_boot) if (TN_boot + FP_boot) > 0 else 0)\n",
    "\n",
    "def bootstrap_confidence_interval(data, alpha=0.05):\n",
    "    sorted_data = np.sort(data)\n",
    "    lower_bound = np.percentile(sorted_data, 100 * (alpha / 2))\n",
    "    upper_bound = np.percentile(sorted_data, 100 * (1 - alpha / 2))\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Calculate mean values\n",
    "mean_accuracy = np.mean(accuracy_scores_boot)\n",
    "mean_precision = np.mean(precision_scores_boot)\n",
    "mean_recall = np.mean(recall_scores_boot)\n",
    "mean_f1 = np.mean(f1_scores_boot)\n",
    "mean_roc_auc = np.mean(roc_auc_scores_boot)\n",
    "mean_ppv = np.mean(ppv_scores_boot)\n",
    "mean_npv = np.mean(npv_scores_boot)\n",
    "mean_specificity = np.mean(specificity_scores_boot)\n",
    "\n",
    "# Print confidence intervals\n",
    "print(f\"95% CI for Accuracy: {bootstrap_confidence_interval(accuracy_scores_boot)}\")\n",
    "print(f\"95% CI for Precision: {bootstrap_confidence_interval(precision_scores_boot)}\")\n",
    "print(f\"95% CI for Recall: {bootstrap_confidence_interval(recall_scores_boot)}\")\n",
    "print(f\"95% CI for F1 Score: {bootstrap_confidence_interval(f1_scores_boot)}\")\n",
    "print(f\"95% CI for ROC AUC: {bootstrap_confidence_interval(roc_auc_scores_boot)}\")\n",
    "print(f\"95% CI for PPV: {bootstrap_confidence_interval(ppv_scores_boot)}\")\n",
    "print(f\"95% CI for NPV: {bootstrap_confidence_interval(npv_scores_boot)}\")\n",
    "print(f\"95% CI for Specificity: {bootstrap_confidence_interval(specificity_scores_boot)}\")\n",
    "\n",
    "# Plot the AUC-ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC-ROC (Mean ROC AUC = {mean_roc_auc:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC-ROC Curve Logistic Regression Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b69d7fc-7630-4cc3-9dc9-856c233f41d0",
   "metadata": {},
   "source": [
    "# 2. Gradient Boosting Tree Classifier (GBT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fec68377-53dc-4997-aa2b-fe3e05b4469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Define the parameter grid for Grid Search - Gradient Boosting Tree (GBT)\n",
    "param_grid_gbt = {\n",
    "    'gbt__n_estimators': [500],\n",
    "    'gbt__learning_rate': [0.1],\n",
    "    'gbt__max_depth': [4],\n",
    "    'gbt__min_samples_split': [5],\n",
    "    'gbt__min_samples_leaf': [4],\n",
    "    'gbt__subsample': [1.0],\n",
    "    'gbt__max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "\n",
    "# Define the model and the pipeline\n",
    "gbt = GradientBoostingClassifier(random_state=42)\n",
    "pipeline_gbt = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gbt', gbt)\n",
    "])\n",
    "\n",
    "# Inner cross-validation loop for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the grid search\n",
    "grid_search_gbt = GridSearchCV(\n",
    "    estimator=pipeline_gbt, \n",
    "    param_grid=param_grid_gbt, \n",
    "    scoring='roc_auc', \n",
    "    cv=inner_cv, \n",
    "    verbose=2, \n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Outer cross-validation loop for model evaluation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "specificity_scores = []\n",
    "conf_matrices = []\n",
    "best_params_list = []  \n",
    "\n",
    "# Nested cross-validation for GBT\n",
    "for train_index, test_index in outer_cv.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Grid search on the inner cross-validation loop\n",
    "    grid_search_gbt.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Best estimator\n",
    "    best_model_gbt = grid_search_gbt.best_estimator_\n",
    "    \n",
    "    # Best parameters\n",
    "    best_params_list.append(grid_search_gbt.best_params_)\n",
    "    print(\"Best Parameters:\", best_params_list)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_probs = best_model_gbt.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "    # Threshold to Classify\n",
    "    threshold = 0.5\n",
    "    y_pred = (y_probs > threshold).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "    precision_scores.append(precision_score(y_val_fold, y_pred))\n",
    "    recall_scores.append(recall_score(y_val_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(y_val_fold, y_probs))\n",
    "    conf_matrices.append(confusion_matrix(y_val_fold, y_pred))\n",
    "    \n",
    "    # Calculate specificity for the current fold\n",
    "    TN = conf_matrices[-1][0, 0]\n",
    "    FP = conf_matrices[-1][0, 1]\n",
    "    specificity = TN / (TN + FP)\n",
    "    specificity_scores.append(specificity)\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "print(f\"Mean Specificity: {np.mean(specificity_scores):.4f}\")\n",
    "\n",
    "# Confusion matrices for each fold\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    print(f\"Confusion Matrix for Fold {i+1}:\\n{cm}\")\n",
    "\n",
    "# Evaluation on the test set with the best model from nested CV\n",
    "best_model_gbt.fit(X_train, y_train)\n",
    "y_probs_test = best_model_gbt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Threshold to Classify\n",
    "threshold = 0.5\n",
    "y_pred_test = (y_probs_test > threshold).astype(int)\n",
    "\n",
    "# Calculate test set metrics\n",
    "test_accuracy_gbt = accuracy_score(y_test, y_pred_test)\n",
    "test_precision_gbt = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall_gbt = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_gbt = f1_score(y_test, y_pred_test, average='weighted')\n",
    "test_roc_auc_gbt = roc_auc_score(y_test, y_probs_test)\n",
    "test_conf_matrix_gbt = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Calculate PPV, NPV, and specificity\n",
    "TP = test_conf_matrix_gbt[1, 1]\n",
    "TN = test_conf_matrix_gbt[0, 0]\n",
    "FP = test_conf_matrix_gbt[0, 1]\n",
    "FN = test_conf_matrix_gbt[1, 0]\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Accuracy: {test_accuracy_gbt:.4f}\")\n",
    "print(f\"Test Precision: {test_precision_gbt:.4f}\")\n",
    "print(f\"Test Recall: {test_recall_gbt:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1_gbt:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc_gbt:.4f}\")\n",
    "print(f\"Test Confusion Matrix_GBT Model:\\n{test_conf_matrix_gbt}\")\n",
    "print(f\"Test Positive Predictive Value (PPV): {PPV:.4f}\")\n",
    "print(f\"Test Negative Predictive Value (NPV): {NPV:.4f}\")\n",
    "print(f\"Test Specificity: {specificity:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix of the test set\n",
    "sns.heatmap(test_conf_matrix_gbt, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.ylabel('Actual/True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix_GBT Model - Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the 95% confidence intervals (CIs) for each evaluation metric using bootstrapping\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "accuracy_scores_boot = []\n",
    "precision_scores_boot = []\n",
    "recall_scores_boot = []\n",
    "f1_scores_boot = []\n",
    "roc_auc_scores_boot = []\n",
    "ppv_scores_boot = []\n",
    "npv_scores_boot = []\n",
    "specificity_scores_boot = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(y_test), len(y_test))\n",
    "    if len(np.unique(y_test.iloc[indices])) < 2:\n",
    "        continue\n",
    "    \n",
    "    y_test_boot = y_test.iloc[indices]\n",
    "    y_pred_test_boot = y_pred_test[indices]\n",
    "    y_probs_test_boot = y_probs_test[indices]\n",
    "    \n",
    "    accuracy_scores_boot.append(accuracy_score(y_test_boot, y_pred_test_boot))\n",
    "    precision_scores_boot.append(precision_score(y_test_boot, y_pred_test_boot))\n",
    "    recall_scores_boot.append(recall_score(y_test_boot, y_pred_test_boot))\n",
    "    f1_scores_boot.append(f1_score(y_test_boot, y_pred_test_boot))\n",
    "    roc_auc_scores_boot.append(roc_auc_score(y_test_boot, y_probs_test_boot))\n",
    "    \n",
    "    cm_boot = confusion_matrix(y_test_boot, y_pred_test_boot)\n",
    "    TP_boot = cm_boot[1, 1]\n",
    "    TN_boot = cm_boot[0, 0]\n",
    "    FP_boot = cm_boot[0, 1]\n",
    "    FN_boot = cm_boot[1, 0]\n",
    "    ppv_scores_boot.append(TP_boot / (TP_boot + FP_boot) if (TP_boot + FP_boot) > 0 else 0)\n",
    "    npv_scores_boot.append(TN_boot / (TN_boot + FN_boot) if (TN_boot + FN_boot) > 0 else 0)\n",
    "    specificity_scores_boot.append(TN_boot / (TN_boot + FP_boot) if (TN_boot + FP_boot) > 0 else 0)\n",
    "\n",
    "def bootstrap_confidence_interval(data, alpha=0.05):\n",
    "    sorted_data = np.sort(data)\n",
    "    lower_bound = np.percentile(sorted_data, 100 * (alpha / 2))\n",
    "    upper_bound = np.percentile(sorted_data, 100 * (1 - alpha / 2))\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Calculate mean values\n",
    "mean_accuracy = np.mean(accuracy_scores_boot)\n",
    "mean_precision = np.mean(precision_scores_boot)\n",
    "mean_recall = np.mean(recall_scores_boot)\n",
    "mean_f1 = np.mean(f1_scores_boot)\n",
    "mean_roc_auc = np.mean(roc_auc_scores_boot)\n",
    "mean_ppv = np.mean(ppv_scores_boot)\n",
    "mean_npv = np.mean(npv_scores_boot)\n",
    "mean_specificity = np.mean(specificity_scores_boot)\n",
    "\n",
    "# Print confidence intervals\n",
    "print(f\"95% CI for Accuracy: {bootstrap_confidence_interval(accuracy_scores_boot)}\")\n",
    "print(f\"95% CI for Precision: {bootstrap_confidence_interval(precision_scores_boot)}\")\n",
    "print(f\"95% CI for Recall: {bootstrap_confidence_interval(recall_scores_boot)}\")\n",
    "print(f\"95% CI for F1 Score: {bootstrap_confidence_interval(f1_scores_boot)}\")\n",
    "print(f\"95% CI for ROC AUC: {bootstrap_confidence_interval(roc_auc_scores_boot)}\")\n",
    "print(f\"95% CI for PPV: {bootstrap_confidence_interval(ppv_scores_boot)}\")\n",
    "print(f\"95% CI for NPV: {bootstrap_confidence_interval(npv_scores_boot)}\")\n",
    "print(f\"95% CI for Specificity: {bootstrap_confidence_interval(specificity_scores_boot)}\")\n",
    "\n",
    "# Plot the AUC-ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC-ROC (Mean ROC AUC = {mean_roc_auc:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC-ROC Curve GBT Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9210b7-a9d3-4780-8a63-b82646c3845f",
   "metadata": {},
   "source": [
    "# SHapley Additive exPlanations (SHAP) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638c6070-96b6-48df-9f0b-f69526cfb842",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a SHAP explainer with the best model and the training dataset\n",
    "explainer = shap.Explainer(best_model_gbt.named_steps['gbt'], X_train)\n",
    "\n",
    "# Compute SHAP values for the test dataset\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Top indices based on mean absolute SHAP values\n",
    "def get_top_shap_indices(shap_values, top_n=15):\n",
    "    # Sum absolute SHAP values across all samples to get feature importance\n",
    "    shap_sum = np.abs(shap_values.values).mean(axis=0)\n",
    "    \n",
    "    # Get indices of the top features\n",
    "    top_indices = np.argsort(shap_sum)[-top_n:]\n",
    "    return top_indices\n",
    "\n",
    "# Get top 15 feature indices from SHAP values\n",
    "top_indices = get_top_shap_indices(shap_values, 15)\n",
    "\n",
    "# Extract SHAP values for the top 15 features\n",
    "shap_values_top = shap_values[:, top_indices]\n",
    "\n",
    "if isinstance(X_test, pd.DataFrame):\n",
    "    X_test_top = X_test.iloc[:, top_indices]\n",
    "    feature_names = X_test.columns[top_indices]\n",
    "else:\n",
    "    X_test_top = X_test[:, top_indices]\n",
    "    feature_names = top_indices  \n",
    "\n",
    "# Create SHAP Explanation object\n",
    "shap_values_explanation = shap.Explanation(\n",
    "    values=shap_values_top,\n",
    "    base_values=shap_values.base_values,\n",
    "    data=X_test_top,\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "# Plot with the built-in SHAP function\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values_explanation, X_test_top, max_display=15, plot_type=\"dot\", show=False)\n",
    "plt.savefig('SHAP_Plot_5Years_GBT_Model.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Plot the bar plot with SHAP values\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values_explanation, X_test_top, max_display=15, plot_type=\"bar\", show=False)\n",
    "\n",
    "# Add values on top of bars with two decimal points\n",
    "shap_sum = np.abs(shap_values.values).mean(axis=0)\n",
    "top_indices_sorted = np.argsort(shap_sum)[-15:]\n",
    "sorted_shap_values = shap_sum[top_indices_sorted]\n",
    "sorted_feature_names = X_test.columns[top_indices_sorted]\n",
    "\n",
    "for i, (value, name) in enumerate(zip(sorted_shap_values, sorted_feature_names)):\n",
    "    plt.text(value, i, f'{value:.2f}', va='center')\n",
    "\n",
    "plt.savefig('SHAP_Plot_5Years_GBT_Model_Bar.png', bbox_inches='tight')\n",
    "plt.close()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4dd535-fc52-44b9-afa8-f54a95e4aa9c",
   "metadata": {},
   "source": [
    "# SHAP FORCE plot - GBT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d0a7c8a-9ebd-4b87-a44f-239fff7e2d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a SHAP explainer with the best model and the training dataset\n",
    "explainer = shap.Explainer(best_model_gbt.named_steps['gbt'], X_train)\n",
    "\n",
    "# Compute SHAP values for the test dataset\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Function to get the top indices based on mean absolute SHAP values\n",
    "def get_top_shap_indices(shap_values, top_n=15):\n",
    "    # Sum absolute SHAP values across all samples to get feature importance\n",
    "    shap_sum = np.abs(shap_values.values).mean(axis=0)\n",
    "    \n",
    "    # Get indices of the top features\n",
    "    top_indices = np.argsort(shap_sum)[-top_n:]\n",
    "    return top_indices\n",
    "\n",
    "# Get top 15 feature indices from SHAP values\n",
    "top_indices = get_top_shap_indices(shap_values, 15)\n",
    "\n",
    "# Extract SHAP values for the top 15 features\n",
    "shap_values_top = shap_values[:, top_indices]\n",
    "\n",
    "if isinstance(X_test, pd.DataFrame):\n",
    "    X_test_top = X_test.iloc[:, top_indices]\n",
    "    feature_names = X_test.columns[top_indices]\n",
    "else:\n",
    "    X_test_top = X_test[:, top_indices]\n",
    "    feature_names = top_indices  \n",
    "\n",
    "# Create SHAP Explanation object\n",
    "shap_values_explanation = shap.Explanation(\n",
    "    values=shap_values_top.values,\n",
    "    base_values=shap_values.base_values,\n",
    "    data=X_test_top,\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "# Global force plot\n",
    "shap.initjs()\n",
    "shap.force_plot(\n",
    "    shap_values_explanation.base_values.mean(), \n",
    "    shap_values_explanation.values.mean(axis=0), \n",
    "    feature_names, \n",
    "    matplotlib=True\n",
    ")\n",
    "plt.savefig('SHAP_Global_5Years_Force_Plot_GBT_Model.png', bbox_inches='tight')\n",
    "plt.close()  \n",
    "\n",
    "# Individual force plot for the first instance\n",
    "shap.initjs()\n",
    "shap.force_plot(\n",
    "    shap_values_explanation.base_values[0], \n",
    "    shap_values_explanation.values[0], \n",
    "    X_test_top.iloc[0], \n",
    "    feature_names, \n",
    "    matplotlib=True\n",
    ")\n",
    "plt.savefig('SHAP_Individual_5Years_Force_Plot_GBT_Model.png', bbox_inches='tight')\n",
    "plt.close()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82828b-9db5-4250-8caa-0902838682cf",
   "metadata": {},
   "source": [
    "# 3. Light GBM Classifier Model (Light GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b68182b-5258-4d85-9e3d-10a145e520a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the parameter grid for Grid Search - (Light GBM)\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    'lgbm__n_estimators': [100, 200],\n",
    "    'lgbm__learning_rate': [0.01, 0.1],\n",
    "    'lgbm__max_depth': [5, 7],\n",
    "    'lgbm__num_leaves': [31, 40],\n",
    "    'lgbm__subsample': [0.8, 0.9],\n",
    "    'lgbm__colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Define the model and the pipeline\n",
    "lgbm = lgb.LGBMClassifier(random_state=42)\n",
    "pipeline_lgbm = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lgbm', lgbm)\n",
    "])\n",
    "\n",
    "# Inner cross-validation loop for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the grid search\n",
    "grid_search_lgbm = GridSearchCV(\n",
    "    estimator=pipeline_lgbm, \n",
    "    param_grid=param_grid_lgbm, \n",
    "    scoring='roc_auc', \n",
    "    cv=inner_cv, \n",
    "    verbose=2, \n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Outer cross-validation loop for model evaluation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "specificity_scores = []\n",
    "conf_matrices = []\n",
    "best_params_list = []  \n",
    "\n",
    "# Nested cross-validation for LightGBM\n",
    "for train_index, test_index in outer_cv.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Grid search on the inner cross-validation loop\n",
    "    grid_search_lgbm.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Best estimator\n",
    "    best_model_lgbm = grid_search_lgbm.best_estimator_\n",
    "    \n",
    "    # Best parameters\n",
    "    best_params_list.append(grid_search_lgbm.best_params_)\n",
    "    print(\"Best Parameters:\", best_params_list)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_probs = best_model_lgbm.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Threshold to Classify\n",
    "    threshold = 0.5\n",
    "    y_pred = (y_probs > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "    precision_scores.append(precision_score(y_val_fold, y_pred))\n",
    "    recall_scores.append(recall_score(y_val_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(y_val_fold, y_probs))\n",
    "    conf_matrices.append(confusion_matrix(y_val_fold, y_pred))\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "\n",
    "# Confusion matrices for each fold\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    print(f\"Confusion Matrix for Fold {i+1}:\\n{cm}\")\n",
    "\n",
    "# Evaluation on the test set with the best model from nested CV\n",
    "best_model_lgbm.fit(X_train, y_train)\n",
    "y_probs_test = best_model_lgbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Threshold to Classify\n",
    "threshold = 0.5\n",
    "y_pred_test = (y_probs_test > threshold).astype(int)\n",
    "\n",
    "# Calculate test set metrics\n",
    "test_accuracy_lgbm = accuracy_score(y_test, y_pred_test)\n",
    "test_precision_lgbm = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall_lgbm = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_lgbm = f1_score(y_test, y_pred_test, average='weighted')\n",
    "test_roc_auc_lgbm = roc_auc_score(y_test, y_probs_test)\n",
    "test_conf_matrix_lgbm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Calculate PPV, NPV, and specificity\n",
    "TP = test_conf_matrix_lgbm[1, 1]\n",
    "TN = test_conf_matrix_lgbm[0, 0]\n",
    "FP = test_conf_matrix_lgbm[0, 1]\n",
    "FN = test_conf_matrix_lgbm[1, 0]\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Accuracy: {test_accuracy_lgbm:.4f}\")\n",
    "print(f\"Test Precision: {test_precision_lgbm:.4f}\")\n",
    "print(f\"Test Recall: {test_recall_lgbm:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1_lgbm:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc_lgbm:.4f}\")\n",
    "print(f\"Test Confusion Matrix_LGBM Model:\\n{test_conf_matrix_lgbm}\")\n",
    "print(f\"Test Positive Predictive Value (PPV): {PPV:.4f}\")\n",
    "print(f\"Test Negative Predictive Value (NPV): {NPV:.4f}\")\n",
    "print(f\"Test Specificity: {specificity:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix of the test set\n",
    "sns.heatmap(test_conf_matrix_lgbm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.ylabel('Actual/True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix_LGBM Model - Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the 95% confidence intervals (CIs) for each evaluation metric using bootstrapping\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "accuracy_scores_boot = []\n",
    "precision_scores_boot = []\n",
    "recall_scores_boot = []\n",
    "f1_scores_boot = []\n",
    "roc_auc_scores_boot = []\n",
    "ppv_scores_boot = []\n",
    "npv_scores_boot = []\n",
    "specificity_scores_boot = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(y_test), len(y_test))\n",
    "    if len(np.unique(y_test.iloc[indices])) < 2:\n",
    "        continue\n",
    "    \n",
    "    y_test_boot = y_test.iloc[indices]\n",
    "    y_pred_test_boot = y_pred_test[indices]\n",
    "    y_probs_test_boot = y_probs_test[indices]\n",
    "    \n",
    "    accuracy_scores_boot.append(accuracy_score(y_test_boot, y_pred_test_boot))\n",
    "    precision_scores_boot.append(precision_score(y_test_boot, y_pred_test_boot))\n",
    "    recall_scores_boot.append(recall_score(y_test_boot, y_pred_test_boot))\n",
    "    f1_scores_boot.append(f1_score(y_test_boot, y_pred_test_boot))\n",
    "    roc_auc_scores_boot.append(roc_auc_score(y_test_boot, y_probs_test_boot))\n",
    "    \n",
    "    cm_boot = confusion_matrix(y_test_boot, y_pred_test_boot)\n",
    "    TP_boot = cm_boot[1, 1]\n",
    "    TN_boot = cm_boot[0, 0]\n",
    "    FP_boot = cm_boot[0, 1]\n",
    "    FN_boot = cm_boot[1, 0]\n",
    "    ppv_scores_boot.append(TP_boot / (TP_boot + FP_boot) if (TP_boot + FP_boot) > 0 else 0)\n",
    "    npv_scores_boot.append(TN_boot / (TN_boot + FN_boot) if (TN_boot + FN_boot) > 0 else 0)\n",
    "    specificity_scores_boot.append(TN_boot / (TN_boot + FP_boot) if (TN_boot + FP_boot) > 0 else 0)\n",
    "\n",
    "def bootstrap_confidence_interval(data, alpha=0.05):\n",
    "    sorted_data = np.sort(data)\n",
    "    lower_bound = np.percentile(sorted_data, 100 * (alpha / 2))\n",
    "    upper_bound = np.percentile(sorted_data, 100 * (1 - alpha / 2))\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "print(f\"95% CI for Accuracy: {bootstrap_confidence_interval(accuracy_scores_boot)}\")\n",
    "print(f\"95% CI for Precision: {bootstrap_confidence_interval(precision_scores_boot)}\")\n",
    "print(f\"95% CI for Recall: {bootstrap_confidence_interval(recall_scores_boot)}\")\n",
    "print(f\"95% CI for F1 Score: {bootstrap_confidence_interval(f1_scores_boot)}\")\n",
    "print(f\"95% CI for ROC AUC: {bootstrap_confidence_interval(roc_auc_scores_boot)}\")\n",
    "print(f\"95% CI for PPV: {bootstrap_confidence_interval(ppv_scores_boot)}\")\n",
    "print(f\"95% CI for NPV: {bootstrap_confidence_interval(npv_scores_boot)}\")\n",
    "print(f\"95% CI for Specificity: {bootstrap_confidence_interval(specificity_scores_boot)}\")\n",
    "\n",
    "# Calculate mean values\n",
    "mean_accuracy = np.mean(accuracy_scores_boot)\n",
    "mean_precision = np.mean(precision_scores_boot)\n",
    "mean_recall = np.mean(recall_scores_boot)\n",
    "mean_f1 = np.mean(f1_scores_boot)\n",
    "mean_roc_auc = np.mean(roc_auc_scores_boot)\n",
    "mean_ppv = np.mean(ppv_scores_boot)\n",
    "mean_npv = np.mean(npv_scores_boot)\n",
    "mean_specificity = np.mean(specificity_scores_boot)\n",
    "\n",
    "# Plot the AUC-ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC-ROC (Mean ROC AUC = {mean_roc_auc:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC-ROC Curve LightGBM Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911eb251-09b7-474d-81e1-585a93d94f14",
   "metadata": {},
   "source": [
    "# 4. XGBoost Classifier Model (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4de06ae-9c0c-4cf8-b229-2c19c101082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid for Grid Search - (XGBoost)\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'xgb__n_estimators': [200],\n",
    "    'xgb__learning_rate': [0.05],\n",
    "    'xgb__max_depth': [5],\n",
    "    'xgb__min_child_weight': [5],\n",
    "    'xgb__subsample': [0.8],\n",
    "    'xgb__colsample_bytree': [0.8]\n",
    "}\n",
    "\n",
    "# Define the model and the pipeline\n",
    "xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', xgb_model)\n",
    "])\n",
    "\n",
    "# Inner cross-validation loop for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the grid search\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    estimator=pipeline_xgb, \n",
    "    param_grid=param_grid_xgb, \n",
    "    scoring='roc_auc', \n",
    "    cv=inner_cv, \n",
    "    verbose=2, \n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Outer cross-validation loop for model evaluation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "conf_matrices = []\n",
    "best_params_list = [] \n",
    "\n",
    "# Nested cross-validation for XGBoost\n",
    "for train_index, test_index in outer_cv.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Grid search on the inner cross-validation loop\n",
    "    grid_search_xgb.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Best estimator\n",
    "    best_model_xgb = grid_search_xgb.best_estimator_\n",
    "    \n",
    "    # Best parameters\n",
    "    best_params_list.append(grid_search_xgb.best_params_)\n",
    "    print(\"Best Parameters:\", best_params_list)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_probs = best_model_xgb.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Threshold to Classify\n",
    "    threshold = 0.5\n",
    "    y_pred = (y_probs > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "    precision_scores.append(precision_score(y_val_fold, y_pred))\n",
    "    recall_scores.append(recall_score(y_val_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(y_val_fold, y_probs))\n",
    "    conf_matrices.append(confusion_matrix(y_val_fold, y_pred))\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "\n",
    "# Print confusion matrices for each fold\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    print(f\"Confusion Matrix for Fold {i+1}:\\n{cm}\")\n",
    "\n",
    "# Evaluation on the test set with the best model from nested CV\n",
    "best_model_xgb.fit(X_train, y_train)\n",
    "y_probs_test = best_model_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Threshold to Classify\n",
    "threshold = 0.5\n",
    "y_pred_test = (y_probs_test > threshold).astype(int)\n",
    "\n",
    "# Calculate test set metrics\n",
    "test_accuracy_xgb = accuracy_score(y_test, y_pred_test)\n",
    "test_precision_xgb = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall_xgb = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_xgb = f1_score(y_test, y_pred_test, average='weighted')\n",
    "test_roc_auc_xgb = roc_auc_score(y_test, y_probs_test)\n",
    "test_conf_matrix_xgb = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Calculate PPV, NPV, and specificity\n",
    "TP = test_conf_matrix_xgb[1, 1]\n",
    "TN = test_conf_matrix_xgb[0, 0]\n",
    "FP = test_conf_matrix_xgb[0, 1]\n",
    "FN = test_conf_matrix_xgb[1, 0]\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Accuracy: {test_accuracy_xgb:.4f}\")\n",
    "print(f\"Test Precision: {test_precision_xgb:.4f}\")\n",
    "print(f\"Test Recall: {test_recall_xgb:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1_xgb:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc_xgb:.4f}\")\n",
    "print(f\"Test Confusion Matrix_XGB Model:\\n{test_conf_matrix_xgb}\")\n",
    "print(f\"Test Positive Predictive Value (PPV): {PPV:.4f}\")\n",
    "print(f\"Test Negative Predictive Value (NPV): {NPV:.4f}\")\n",
    "print(f\"Test Specificity: {specificity:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix of the test set\n",
    "sns.heatmap(test_conf_matrix_xgb, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.ylabel('Actual/True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix_XGB Model - Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the 95% confidence intervals (CIs) for each evaluation metric using bootstrapping\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "accuracy_scores_boot = []\n",
    "precision_scores_boot = []\n",
    "recall_scores_boot = []\n",
    "f1_scores_boot = []\n",
    "roc_auc_scores_boot = []\n",
    "ppv_scores_boot = []\n",
    "npv_scores_boot = []\n",
    "specificity_scores_boot = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(y_test), len(y_test))\n",
    "    if len(np.unique(y_test.iloc[indices])) < 2:\n",
    "        continue\n",
    "    \n",
    "    y_test_boot = y_test.iloc[indices]\n",
    "    y_pred_test_boot = y_pred_test[indices]\n",
    "    y_probs_test_boot = y_probs_test[indices]\n",
    "    \n",
    "    accuracy_scores_boot.append(accuracy_score(y_test_boot, y_pred_test_boot))\n",
    "    precision_scores_boot.append(precision_score(y_test_boot, y_pred_test_boot))\n",
    "    recall_scores_boot.append(recall_score(y_test_boot, y_pred_test_boot))\n",
    "    f1_scores_boot.append(f1_score(y_test_boot, y_pred_test_boot))\n",
    "    roc_auc_scores_boot.append(roc_auc_score(y_test_boot, y_probs_test_boot))\n",
    "    \n",
    "    cm_boot = confusion_matrix(y_test_boot, y_pred_test_boot)\n",
    "    TP_boot = cm_boot[1, 1]\n",
    "    TN_boot = cm_boot[0, 0]\n",
    "    FP_boot = cm_boot[0, 1]\n",
    "    FN_boot = cm_boot[1, 0]\n",
    "    ppv_scores_boot.append(TP_boot / (TP_boot + FP_boot) if (TP_boot + FP_boot) > 0 else 0)\n",
    "    npv_scores_boot.append(TN_boot / (TN_boot + FN_boot) if (TN_boot + FN_boot) > 0 else 0)\n",
    "    specificity_scores_boot.append(TN_boot / (TN_boot + FP_boot) if (TN_boot + FP_boot) > 0 else 0)\n",
    "\n",
    "def bootstrap_confidence_interval(data, alpha=0.05):\n",
    "    sorted_data = np.sort(data)\n",
    "    lower_bound = np.percentile(sorted_data, 100 * (alpha / 2))\n",
    "    upper_bound = np.percentile(sorted_data, 100 * (1 - alpha / 2))\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "print(f\"95% CI for Accuracy: {bootstrap_confidence_interval(accuracy_scores_boot)}\")\n",
    "print(f\"95% CI for Precision: {bootstrap_confidence_interval(precision_scores_boot)}\")\n",
    "print(f\"95% CI for Recall: {bootstrap_confidence_interval(recall_scores_boot)}\")\n",
    "print(f\"95% CI for F1 Score: {bootstrap_confidence_interval(f1_scores_boot)}\")\n",
    "print(f\"95% CI for ROC AUC: {bootstrap_confidence_interval(roc_auc_scores_boot)}\")\n",
    "print(f\"95% CI for PPV: {bootstrap_confidence_interval(ppv_scores_boot)}\")\n",
    "print(f\"95% CI for NPV: {bootstrap_confidence_interval(npv_scores_boot)}\")\n",
    "print(f\"95% CI for Specificity: {bootstrap_confidence_interval(specificity_scores_boot)}\")\n",
    "\n",
    "# Calculate mean values\n",
    "mean_accuracy = np.mean(accuracy_scores_boot)\n",
    "mean_precision = np.mean(precision_scores_boot)\n",
    "mean_recall = np.mean(recall_scores_boot)\n",
    "mean_f1 = np.mean(f1_scores_boot)\n",
    "mean_roc_auc = np.mean(roc_auc_scores_boot)\n",
    "mean_ppv = np.mean(ppv_scores_boot)\n",
    "mean_npv = np.mean(npv_scores_boot)\n",
    "mean_specificity = np.mean(specificity_scores_boot)\n",
    "\n",
    "# Plot the AUC-ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC-ROC (Mean ROC AUC = {mean_roc_auc:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC-ROC Curve XGBoost Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c923747-6c72-44e3-bfb9-a447cc3d6ee0",
   "metadata": {},
   "source": [
    "# 5. Random Forest Classifier Model (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff47aaa1-0cbc-4f8b-a15c-23f03a45ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid for Grid Search - Random Forest (RF)\n",
    "param_grid_rf = {\n",
    "    'rf__n_estimators': [200],\n",
    "    'rf__max_depth': [30],\n",
    "    'rf__min_samples_split': [8],\n",
    "    'rf__min_samples_leaf': [5],\n",
    "    'rf__bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Define the model and the pipeline\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "pipeline_rf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', rf_model)\n",
    "])\n",
    "\n",
    "# Inner cross-validation loop for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the grid search\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=pipeline_rf, \n",
    "    param_grid=param_grid_rf, \n",
    "    scoring='roc_auc', \n",
    "    cv=inner_cv, \n",
    "    verbose=2, \n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Outer cross-validation loop for model evaluation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "specificity_scores = []\n",
    "conf_matrices = []\n",
    "best_params_list = []  \n",
    "\n",
    "# Nested cross-validation for Random Forest\n",
    "for train_index, test_index in outer_cv.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Grid search on the inner cross-validation loop\n",
    "    grid_search_rf.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Best estimator\n",
    "    best_model_rf = grid_search_rf.best_estimator_\n",
    "    \n",
    "    # Best parameters\n",
    "    best_params_list.append(grid_search_rf.best_params_)\n",
    "    print(\"Best Parameters:\", best_params_list)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_probs = best_model_rf.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Threshold to Classify\n",
    "    threshold = 0.5\n",
    "    y_pred = (y_probs > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "    precision_scores.append(precision_score(y_val_fold, y_pred))\n",
    "    recall_scores.append(recall_score(y_val_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(y_val_fold, y_probs))\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_val_fold, y_pred)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "    \n",
    "    TN = conf_matrix[0, 0]\n",
    "    FP = conf_matrix[0, 1]\n",
    "    specificity = TN / (TN + FP)\n",
    "    specificity_scores.append(specificity)\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "print(f\"Mean Specificity: {np.mean(specificity_scores):.4f}\")\n",
    "\n",
    "# Confusion matrices for each fold\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    print(f\"Confusion Matrix for Fold {i+1}:\\n{cm}\")\n",
    "\n",
    "# Evaluation on the test set with the best model from nested CV\n",
    "best_model_rf.fit(X_train, y_train)\n",
    "y_probs_test = best_model_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Threshold to Classify\n",
    "threshold = 0.5\n",
    "y_pred_test = (y_probs_test > threshold).astype(int)\n",
    "\n",
    "# Calculate test set metrics\n",
    "test_accuracy_rf = accuracy_score(y_test, y_pred_test)\n",
    "test_precision_rf = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall_rf = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_rf = f1_score(y_test, y_pred_test, average='weighted')\n",
    "test_roc_auc_rf = roc_auc_score(y_test, y_probs_test)\n",
    "test_conf_matrix_rf = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Calculate PPV, NPV, and specificity\n",
    "TP = test_conf_matrix_rf[1, 1]\n",
    "TN = test_conf_matrix_rf[0, 0]\n",
    "FP = test_conf_matrix_rf[0, 1]\n",
    "FN = test_conf_matrix_rf[1, 0]\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Accuracy: {test_accuracy_rf:.4f}\")\n",
    "print(f\"Test Precision: {test_precision_rf:.4f}\")\n",
    "print(f\"Test Recall: {test_recall_rf:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1_rf:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc_rf:.4f}\")\n",
    "print(f\"Test Confusion Matrix_RF Model:\\n{test_conf_matrix_rf}\")\n",
    "print(f\"Test Positive Predictive Value (PPV): {PPV:.4f}\")\n",
    "print(f\"Test Negative Predictive Value (NPV): {NPV:.4f}\")\n",
    "print(f\"Test Specificity: {specificity:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix of the test set\n",
    "sns.heatmap(test_conf_matrix_rf, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.ylabel('Actual/True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix_RF Model - Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the 95% confidence intervals (CIs) for each evaluation metric using bootstrapping\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "accuracy_scores_boot = []\n",
    "precision_scores_boot = []\n",
    "recall_scores_boot = []\n",
    "f1_scores_boot = []\n",
    "roc_auc_scores_boot = []\n",
    "ppv_scores_boot = []\n",
    "npv_scores_boot = []\n",
    "specificity_scores_boot = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(y_test), len(y_test))\n",
    "    if len(np.unique(y_test.iloc[indices])) < 2:\n",
    "        continue\n",
    "    \n",
    "    y_test_boot = y_test.iloc[indices]\n",
    "    y_pred_test_boot = y_pred_test[indices]\n",
    "    y_probs_test_boot = y_probs_test[indices]\n",
    "    \n",
    "    accuracy_scores_boot.append(accuracy_score(y_test_boot, y_pred_test_boot))\n",
    "    precision_scores_boot.append(precision_score(y_test_boot, y_pred_test_boot))\n",
    "    recall_scores_boot.append(recall_score(y_test_boot, y_pred_test_boot))\n",
    "    f1_scores_boot.append(f1_score(y_test_boot, y_pred_test_boot))\n",
    "    roc_auc_scores_boot.append(roc_auc_score(y_test_boot, y_probs_test_boot))\n",
    "    \n",
    "    cm_boot = confusion_matrix(y_test_boot, y_pred_test_boot)\n",
    "    TP_boot = cm_boot[1, 1]\n",
    "    TN_boot = cm_boot[0, 0]\n",
    "    FP_boot = cm_boot[0, 1]\n",
    "    FN_boot = cm_boot[1, 0]\n",
    "    ppv_scores_boot.append(TP_boot / (TP_boot + FP_boot) if (TP_boot + FP_boot) > 0 else 0)\n",
    "    npv_scores_boot.append(TN_boot / (TN_boot + FN_boot) if (TN_boot + FN_boot) > 0 else 0)\n",
    "    specificity_scores_boot.append(TN_boot / (TN_boot + FP_boot) if (TN_boot + FP_boot) > 0 else 0)\n",
    "\n",
    "def bootstrap_confidence_interval(data, alpha=0.05):\n",
    "    sorted_data = np.sort(data)\n",
    "    lower_bound = np.percentile(sorted_data, 100 * (alpha / 2))\n",
    "    upper_bound = np.percentile(sorted_data, 100 * (1 - alpha / 2))\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Calculate mean values\n",
    "mean_accuracy = np.mean(accuracy_scores_boot)\n",
    "mean_precision = np.mean(precision_scores_boot)\n",
    "mean_recall = np.mean(recall_scores_boot)\n",
    "mean_f1 = np.mean(f1_scores_boot)\n",
    "mean_roc_auc = np.mean(roc_auc_scores_boot)\n",
    "mean_ppv = np.mean(ppv_scores_boot)\n",
    "mean_npv = np.mean(npv_scores_boot)\n",
    "mean_specificity = np.mean(specificity_scores_boot)\n",
    "\n",
    "# Print confidence intervals\n",
    "print(f\"95% CI for Accuracy: {bootstrap_confidence_interval(accuracy_scores_boot)}\")\n",
    "print(f\"95% CI for Precision: {bootstrap_confidence_interval(precision_scores_boot)}\")\n",
    "print(f\"95% CI for Recall: {bootstrap_confidence_interval(recall_scores_boot)}\")\n",
    "print(f\"95% CI for F1 Score: {bootstrap_confidence_interval(f1_scores_boot)}\")\n",
    "print(f\"95% CI for ROC AUC: {bootstrap_confidence_interval(roc_auc_scores_boot)}\")\n",
    "print(f\"95% CI for PPV: {bootstrap_confidence_interval(ppv_scores_boot)}\")\n",
    "print(f\"95% CI for NPV: {bootstrap_confidence_interval(npv_scores_boot)}\")\n",
    "print(f\"95% CI for Specificity: {bootstrap_confidence_interval(specificity_scores_boot)}\")\n",
    "\n",
    "# Plot the AUC-ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC-ROC (Mean ROC AUC = {mean_roc_auc:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC-ROC Curve Random Forest Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d014011-980a-47cc-a31d-22411e7c1e91",
   "metadata": {},
   "source": [
    "# 6. AdaBoost Classifier Model (AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b90298c-868d-4213-bbdb-701863ece2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid - AdaBoost\n",
    "param_grid_ada = {\n",
    "    'ada__n_estimators': [200],\n",
    "    'ada__learning_rate': [0.1],\n",
    "    'ada__estimator__max_depth': [3]\n",
    "}\n",
    "\n",
    "\n",
    "# Define the model and the pipeline\n",
    "ada = AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ada', ada)\n",
    "])\n",
    "\n",
    "\n",
    "# Inner cross-validation loop for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline, \n",
    "    param_grid=param_grid_ada, \n",
    "    scoring='roc_auc', \n",
    "    cv=inner_cv, \n",
    "    verbose=2, \n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Outer cross-validation loop for model evaluation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "specificity_scores = []\n",
    "conf_matrices = []\n",
    "best_params_list = []\n",
    "\n",
    "# Nested cross-validation for AdaBoost\n",
    "for train_index, test_index in outer_cv.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Grid search on the inner cross-validation loop\n",
    "    grid_search.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Best parameters\n",
    "    best_params_list.append(grid_search.best_params_)\n",
    "    print(\"Best Parameters:\", best_params_list)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_probs = best_model.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Threshold to Classify\n",
    "    threshold = 0.5\n",
    "    y_pred = (y_probs > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "    precision_scores.append(precision_score(y_val_fold, y_pred))\n",
    "    recall_scores.append(recall_score(y_val_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
    "    roc_auc_scores.append(roc_auc_score(y_val_fold, y_probs))\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_val_fold, y_pred)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "    \n",
    "    TN = conf_matrix[0, 0]\n",
    "    FP = conf_matrix[0, 1]\n",
    "    specificity = TN / (TN + FP)\n",
    "    specificity_scores.append(specificity)\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n",
    "print(f\"Mean Specificity: {np.mean(specificity_scores):.4f}\")\n",
    "\n",
    "# Confusion matrices for each fold\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    print(f\"Confusion Matrix for Fold {i+1}:\\n{cm}\")\n",
    "\n",
    "# Evaluation on the test set with the best model from nested CV\n",
    "best_model.fit(X_train, y_train)\n",
    "y_probs_test = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Use a Threshold to Classify\n",
    "threshold = 0.5\n",
    "y_pred_test = (y_probs_test > threshold).astype(int)\n",
    "\n",
    "# Calculate test set metrics\n",
    "test_accuracy_ada = accuracy_score(y_test, y_pred_test)\n",
    "test_precision_ada = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall_ada = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_ada = f1_score(y_test, y_pred_test, average='weighted')\n",
    "test_roc_auc_ada = roc_auc_score(y_test, y_probs_test)\n",
    "test_conf_matrix_ada = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Calculate PPV, NPV, and specificity\n",
    "TP = test_conf_matrix_ada[1, 1]\n",
    "TN = test_conf_matrix_ada[0, 0]\n",
    "FP = test_conf_matrix_ada[0, 1]\n",
    "FN = test_conf_matrix_ada[1, 0]\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Print test set metrics\n",
    "print(f\"Test Accuracy: {test_accuracy_ada:.4f}\")\n",
    "print(f\"Test Precision: {test_precision_ada:.4f}\")\n",
    "print(f\"Test Recall: {test_recall_ada:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1_ada:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc_ada:.4f}\")\n",
    "print(f\"Test Confusion Matrix_AdaBoost Model:\\n{test_conf_matrix_ada}\")\n",
    "print(f\"Test Positive Predictive Value (PPV): {PPV:.4f}\")\n",
    "print(f\"Test Negative Predictive Value (NPV): {NPV:.4f}\")\n",
    "print(f\"Test Specificity: {specificity:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix of the test set\n",
    "sns.heatmap(test_conf_matrix_ada, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.ylabel('Actual/True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix_AdaBoost Model - Test Set')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the 95% confidence intervals (CIs) for each evaluation metric using bootstrapping\n",
    "n_bootstraps = 1000\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "accuracy_scores_boot = []\n",
    "precision_scores_boot = []\n",
    "recall_scores_boot = []\n",
    "f1_scores_boot = []\n",
    "roc_auc_scores_boot = []\n",
    "ppv_scores_boot = []\n",
    "npv_scores_boot = []\n",
    "specificity_scores_boot = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = rng.randint(0, len(y_test), len(y_test))\n",
    "    if len(np.unique(y_test.iloc[indices])) < 2:\n",
    "        continue\n",
    "    \n",
    "    y_test_boot = y_test.iloc[indices]\n",
    "    y_pred_test_boot = y_pred_test[indices]\n",
    "    y_probs_test_boot = y_probs_test[indices]\n",
    "    \n",
    "    accuracy_scores_boot.append(accuracy_score(y_test_boot, y_pred_test_boot))\n",
    "    precision_scores_boot.append(precision_score(y_test_boot, y_pred_test_boot))\n",
    "    recall_scores_boot.append(recall_score(y_test_boot, y_pred_test_boot))\n",
    "    f1_scores_boot.append(f1_score(y_test_boot, y_pred_test_boot))\n",
    "    roc_auc_scores_boot.append(roc_auc_score(y_test_boot, y_probs_test_boot))\n",
    "    \n",
    "    cm_boot = confusion_matrix(y_test_boot, y_pred_test_boot)\n",
    "    TP_boot = cm_boot[1, 1]\n",
    "    TN_boot = cm_boot[0, 0]\n",
    "    FP_boot = cm_boot[0, 1]\n",
    "    FN_boot = cm_boot[1, 0]\n",
    "    ppv_scores_boot.append(TP_boot / (TP_boot + FP_boot) if (TP_boot + FP_boot) > 0 else 0)\n",
    "    npv_scores_boot.append(TN_boot / (TN_boot + FN_boot) if (TN_boot + FN_boot) > 0 else 0)\n",
    "    specificity_scores_boot.append(TN_boot / (TN_boot + FP_boot) if (TN_boot + FP_boot) > 0 else 0)\n",
    "\n",
    "def bootstrap_confidence_interval(data, alpha=0.05):\n",
    "    sorted_data = np.sort(data)\n",
    "    lower_bound = np.percentile(sorted_data, 100 * (alpha / 2))\n",
    "    upper_bound = np.percentile(sorted_data, 100 * (1 - alpha / 2))\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Calculate mean values\n",
    "mean_accuracy = np.mean(accuracy_scores_boot)\n",
    "mean_precision = np.mean(precision_scores_boot)\n",
    "mean_recall = np.mean(recall_scores_boot)\n",
    "mean_f1 = np.mean(f1_scores_boot)\n",
    "mean_roc_auc = np.mean(roc_auc_scores_boot)\n",
    "mean_ppv = np.mean(ppv_scores_boot)\n",
    "mean_npv = np.mean(npv_scores_boot)\n",
    "mean_specificity = np.mean(specificity_scores_boot)\n",
    "\n",
    "# Print confidence intervals\n",
    "print(f\"95% CI for Accuracy: {bootstrap_confidence_interval(accuracy_scores_boot)}\")\n",
    "print(f\"95% CI for Precision: {bootstrap_confidence_interval(precision_scores_boot)}\")\n",
    "print(f\"95% CI for Recall: {bootstrap_confidence_interval(recall_scores_boot)}\")\n",
    "print(f\"95% CI for F1 Score: {bootstrap_confidence_interval(f1_scores_boot)}\")\n",
    "print(f\"95% CI for ROC AUC: {bootstrap_confidence_interval(roc_auc_scores_boot)}\")\n",
    "print(f\"95% CI for PPV: {bootstrap_confidence_interval(ppv_scores_boot)}\")\n",
    "print(f\"95% CI for NPV: {bootstrap_confidence_interval(npv_scores_boot)}\")\n",
    "print(f\"95% CI for Specificity: {bootstrap_confidence_interval(specificity_scores_boot)}\")\n",
    "\n",
    "# Plot the AUC-ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC-ROC (Mean ROC AUC = {mean_roc_auc:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC-ROC Curve AdaBoost Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cdf6a0-977e-458d-ae76-1d7b0e8eadb5",
   "metadata": {},
   "source": [
    "# Combined AUC-ROC Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2347a6c-85cc-4363-acb0-652c47ddc79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to train a model and get ROC curve data\n",
    "def train_and_evaluate_model(model, param_grid, X_train, y_train, X_test, y_test):\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline, \n",
    "        param_grid=param_grid, \n",
    "        scoring='roc_auc', \n",
    "        cv=inner_cv, \n",
    "        verbose=2, \n",
    "        n_jobs=4\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_probs_test = best_model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_probs_test)\n",
    "    return fpr, tpr, roc_auc_score(y_test, y_probs_test), grid_search.best_params_\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grid_lr = {\n",
    "    'model__C': [100],\n",
    "    'model__class_weight': ['balanced'],\n",
    "    'model__fit_intercept': [True],\n",
    "    'model__max_iter': [200],\n",
    "    'model__penalty': ['l2'],\n",
    "    'model__solver': ['lbfgs']\n",
    "}\n",
    "param_grid_gbt = {\n",
    "    'model__n_estimators': [500],\n",
    "    'model__learning_rate': [0.1],\n",
    "    'model__max_depth': [4],\n",
    "    'model__min_samples_split': [5],\n",
    "    'model__min_samples_leaf': [4],\n",
    "    'model__subsample': [1.0],\n",
    "    'model__max_features': ['sqrt']\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [200],\n",
    "    'model__max_depth': [30],\n",
    "    'model__min_samples_split': [8],\n",
    "    'model__min_samples_leaf': [5],\n",
    "    'model__bootstrap': [True]\n",
    "}\n",
    "param_grid_ada = {\n",
    "    'model__n_estimators': [200],\n",
    "    'model__learning_rate': [0.1],\n",
    "    'model__estimator__max_depth': [3]\n",
    "}\n",
    "param_grid_lgbm = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1],\n",
    "    'model__max_depth': [5, 7],\n",
    "    'model__num_leaves': [31, 40],\n",
    "    'model__subsample': [0.8, 0.9],\n",
    "    'model__colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "param_grid_xgb = {\n",
    "    'model__n_estimators': [200],\n",
    "    'model__learning_rate': [0.05],\n",
    "    'model__max_depth': [5],\n",
    "    'model__min_child_weight': [5],\n",
    "    'model__subsample': [0.8],\n",
    "    'model__colsample_bytree': [0.8]\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "fpr_lr, tpr_lr, roc_auc_lr, best_params_lr = train_and_evaluate_model(LogisticRegression(random_state=42), param_grid_lr, X_train, y_train, X_test, y_test)\n",
    "fpr_gbt, tpr_gbt, roc_auc_gbt, best_params_gbt = train_and_evaluate_model(GradientBoostingClassifier(random_state=42), param_grid_gbt, X_train, y_train, X_test, y_test)\n",
    "fpr_rf, tpr_rf, roc_auc_rf, best_params_rf = train_and_evaluate_model(RandomForestClassifier(random_state=42), param_grid_rf, X_train, y_train, X_test, y_test)\n",
    "fpr_ada, tpr_ada, roc_auc_ada, best_params_ada = train_and_evaluate_model(AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42), param_grid_ada, X_train, y_train, X_test, y_test)\n",
    "fpr_lgbm, tpr_lgbm, roc_auc_lgbm, best_params_lgbm = train_and_evaluate_model(lgb.LGBMClassifier(random_state=42), param_grid_lgbm, X_train, y_train, X_test, y_test)\n",
    "fpr_xgb, tpr_xgb, roc_auc_xgb, best_params_xgb = train_and_evaluate_model(xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), param_grid_xgb, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Plot combined AUC-ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_lr:.2f})')\n",
    "plt.plot(fpr_gbt, tpr_gbt, label=f'GBT (AUC = {roc_auc_gbt:.2f})')\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.2f})')\n",
    "plt.plot(fpr_ada, tpr_ada, label=f'AdaBoost (AUC = {roc_auc_ada:.2f})')\n",
    "plt.plot(fpr_lgbm, tpr_lgbm, label=f'LightGBM (AUC = {roc_auc_lgbm:.2f})')\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {roc_auc_xgb:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='black')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db7ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
